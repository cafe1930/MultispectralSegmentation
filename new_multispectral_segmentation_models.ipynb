{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0cede1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import json\n",
    "import yaml\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import tv_tensors\n",
    "from torchvision import models\n",
    "\n",
    "import types\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.base import (\n",
    "    ClassificationHead,\n",
    "    SegmentationHead,\n",
    "    SegmentationModel,\n",
    ")\n",
    "\n",
    "from segmentation_models_pytorch.base import modules as md\n",
    "\n",
    "from segmentation_models_pytorch.encoders import get_encoder\n",
    "from segmentation_models_pytorch.base.hub_mixin import supports_config_loading\n",
    "from typing import Any, Dict, Optional, Union, Callable, Sequence, List, Literal\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger, Logger\n",
    "\n",
    "from torchmetrics import classification\n",
    "from torchmetrics import segmentation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e32608",
   "metadata": {},
   "source": [
    "# Работа с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "678fd044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "class SegmentationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path_to_dataset_root:str, samples_df:pd.DataFrame, channel_indices:list, transforms:v2._transform.Transform, dtype:torch.dtype, device:torch.device):\n",
    "        '''\n",
    "        In:\n",
    "            path_to_dataset_root - путь до корневой папки с датасетом\n",
    "            samples_df - pandas.DataFrame с информацией о файлах\n",
    "            channel_indices - список с номерами каналов мультиспектрального изображения\n",
    "            transforms - аугментация изображений\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.path_to_dataset_root = path_to_dataset_root\n",
    "        self.samples_df = samples_df\n",
    "        self.specter_bands_list = [i for i in channel_indices if isinstance(i, int)]\n",
    "        self.specter_indices_names = [s for s in channel_indices if isinstance(s, str)]\n",
    "        self.dtype_trasform = v2.ToDtype(dtype=dtype, scale=True)\n",
    "        self.other_transforms = transforms\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples_df)\n",
    "    @staticmethod\n",
    "    def compute_spectral_index(index_name, image):\n",
    "        if index_name.lower() == 'ndvi':\n",
    "            b0 = image[7] # NIR, B8\n",
    "            b1 = image[3] # RED, B4\n",
    "            \n",
    "        elif index_name.lower() == 'ndbi':\n",
    "            b0 = image[10] #SWIR, B11\n",
    "            b1 = image[7] #NIR, B8\n",
    "\n",
    "        elif index_name.lower() == 'ndwi':\n",
    "            b0 = image[2] #green, B3\n",
    "            b1 = image[7] #NIR, B8\n",
    "\n",
    "        elif index_name.lower() == 'ndre':\n",
    "            b0 = image[7] #NIR, B8\n",
    "            b1 = image[5] #Red Edge, B6\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            index = (b0 - b1)/(b0 + b1)\n",
    "            \n",
    "        index = np.nan_to_num(index, nan=-5)\n",
    "\n",
    "        return index\n",
    "            \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples_df.iloc[idx]\n",
    "\n",
    "        file_name = sample['file_name']\n",
    "\n",
    "        path_to_image = os.path.join(self.path_to_dataset_root, 'images', f'{file_name}.npy')\n",
    "        path_to_labels = os.path.join(self.path_to_dataset_root, 'labels', f'{file_name}.npy')\n",
    "\n",
    "        image = np.load(path_to_image)\n",
    "        spectral_indices = []\n",
    "        # вычисляем спектральные индексы\n",
    "        if len(self.specter_indices_names) > 0:\n",
    "            for sp_index_name in self.specter_indices_names:\n",
    "                spectral_index = self.compute_spectral_index(sp_index_name, image)\n",
    "                spectral_index = torch.as_tensor(spectral_index)\n",
    "                spectral_indices.append(spectral_index.unsqueeze(0))\n",
    "\n",
    "            spectral_indices = torch.cat(spectral_indices)\n",
    "            spectral_indices = self.dtype_trasform(spectral_indices)\n",
    "\n",
    "\n",
    "        image = torch.as_tensor(image[self.specter_bands_list], dtype=torch.int16)\n",
    "        image = self.dtype_trasform(image)\n",
    "        # добавляем спектральные индексы\n",
    "        if len(self.specter_indices_names) > 0:\n",
    "            image = torch.cat([image, spectral_indices], dim=0) \n",
    "        #image = np.load(path_to_image)\n",
    "        # метки читаем как одноканальное изображение\n",
    "        label = np.load(path_to_labels)\n",
    "        label = np.where(label >= 0, label, 0)\n",
    "        #label = torch.as_tensor(np.load(path_to_labels), dtype=torch.uint8).long()\n",
    "        label = torch.as_tensor(label, dtype=torch.uint8).long()\n",
    "        \n",
    "        image = tv_tensors.Image(image, device=self.device)\n",
    "        label = tv_tensors.Mask(label, device=self.device)\n",
    "\n",
    "        transforms_dict = {'image':image, 'mask':label}\n",
    "        transformed = self.other_transforms(transforms_dict)\n",
    "        return transformed['image'], transformed['mask']#, image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fac0331",
   "metadata": {},
   "source": [
    "# Описание модуля Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7082272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pred_mask(pred):\n",
    "    '''\n",
    "    Определение маски классов на основе сгенерированной softmax маски\n",
    "    '''\n",
    "    #pred = pred.detach()\n",
    "    _, pred_mask = pred.max(dim=1)\n",
    "    return pred_mask#.cpu().numpy()\n",
    "\n",
    "class LightningSegmentationModule(L.LightningModule):\n",
    "    def __init__(self, model:nn.Module, criterion:nn.Module, optimizer_cfg:dict, metrics_dict:dict, name2class_idx_dict:dict) -> None:\n",
    "        '''\n",
    "        Модуль Lightning для обучения сегментационной сети\n",
    "        In:\n",
    "            model - нейронная сеть\n",
    "            criterion - функция потерь\n",
    "            \n",
    "            name2class_idx_dict - словарь с отображением {class_name(str): class_idx(int)}\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer_cfg = optimizer_cfg\n",
    "        self.metrics_dict = metrics_dict\n",
    "        \n",
    "        self.name2class_idx_dict = name2class_idx_dict\n",
    "        # словарь, выполняющий обратное отображение class_idx в class_name\n",
    "        self.class_idx2name_dict = {v:k for k, v in name2class_idx_dict.items()}\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self.optimizer_cfg['optmizer'](self.parameters(), **self.optimizer_cfg['optimizer_args'])\n",
    "        ret_dict = {'optimizer': optimizer}\n",
    "        if self.optimizer_cfg['lr_scheduler'] is not None:\n",
    "            scheduler = self.optimizer_cfg['lr_scheduler'](optimizer, **self.optimizer_cfg['lr_scheduler_args'])\n",
    "            ret_dict['lr_scheduler'] = {'scheduler': scheduler}\n",
    "            ret_dict['lr_scheduler'].update(self.optimizer_cfg['lr_scheduler_params'])\n",
    "        \n",
    "        return ret_dict\n",
    "\n",
    "    def compute_metrics(self, pred_labels, true_labels, mode):\n",
    "        metrics_names_list = self.metrics_dict[mode].keys()\n",
    "        for metric_name in metrics_names_list:\n",
    "            if 'dice' in metric_name.lower():\n",
    "                self.metrics_dict[mode][metric_name].update(pred_labels, true_labels)\n",
    "            else:\n",
    "                self.metrics_dict[mode][metric_name].update(pred_labels.reshape(-1), true_labels.reshape(-1))\n",
    "        \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, true_labels = batch\n",
    "        pred = self.model(data)\n",
    "        loss = self.criterion(pred, true_labels)\n",
    "        # вычисление сгенерированной маски\n",
    "        pred_labels = compute_pred_mask(pred)\n",
    "        #true_labels = true_labels.detach().cpu().numpy()\n",
    "        \n",
    "        self.compute_metrics(pred_labels=pred_labels, true_labels=true_labels, mode='train')\n",
    "\n",
    "        # т.к. мы вычисляем общую ошибку на всей эпохе, то записываем в лог только значение функции потерь\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data, true_labels = batch\n",
    "        pred = self.model(data)\n",
    "        loss = self.criterion(pred, true_labels)\n",
    "        pred_labels = compute_pred_mask(pred)\n",
    "        self.compute_metrics(pred_labels=pred_labels, true_labels=true_labels, mode='val')\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def log_metrics(self, mode):\n",
    "        for metric_name, metric in self.metrics_dict[mode].items():\n",
    "            metric_val = metric.compute()\n",
    "            if 'confusion' in metric_name.lower():\n",
    "                disp_name = f'{mode}_{metric_name}'\n",
    "                self.log(disp_name, metric_val.cpu().tolist(), on_step=False, on_epoch=True, prog_bar=False)\n",
    "            else:\n",
    "                for i, value in enumerate(metric_val):\n",
    "                    class_name = self.class_idx2name_dict[i]\n",
    "                    disp_name = f'{mode}_{metric_name}_{class_name}'\n",
    "                    self.log(disp_name, value, on_step=False, on_epoch=True, prog_bar=True)\n",
    "                disp_name = f'{mode}_{metric_name}_mean'\n",
    "                self.log(disp_name, metric_val.mean(), on_step=False, on_epoch=True, prog_bar=True)\n",
    "            self.metrics_dict[mode][metric_name].reset()\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        '''\n",
    "        Декодирование результатов тренировочной эпохи и запись их в лог\n",
    "        '''\n",
    "        self.log_metrics(mode='train')\n",
    " \n",
    "    def on_validation_epoch_end(self):\n",
    "        '''\n",
    "        Декодирование результатов тестовой эпохи и запись их в лог\n",
    "        (работает точно также, как и )\n",
    "        '''\n",
    "        self.log_metrics(mode='val')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b4de78",
   "metadata": {},
   "source": [
    "# Новые функции потерь (Dice-Crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5889a22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceCELoss(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            ce_weight,\n",
    "            ce_ignore_index,\n",
    "            ce_reducion,\n",
    "            ce_label_smoothing,\n",
    "            dice_mode,\n",
    "            dice_classes,\n",
    "            dice_log_loss,\n",
    "            dice_from_logits,\n",
    "            dice_smooth,\n",
    "            dice_ignore_index,\n",
    "            dice_eps,\n",
    "            losses_weight: List = [0.5, 0.5],\n",
    "            is_trainable_weights: bool = False,\n",
    "            weights_processing_type: str = None,\n",
    "            ):\n",
    "        super().__init__()\n",
    "        self.dice = smp.losses.DiceLoss(\n",
    "            mode=dice_mode,\n",
    "            classes=dice_classes,\n",
    "            log_loss=dice_log_loss,\n",
    "            from_logits=dice_from_logits,\n",
    "            smooth=dice_smooth,\n",
    "            ignore_index=dice_ignore_index,\n",
    "            eps=dice_eps\n",
    "            )\n",
    "        self.ce = nn.CrossEntropyLoss(\n",
    "            weight=ce_weight,\n",
    "            ignore_index=ce_ignore_index,\n",
    "            reduction=ce_reducion,\n",
    "            label_smoothing=ce_label_smoothing,\n",
    "        )\n",
    "        self.loss_weights = torch.tensor(losses_weight)\n",
    "        if is_trainable_weights:\n",
    "            self.loss_weights = nn.Parameter(self.loss_weights)\n",
    "        self.weights_processing_type = weights_processing_type\n",
    "\n",
    "    def forward(self, pred, true):\n",
    "        weights = self.loss_weights\n",
    "        if self.weights_processing_type == 'softmax':\n",
    "            weights = weights.softmax(dim=0)\n",
    "        elif self.weights_processing_type == 'sigmoid':\n",
    "            weights = weights.softmax(dim=0)\n",
    "\n",
    "        ce_loss = self.ce(pred, true) * weights[0]\n",
    "        dice_loss = self.dice(pred, true) * weights[1]\n",
    "        return ce_loss + dice_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7444b3b3",
   "metadata": {},
   "source": [
    "# Адаптация MANet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c55c40b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFABBlockMod(smp.decoders.manet.decoder.MFABBlock):\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, skip: Optional[torch.Tensor] = None, scale_factor=2.0,\n",
    "    ) -> torch.Tensor:\n",
    "        x = self.hl_conv(x)\n",
    "        x = F.interpolate(x, scale_factor=scale_factor, mode=self.interpolation_mode)\n",
    "        attention_hl = self.SE_hl(x)\n",
    "        if skip is not None:\n",
    "            attention_ll = self.SE_ll(skip)\n",
    "            attention_hl = attention_hl + attention_ll\n",
    "            x = x * attention_hl\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class DecoderBlockMod(smp.decoders.manet.decoder.DecoderBlock):\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, skip: Optional[torch.Tensor] = None, scale_factor=2.0,\n",
    "    ) -> torch.Tensor:\n",
    "        x = F.interpolate(x, scale_factor=scale_factor, mode=self.interpolation_mode)\n",
    "        if skip is not None:\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class MAnetDecoderMod(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_channels: List[int],\n",
    "        decoder_channels: List[int],\n",
    "        n_blocks: int = 5,\n",
    "        reduction: int = 16,\n",
    "        use_norm: Union[bool, str, Dict[str, Any]] = \"batchnorm\",\n",
    "        pab_channels: int = 64,\n",
    "        interpolation_mode: str = \"nearest\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if n_blocks != len(decoder_channels):\n",
    "            raise ValueError(\n",
    "                \"Model depth is {}, but you provide `decoder_channels` for {} blocks.\".format(\n",
    "                    n_blocks, len(decoder_channels)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # remove first skip with same spatial resolution\n",
    "        encoder_channels = encoder_channels[1:]\n",
    "\n",
    "        # reverse channels to start from head of encoder\n",
    "        encoder_channels = encoder_channels[::-1]\n",
    "\n",
    "        # computing blocks input and output channels\n",
    "        head_channels = encoder_channels[0]\n",
    "        in_channels = [head_channels] + list(decoder_channels[:-1])\n",
    "        skip_channels = list(encoder_channels[1:]) + [0]\n",
    "        out_channels = decoder_channels\n",
    "\n",
    "        self.center = smp.decoders.manet.decoder.PABBlock(head_channels, pab_channels=pab_channels)\n",
    "\n",
    "        # combine decoder keyword arguments\n",
    "        kwargs = dict(\n",
    "            use_norm=use_norm, interpolation_mode=interpolation_mode\n",
    "        )  # no attention type here\n",
    "        blocks = [\n",
    "            MFABBlockMod(in_ch, skip_ch, out_ch, reduction=reduction, **kwargs)\n",
    "            if skip_ch > 0\n",
    "            else DecoderBlockMod(in_ch, skip_ch, out_ch, **kwargs)\n",
    "            for in_ch, skip_ch, out_ch in zip(in_channels, skip_channels, out_channels)\n",
    "        ]\n",
    "        # for the last we dont have skip connection -> use simple decoder block\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "\n",
    "    def forward(self, features: List[torch.Tensor]) -> torch.Tensor:\n",
    "        bs, channels, img_rows, img_cols = features[0].shape\n",
    "        _, _, feat1_rows, feat1_cols = features[1].shape\n",
    "        upsample_scale_factors_list = [2.0 for i in range(len(self.blocks))]\n",
    "        if (img_rows, img_cols) == (feat1_rows, feat1_cols):\n",
    "            upsample_scale_factors_list[-1] = 1.0\n",
    "\n",
    "        features = features[1:]  # remove first skip with same spatial resolution\n",
    "        features = features[::-1]  # reverse channels to start from head of encoder\n",
    "\n",
    "        head = features[0]\n",
    "        skips = features[1:]\n",
    "\n",
    "\n",
    "        x = self.center(head)\n",
    "        for i, (decoder_block, scale_factor) in enumerate(zip(self.blocks, upsample_scale_factors_list)):\n",
    "            skip = skips[i] if i < len(skips) else None\n",
    "            x = decoder_block(x, skip, scale_factor)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class MAnetMod(SegmentationModel):\n",
    "    \"\"\"MAnet_ :  Multi-scale Attention Net. The MA-Net can capture rich contextual dependencies based on\n",
    "    the attention mechanism, using two blocks:\n",
    "     - Position-wise Attention Block (PAB), which captures the spatial dependencies between pixels in a global view\n",
    "     - Multi-scale Fusion Attention Block (MFAB), which  captures the channel dependencies between any feature map by\n",
    "       multi-scale semantic feature fusion\n",
    "\n",
    "    Args:\n",
    "        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n",
    "            to extract features of different spatial resolution\n",
    "        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features\n",
    "            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n",
    "            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n",
    "            Default is 5\n",
    "        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and\n",
    "            other pretrained weights (see table with available weights for each encoder_name)\n",
    "        decoder_channels: List of integers which specify **in_channels** parameter for convolutions used in decoder.\n",
    "            Length of the list should be the same as **encoder_depth**\n",
    "        decoder_use_norm: Specifies normalization between Conv2D and activation.\n",
    "            Accepts the following types:\n",
    "            - **True**: Defaults to `\"batchnorm\"`.\n",
    "            - **False**: No normalization (`nn.Identity`).\n",
    "            - **str**: Specifies normalization type using default parameters. Available values:\n",
    "              `\"batchnorm\"`, `\"identity\"`, `\"layernorm\"`, `\"instancenorm\"`, `\"inplace\"`.\n",
    "            - **dict**: Fully customizable normalization settings. Structure:\n",
    "              ```python\n",
    "              {\"type\": <norm_type>, **kwargs}\n",
    "              ```\n",
    "              where `norm_name` corresponds to normalization type (see above), and `kwargs` are passed directly to the normalization layer as defined in PyTorch documentation.\n",
    "\n",
    "            **Example**:\n",
    "            ```python\n",
    "            decoder_use_norm={\"type\": \"layernorm\", \"eps\": 1e-2}\n",
    "            ```\n",
    "        decoder_pab_channels: A number of channels for PAB module in decoder.\n",
    "            Default is 64.\n",
    "        decoder_interpolation: Interpolation mode used in decoder of the model. Available options are\n",
    "            **\"nearest\"**, **\"bilinear\"**, **\"bicubic\"**, **\"area\"**, **\"nearest-exact\"**. Default is **\"nearest\"**.\n",
    "        in_channels: A number of input channels for the model, default is 3 (RGB images)\n",
    "        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n",
    "        activation: An activation function to apply after the final convolution layer.\n",
    "            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**,\n",
    "            **callable** and **None**. Default is **None**.\n",
    "        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build\n",
    "            on top of encoder if **aux_params** is not **None** (default). Supported params:\n",
    "                - classes (int): A number of classes\n",
    "                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n",
    "                - dropout (float): Dropout factor in [0, 1)\n",
    "                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\"\n",
    "                    (could be **None** to return logits)\n",
    "        kwargs: Arguments passed to the encoder class ``__init__()`` function. Applies only to ``timm`` models. Keys with ``None`` values are pruned before passing.\n",
    "\n",
    "    Returns:\n",
    "        ``torch.nn.Module``: **MAnet**\n",
    "\n",
    "    .. _MAnet:\n",
    "        https://ieeexplore.ieee.org/abstract/document/9201310\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    @supports_config_loading\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_name: str = \"resnet34\",\n",
    "        encoder_depth: int = 5,\n",
    "        encoder_weights: Optional[str] = \"imagenet\",\n",
    "        decoder_use_norm: Union[bool, str, Dict[str, Any]] = \"batchnorm\",\n",
    "        decoder_channels: Sequence[int] = (256, 128, 64, 32, 16),\n",
    "        decoder_pab_channels: int = 64,\n",
    "        decoder_interpolation: str = \"nearest\",\n",
    "        in_channels: int = 3,\n",
    "        classes: int = 1,\n",
    "        activation: Optional[Union[str, Callable]] = None,\n",
    "        aux_params: Optional[dict] = None,\n",
    "        **kwargs: dict[str, Any],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        decoder_use_batchnorm = kwargs.pop(\"decoder_use_batchnorm\", None)\n",
    "        if decoder_use_batchnorm is not None:\n",
    "            warnings.warn(\n",
    "                \"The usage of decoder_use_batchnorm is deprecated. Please modify your code for decoder_use_norm\",\n",
    "                DeprecationWarning,\n",
    "                stacklevel=2,\n",
    "            )\n",
    "            decoder_use_norm = decoder_use_batchnorm\n",
    "\n",
    "        self.encoder = get_encoder(\n",
    "            encoder_name,\n",
    "            in_channels=in_channels,\n",
    "            depth=encoder_depth,\n",
    "            weights=encoder_weights,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.decoder = MAnetDecoderMod(\n",
    "            encoder_channels=self.encoder.out_channels,\n",
    "            decoder_channels=decoder_channels,\n",
    "            n_blocks=encoder_depth,\n",
    "            use_norm=decoder_use_norm,\n",
    "            pab_channels=decoder_pab_channels,\n",
    "            interpolation_mode=decoder_interpolation,\n",
    "        )\n",
    "\n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=decoder_channels[-1],\n",
    "            out_channels=classes,\n",
    "            activation=activation,\n",
    "            kernel_size=3,\n",
    "        )\n",
    "\n",
    "        if aux_params is not None:\n",
    "            self.classification_head = ClassificationHead(\n",
    "                in_channels=self.encoder.out_channels[-1], **aux_params\n",
    "            )\n",
    "        else:\n",
    "            self.classification_head = None\n",
    "\n",
    "        self.name = \"manet-{}\".format(encoder_name)\n",
    "        self.initialize()\n",
    "\n",
    "\n",
    "#model = MAnetMod()\n",
    "#model.encoder.conv1.stride = 1\n",
    "#ret = model(torch.randn(1, 3, 96, 96))\n",
    "#ret.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200468db",
   "metadata": {},
   "source": [
    "# Адаптация FPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "617f9a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPNMod(SegmentationModel):\n",
    "    \"\"\"FPN_ is a fully convolution neural network for image semantic segmentation.\n",
    "\n",
    "    Args:\n",
    "        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n",
    "            to extract features of different spatial resolution\n",
    "        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features\n",
    "            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n",
    "            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n",
    "            Default is 5\n",
    "        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and\n",
    "            other pretrained weights (see table with available weights for each encoder_name)\n",
    "        decoder_pyramid_channels: A number of convolution filters in Feature Pyramid of FPN_\n",
    "        decoder_segmentation_channels: A number of convolution filters in segmentation blocks of FPN_\n",
    "        decoder_merge_policy: Determines how to merge pyramid features inside FPN. Available options are **add**\n",
    "            and **cat**\n",
    "        decoder_dropout: Spatial dropout rate in range (0, 1) for feature pyramid in FPN_\n",
    "        decoder_interpolation: Interpolation mode used in decoder of the model. Available options are\n",
    "            **\"nearest\"**, **\"bilinear\"**, **\"bicubic\"**, **\"area\"**, **\"nearest-exact\"**. Default is **\"nearest\"**.\n",
    "        in_channels: A number of input channels for the model, default is 3 (RGB images)\n",
    "        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n",
    "        activation: An activation function to apply after the final convolution layer.\n",
    "            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**,\n",
    "            **callable** and **None**. Default is **None**.\n",
    "        upsampling: Final upsampling factor. Default is 4 to preserve input-output spatial shape identity\n",
    "        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build\n",
    "            on top of encoder if **aux_params** is not **None** (default). Supported params:\n",
    "                - classes (int): A number of classes\n",
    "                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n",
    "                - dropout (float): Dropout factor in [0, 1)\n",
    "                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\"\n",
    "                    (could be **None** to return logits)\n",
    "        kwargs: Arguments passed to the encoder class ``__init__()`` function. Applies only to ``timm`` models. Keys with ``None`` values are pruned before passing.\n",
    "\n",
    "    Returns:\n",
    "        ``torch.nn.Module``: **FPN**\n",
    "\n",
    "    .. _FPN:\n",
    "        http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    @supports_config_loading\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_name: str = \"resnet34\",\n",
    "        encoder_depth: int = 5,\n",
    "        encoder_weights: Optional[str] = \"imagenet\",\n",
    "        encoder_type:str = 'conv',\n",
    "        decoder_pyramid_channels: int = 256,\n",
    "        decoder_segmentation_channels: int = 128,\n",
    "        decoder_merge_policy: str = \"add\",\n",
    "        decoder_dropout: float = 0.2,\n",
    "        decoder_interpolation: str = \"nearest\",\n",
    "        in_channels: int = 3,\n",
    "        classes: int = 1,\n",
    "        activation: Optional[str] = None,\n",
    "        upsampling: int = 4,\n",
    "        aux_params: Optional[dict] = None,\n",
    "        **kwargs: dict[str, Any],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # validate input params\n",
    "        if encoder_name.startswith(\"mit_b\") and encoder_depth != 5:\n",
    "            raise ValueError(\n",
    "                \"Encoder {} support only encoder_depth=5\".format(encoder_name)\n",
    "            )\n",
    "\n",
    "        self.encoder = get_encoder(\n",
    "            encoder_name,\n",
    "            in_channels=in_channels,\n",
    "            depth=encoder_depth,\n",
    "            weights=encoder_weights,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.decoder = FPNDecoderMod(\n",
    "            encoder_channels=self.encoder.out_channels,\n",
    "            encoder_depth=encoder_depth,\n",
    "            pyramid_channels=decoder_pyramid_channels,\n",
    "            segmentation_channels=decoder_segmentation_channels,\n",
    "            dropout=decoder_dropout,\n",
    "            merge_policy=decoder_merge_policy,\n",
    "            interpolation_mode=decoder_interpolation,\n",
    "            encoder_type=encoder_type,\n",
    "        )\n",
    "\n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=self.decoder.out_channels,\n",
    "            out_channels=classes,\n",
    "            activation=activation,\n",
    "            kernel_size=1,\n",
    "            upsampling=upsampling,\n",
    "        )\n",
    "\n",
    "        if aux_params is not None:\n",
    "            self.classification_head = ClassificationHead(\n",
    "                in_channels=self.encoder.out_channels[-1], **aux_params\n",
    "            )\n",
    "        else:\n",
    "            self.classification_head = None\n",
    "\n",
    "        self.name = \"fpn-{}\".format(encoder_name)\n",
    "        self.initialize()\n",
    "\n",
    "class FPNModBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pyramid_channels: int,\n",
    "        skip_channels: int,\n",
    "        interpolation_mode: str = \"nearest\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.skip_conv = nn.Conv2d(skip_channels, pyramid_channels, kernel_size=1)\n",
    "        self.interpolation_mode = interpolation_mode\n",
    "\n",
    "    def forward(self, x: torch.Tensor, skip: torch.Tensor, scale_factor: float) -> torch.Tensor:\n",
    "        x = F.interpolate(x, scale_factor=scale_factor, mode=self.interpolation_mode)\n",
    "        if skip.size(1) != 0:\n",
    "            #print(x.shape, skip.shape)\n",
    "            skip = self.skip_conv(skip)\n",
    "            x = x + skip\n",
    "        return x\n",
    "\n",
    "class FPNDecoderMod(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_channels: List[int],\n",
    "        encoder_depth: int = 5,\n",
    "        pyramid_channels: int = 256,\n",
    "        segmentation_channels: int = 128,\n",
    "        dropout: float = 0.2,\n",
    "        merge_policy: Literal[\"add\", \"cat\"] = \"add\",\n",
    "        interpolation_mode: str = \"nearest\",\n",
    "        encoder_type:str = 'conv',\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.out_channels = (\n",
    "            segmentation_channels\n",
    "            if merge_policy == \"add\"\n",
    "            else segmentation_channels * 4\n",
    "        )\n",
    "        #print(self.out_channels)\n",
    "        if encoder_depth < 3:\n",
    "            raise ValueError(\n",
    "                \"Encoder depth for FPN decoder cannot be less than 3, got {}.\".format(\n",
    "                    encoder_depth\n",
    "                )\n",
    "            )\n",
    "\n",
    "        encoder_channels = encoder_channels[::-1]\n",
    "        encoder_channels = encoder_channels[: encoder_depth + 1]\n",
    "        \n",
    "        self.p6 = nn.Conv2d(encoder_channels[0], pyramid_channels, kernel_size=1)\n",
    "        '''\n",
    "        self.p5 = smp.decoders.fpn.decoder.FPNBlock(pyramid_channels, encoder_channels[1], interpolation_mode)\n",
    "        self.p4 = smp.decoders.fpn.decoder.FPNBlock(pyramid_channels, encoder_channels[2], interpolation_mode)\n",
    "        self.p3 = smp.decoders.fpn.decoder.FPNBlock(pyramid_channels, encoder_channels[3], interpolation_mode)\n",
    "        self.p2 = smp.decoders.fpn.decoder.FPNBlock(pyramid_channels, encoder_channels[4], interpolation_mode)\n",
    "        '''\n",
    "        self.p5 = FPNModBlock(pyramid_channels, encoder_channels[1], interpolation_mode)\n",
    "        self.p4 = FPNModBlock(pyramid_channels, encoder_channels[2], interpolation_mode)\n",
    "        self.p3 = FPNModBlock(pyramid_channels, encoder_channels[3], interpolation_mode)\n",
    "        self.p2 = FPNModBlock(pyramid_channels, encoder_channels[4], interpolation_mode)\n",
    "        \n",
    "        if encoder_type == 'conv':\n",
    "            upsamples_list = [4, 3, 2, 1, 0]\n",
    "        elif encoder_type == 'vit':\n",
    "            upsamples_list = [3, 2, 1, 0, 0]\n",
    "\n",
    "\n",
    "        self.seg_blocks = nn.ModuleList(\n",
    "            [\n",
    "                smp.decoders.fpn.decoder.SegmentationBlock(\n",
    "                    pyramid_channels, segmentation_channels, n_upsamples=n_upsamples\n",
    "                )\n",
    "                for n_upsamples in upsamples_list\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.merge = smp.decoders.fpn.decoder.MergeBlock(merge_policy)\n",
    "        self.dropout = nn.Dropout2d(p=dropout, inplace=True)\n",
    "\n",
    "    def forward(self, features: List[torch.Tensor]) -> torch.Tensor:\n",
    "        c2, c3, c4, c5, c6 = features[-5:]\n",
    "\n",
    "        #print([c2.shape, c3.shape, c4.shape, c5.shape, c6.shape])\n",
    "        #print([f.shape for f in features])\n",
    "        #print()\n",
    "        #print(f'c6:{c6.shape}')\n",
    "        p6 = self.p6(c6)\n",
    "        #print(f'p6:{p6.shape};c5:{c5.shape}')\n",
    "        p5 = self.p5(p6, c5, scale_factor=2.0)\n",
    "        #print(f'p5:{p5.shape};c4:{c4.shape}')\n",
    "        p4 = self.p4(p5, c4, scale_factor=2.0)\n",
    "        #print(f'p4:{p4.shape};c3:{c3.shape}')\n",
    "        p3 = self.p3(p4, c3, scale_factor=2.0)\n",
    "        #print(f'p3:{p3.shape};c2:{c2.shape}')\n",
    "        p2 = self.p2(p3, c2, scale_factor=2.0)\n",
    "        #print(f'p2:{p4.shape}')\n",
    "\n",
    "        s6 = self.seg_blocks[0](p6)\n",
    "        s5 = self.seg_blocks[1](p5)\n",
    "        s4 = self.seg_blocks[2](p4)\n",
    "        s3 = self.seg_blocks[3](p3)\n",
    "        s2 = self.seg_blocks[4](p2)\n",
    "\n",
    "        feature_pyramid = [s6, s5, s4, s3, s2]\n",
    "\n",
    "        #print([f.shape for f in feature_pyramid])\n",
    "\n",
    "        x = self.merge(feature_pyramid)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "#model = FPNMod(encoder_name='mit_b0', upsampling=0, encoder_type='vit', image_size=(96, 96))\n",
    "#model.encoder.patch_embed1.proj.stride=1\n",
    "\n",
    "#model = FPNMod(encoder_name='resnet34', upsampling=0)\n",
    "#model.encoder.conv1.stride=(1,1)\n",
    "\n",
    "#ret = model(torch.randn(1, 3, 96, 96))\n",
    "#ret.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaf945c",
   "metadata": {},
   "source": [
    "# UNet with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0d3e9abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetAtt(SegmentationModel):\n",
    "    \"\"\"\n",
    "    U-Net is a fully convolutional neural network architecture designed for semantic image segmentation.\n",
    "\n",
    "    It consists of two main parts:\n",
    "\n",
    "    1. An encoder (downsampling path) that extracts increasingly abstract features\n",
    "    2. A decoder (upsampling path) that gradually recovers spatial details\n",
    "\n",
    "    The key is the use of skip connections between corresponding encoder and decoder layers.\n",
    "    These connections allow the decoder to access fine-grained details from earlier encoder layers,\n",
    "    which helps produce more precise segmentation masks.\n",
    "\n",
    "    The skip connections work by concatenating feature maps from the encoder directly into the decoder\n",
    "    at corresponding resolutions. This helps preserve important spatial information that would\n",
    "    otherwise be lost during the encoding process.\n",
    "\n",
    "    Args:\n",
    "        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n",
    "            to extract features of different spatial resolution\n",
    "        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features\n",
    "            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n",
    "            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n",
    "            Default is 5\n",
    "        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and\n",
    "            other pretrained weights (see table with available weights for each encoder_name)\n",
    "        decoder_channels: List of integers which specify **in_channels** parameter for convolutions used in decoder.\n",
    "            Length of the list should be the same as **encoder_depth**\n",
    "        decoder_use_norm:     Specifies normalization between Conv2D and activation.\n",
    "            Accepts the following types:\n",
    "            - **True**: Defaults to `\"batchnorm\"`.\n",
    "            - **False**: No normalization (`nn.Identity`).\n",
    "            - **str**: Specifies normalization type using default parameters. Available values:\n",
    "              `\"batchnorm\"`, `\"identity\"`, `\"layernorm\"`, `\"instancenorm\"`, `\"inplace\"`.\n",
    "            - **dict**: Fully customizable normalization settings. Structure:\n",
    "              ```python\n",
    "              {\"type\": <norm_type>, **kwargs}\n",
    "              ```\n",
    "              where `norm_name` corresponds to normalization type (see above), and `kwargs` are passed directly to the normalization layer as defined in PyTorch documentation.\n",
    "\n",
    "            **Example**:\n",
    "            ```python\n",
    "            decoder_use_norm={\"type\": \"layernorm\", \"eps\": 1e-2}\n",
    "            ```\n",
    "        decoder_attention_type: Attention module used in decoder of the model. Available options are\n",
    "            **None** and **scse** (https://arxiv.org/abs/1808.08127).\n",
    "        decoder_interpolation: Interpolation mode used in decoder of the model. Available options are\n",
    "            **\"nearest\"**, **\"bilinear\"**, **\"bicubic\"**, **\"area\"**, **\"nearest-exact\"**. Default is **\"nearest\"**.\n",
    "        in_channels: A number of input channels for the model, default is 3 (RGB images)\n",
    "        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n",
    "        activation: An activation function to apply after the final convolution layer.\n",
    "            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**,\n",
    "            **callable** and **None**. Default is **None**.\n",
    "        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build\n",
    "            on top of encoder if **aux_params** is not **None** (default). Supported params:\n",
    "                - classes (int): A number of classes\n",
    "                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n",
    "                - dropout (float): Dropout factor in [0, 1)\n",
    "                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\"\n",
    "                    (could be **None** to return logits)\n",
    "        kwargs: Arguments passed to the encoder class ``__init__()`` function. Applies only to ``timm`` models. Keys with ``None`` values are pruned before passing.\n",
    "\n",
    "    Returns:\n",
    "        ``torch.nn.Module``: Unet\n",
    "\n",
    "    Example:\n",
    "        .. code-block:: python\n",
    "\n",
    "            import torch\n",
    "            import segmentation_models_pytorch as smp\n",
    "\n",
    "            model = smp.Unet(\"resnet18\", encoder_weights=\"imagenet\", classes=5)\n",
    "            model.eval()\n",
    "\n",
    "            # generate random images\n",
    "            images = torch.rand(2, 3, 256, 256)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                mask = model(images)\n",
    "\n",
    "            print(mask.shape)\n",
    "            # torch.Size([2, 5, 256, 256])\n",
    "\n",
    "    .. _Unet:\n",
    "        https://arxiv.org/abs/1505.04597\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    requires_divisible_input_shape = False\n",
    "\n",
    "    @supports_config_loading\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_name: str = \"resnet34\",\n",
    "        encoder_depth: int = 5,\n",
    "        encoder_weights: Optional[str] = \"imagenet\",\n",
    "        decoder_use_norm: Union[bool, str, Dict[str, Any]] = \"batchnorm\",\n",
    "        decoder_channels: Sequence[int] = (256, 128, 64, 32, 16),\n",
    "        decoder_attention_type: Optional[str] = None,\n",
    "        decoder_interpolation: str = \"nearest\",\n",
    "        in_channels: int = 3,\n",
    "        classes: int = 1,\n",
    "        activation: Optional[Union[str, Callable]] = None,\n",
    "        aux_params: Optional[dict] = None,\n",
    "        **kwargs: dict[str, Any],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        decoder_use_batchnorm = kwargs.pop(\"decoder_use_batchnorm\", None)\n",
    "        if decoder_use_batchnorm is not None:\n",
    "            warnings.warn(\n",
    "                \"The usage of decoder_use_batchnorm is deprecated. Please modify your code for decoder_use_norm\",\n",
    "                DeprecationWarning,\n",
    "                stacklevel=2,\n",
    "            )\n",
    "            decoder_use_norm = decoder_use_batchnorm\n",
    "\n",
    "        self.encoder = get_encoder(\n",
    "            encoder_name,\n",
    "            in_channels=in_channels,\n",
    "            depth=encoder_depth,\n",
    "            weights=encoder_weights,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        add_center_block = encoder_name.startswith(\"vgg\")\n",
    "\n",
    "        self.decoder = UnetDecoderAtt(\n",
    "            encoder_channels=self.encoder.out_channels,\n",
    "            decoder_channels=decoder_channels,\n",
    "            n_blocks=encoder_depth,\n",
    "            use_norm=decoder_use_norm,\n",
    "            add_center_block=add_center_block,\n",
    "            attention_type=decoder_attention_type,\n",
    "            interpolation_mode=decoder_interpolation,\n",
    "        )\n",
    "\n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=decoder_channels[-1],\n",
    "            out_channels=classes,\n",
    "            activation=activation,\n",
    "            kernel_size=3,\n",
    "        )\n",
    "\n",
    "        if aux_params is not None:\n",
    "            self.classification_head = ClassificationHead(\n",
    "                in_channels=self.encoder.out_channels[-1], **aux_params\n",
    "            )\n",
    "        else:\n",
    "            self.classification_head = None\n",
    "\n",
    "        self.name = \"u-{}\".format(encoder_name)\n",
    "        self.initialize()\n",
    "\n",
    "class UnetDecoderBlock(nn.Module):\n",
    "    \"\"\"A decoder block in the U-Net architecture that performs upsampling and feature fusion.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        skip_channels: int,\n",
    "        out_channels: int,\n",
    "        use_norm: Union[bool, str, Dict[str, Any]] = \"batchnorm\",\n",
    "        attention_type: Optional[str] = None,\n",
    "        interpolation_mode: str = \"nearest\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.interpolation_mode = interpolation_mode\n",
    "        self.conv1 = md.Conv2dReLU(\n",
    "            in_channels + skip_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_norm=use_norm,\n",
    "        )\n",
    "        self.attention1 = md.Attention(\n",
    "            attention_type, in_channels=in_channels + skip_channels\n",
    "        )\n",
    "        self.conv2 = md.Conv2dReLU(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_norm=use_norm,\n",
    "        )\n",
    "        self.attention2 = md.Attention(attention_type, in_channels=out_channels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        feature_map: torch.Tensor,\n",
    "        target_height: int,\n",
    "        target_width: int,\n",
    "        skip_connection: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        feature_map = F.interpolate(\n",
    "            feature_map,\n",
    "            size=(target_height, target_width),\n",
    "            mode=self.interpolation_mode,\n",
    "        )\n",
    "        if skip_connection is not None:\n",
    "            feature_map = torch.cat([feature_map, skip_connection], dim=1)\n",
    "            feature_map = self.attention1(feature_map)\n",
    "        feature_map = self.conv1(feature_map)\n",
    "        feature_map = self.conv2(feature_map)\n",
    "        feature_map = self.attention2(feature_map)\n",
    "        return feature_map\n",
    "\n",
    "class UnetDecoderAtt(nn.Module):\n",
    "    \"\"\"The decoder part of the U-Net architecture.\n",
    "\n",
    "    Takes encoded features from different stages of the encoder and progressively upsamples them while\n",
    "    combining with skip connections. This helps preserve fine-grained details in the final segmentation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_channels: Sequence[int],\n",
    "        decoder_channels: Sequence[int],\n",
    "        n_blocks: int = 5,\n",
    "        use_norm: Union[bool, str, Dict[str, Any]] = \"batchnorm\",\n",
    "        attention_type: Optional[str] = None,\n",
    "        add_center_block: bool = False,\n",
    "        interpolation_mode: str = \"nearest\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if n_blocks != len(decoder_channels):\n",
    "            raise ValueError(\n",
    "                \"Model depth is {}, but you provide `decoder_channels` for {} blocks.\".format(\n",
    "                    n_blocks, len(decoder_channels)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # remove first skip with same spatial resolution\n",
    "        encoder_channels = encoder_channels[1:]\n",
    "        # reverse channels to start from head of encoder\n",
    "        encoder_channels = encoder_channels[::-1]\n",
    "\n",
    "        # computing blocks input and output channels\n",
    "        head_channels = encoder_channels[0]\n",
    "        in_channels = [head_channels] + list(decoder_channels[:-1])\n",
    "        skip_channels = list(encoder_channels[1:]) + [0]\n",
    "        out_channels = decoder_channels\n",
    "\n",
    "        if add_center_block:\n",
    "            self.center = smp.decoders.unet.decoder.UnetCenterBlock(\n",
    "                head_channels,\n",
    "                head_channels,\n",
    "                use_norm=use_norm,\n",
    "            )\n",
    "        else:\n",
    "            self.center = nn.Identity()\n",
    "\n",
    "        # combine decoder keyword arguments\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for block_in_channels, block_skip_channels, block_out_channels in zip(\n",
    "            in_channels, skip_channels, out_channels\n",
    "        ):\n",
    "            block = UnetDecoderBlockAtt(\n",
    "                block_in_channels,\n",
    "                block_skip_channels,\n",
    "                block_out_channels,\n",
    "                use_norm=use_norm,\n",
    "                attention_type=attention_type,\n",
    "                interpolation_mode=interpolation_mode,\n",
    "            )\n",
    "            self.blocks.append(block)\n",
    "\n",
    "    def forward(self, features: List[torch.Tensor]) -> torch.Tensor:\n",
    "        # spatial shapes of features: [hw, hw/2, hw/4, hw/8, ...]\n",
    "        spatial_shapes = [feature.shape[2:] for feature in features]\n",
    "        spatial_shapes = spatial_shapes[::-1]\n",
    "\n",
    "        features = features[1:]  # remove first skip with same spatial resolution\n",
    "        features = features[::-1]  # reverse channels to start from head of encoder\n",
    "\n",
    "        head = features[0]\n",
    "        skip_connections = features[1:]\n",
    "\n",
    "        x = self.center(head)\n",
    "\n",
    "        for i, decoder_block in enumerate(self.blocks):\n",
    "            # upsample to the next spatial shape\n",
    "            height, width = spatial_shapes[i + 1]\n",
    "            skip_connection = skip_connections[i] if i < len(skip_connections) else None\n",
    "            x = decoder_block(x, height, width, skip_connection=skip_connection)\n",
    "\n",
    "        return x\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079b9d59",
   "metadata": {},
   "source": [
    "# Адаптация FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "21b14254",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNDecoderBlock(nn.Module):\n",
    "    \"\"\"A decoder block in the FCN architecture that performs upsampling and feature fusion.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        use_norm: Union[bool, str, Dict[str, Any]] = \"batchnorm\",\n",
    "        attention_type: Optional[str] = None,\n",
    "        interpolation_mode: str = \"nearest\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.interpolation_mode = interpolation_mode\n",
    "        self.conv1 = md.Conv2dReLU(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_norm=use_norm,\n",
    "        )\n",
    "        self.attention1 = md.Attention(\n",
    "            attention_type, in_channels=in_channels\n",
    "        )\n",
    "        self.conv2 = md.Conv2dReLU(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_norm=use_norm,\n",
    "        )\n",
    "        self.attention2 = md.Attention(attention_type, in_channels=out_channels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        feature_map: torch.Tensor,\n",
    "        target_height: int,\n",
    "        target_width: int,\n",
    "        skip_connection: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        # сначала интерполяция и свертка\n",
    "        feature_map = F.interpolate(\n",
    "            feature_map,\n",
    "            size=(target_height, target_width),\n",
    "            mode=self.interpolation_mode,\n",
    "        )\n",
    "        feature_map = self.conv1(feature_map)\n",
    "        feature_map = self.attention1(feature_map)\n",
    "        \n",
    "        # потом сложение и выходная свертка\n",
    "        if skip_connection is not None:\n",
    "            feature_map = feature_map + skip_connection\n",
    "        feature_map = self.conv2(feature_map)\n",
    "        feature_map = self.attention2(feature_map)\n",
    "        \n",
    "        return feature_map\n",
    "    \n",
    "class FCNDecoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            encoder_channels: Sequence[int],\n",
    "            decoder_last_channel: Sequence[int],\n",
    "            n_blocks: int = 5,\n",
    "            use_norm: Union[bool, str, Dict[str, Any]] = \"batchnorm\",\n",
    "            attention_type: Optional[str] = None,\n",
    "            add_center_block: bool = False,\n",
    "            interpolation_mode: str = \"nearest\",\n",
    "        ):\n",
    "            super().__init__()\n",
    "            # remove first skip with same spatial resolution\n",
    "            encoder_channels = encoder_channels[1:]\n",
    "            # reverse channels to start from head of encoder\n",
    "            encoder_channels = encoder_channels[::-1]\n",
    "\n",
    "            # computing blocks input and output channels\n",
    "            head_channels = encoder_channels[0]\n",
    "            in_channels = encoder_channels\n",
    "            out_channels = encoder_channels[1:] + [decoder_last_channel]\n",
    "            \n",
    "            if add_center_block:\n",
    "                self.center = smp.decoders.unet.decoder.UnetCenterBlock(\n",
    "                    head_channels,\n",
    "                    head_channels//2,\n",
    "                    use_norm=use_norm,\n",
    "                )\n",
    "            else:\n",
    "                self.center = nn.Identity()\n",
    "\n",
    "            # combine decoder keyword arguments\n",
    "            self.blocks = nn.ModuleList()\n",
    "            for block_in_channels, block_out_channels in zip(\n",
    "                in_channels, out_channels\n",
    "            ):\n",
    "                block = FCNDecoderBlock(\n",
    "                    block_in_channels,\n",
    "                    block_out_channels,\n",
    "                    use_norm=use_norm,\n",
    "                    attention_type=attention_type,\n",
    "                    interpolation_mode=interpolation_mode,\n",
    "                )\n",
    "                self.blocks.append(block)\n",
    "\n",
    "    def forward(self, features: List[torch.Tensor]) -> torch.Tensor:\n",
    "        # spatial shapes of features: [hw, hw/2, hw/4, hw/8, ...]\n",
    "        spatial_shapes = [feature.shape[2:] for feature in features]\n",
    "        spatial_shapes = spatial_shapes[::-1]\n",
    "\n",
    "        features = features[1:]  # remove first skip with same spatial resolution\n",
    "        features = features[::-1]  # reverse channels to start from head of encoder\n",
    "        \n",
    "\n",
    "        head = features[0]\n",
    "        skip_connections = features[1:]\n",
    "        \n",
    "\n",
    "        x = self.center(head)\n",
    "\n",
    "        for i, decoder_block in enumerate(self.blocks):\n",
    "            # upsample to the next spatial shape\n",
    "            height, width = spatial_shapes[i + 1]\n",
    "            \n",
    "            skip_connection = skip_connections[i] if i < len(skip_connections) else None\n",
    "            \n",
    "            x = decoder_block(x, height, width, skip_connection=skip_connection)\n",
    "\n",
    "        return x\n",
    "\n",
    "class FCN(SegmentationModel):\n",
    "    \"\"\"\n",
    "    FCN is a fully convolutional neural network architecture designed for semantic image segmentation.\n",
    "\n",
    "    It consists of two main parts:\n",
    "\n",
    "    1. An encoder (downsampling path) that extracts increasingly abstract features\n",
    "    2. A decoder (upsampling path) that gradually recovers spatial details\n",
    "\n",
    "    The key is the use of skip connections between corresponding encoder and decoder layers.\n",
    "    These connections allow the decoder to access fine-grained details from earlier encoder layers,\n",
    "    which helps produce more precise segmentation masks.\n",
    "\n",
    "    The skip connections work by concatenating feature maps from the encoder directly into the decoder\n",
    "    at corresponding resolutions. This helps preserve important spatial information that would\n",
    "    otherwise be lost during the encoding process.\n",
    "\n",
    "    Args:\n",
    "        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n",
    "            to extract features of different spatial resolution\n",
    "        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features\n",
    "            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n",
    "            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n",
    "            Default is 5\n",
    "        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and\n",
    "            other pretrained weights (see table with available weights for each encoder_name)\n",
    "        decoder_channels: List of integers which specify **in_channels** parameter for convolutions used in decoder.\n",
    "            Length of the list should be the same as **encoder_depth**\n",
    "        decoder_use_norm:     Specifies normalization between Conv2D and activation.\n",
    "            Accepts the following types:\n",
    "            - **True**: Defaults to `\"batchnorm\"`.\n",
    "            - **False**: No normalization (`nn.Identity`).\n",
    "            - **str**: Specifies normalization type using default parameters. Available values:\n",
    "              `\"batchnorm\"`, `\"identity\"`, `\"layernorm\"`, `\"instancenorm\"`, `\"inplace\"`.\n",
    "            - **dict**: Fully customizable normalization settings. Structure:\n",
    "              ```python\n",
    "              {\"type\": <norm_type>, **kwargs}\n",
    "              ```\n",
    "              where `norm_name` corresponds to normalization type (see above), and `kwargs` are passed directly to the normalization layer as defined in PyTorch documentation.\n",
    "\n",
    "            **Example**:\n",
    "            ```python\n",
    "            decoder_use_norm={\"type\": \"layernorm\", \"eps\": 1e-2}\n",
    "            ```\n",
    "        decoder_attention_type: Attention module used in decoder of the model. Available options are\n",
    "            **None** and **scse** (https://arxiv.org/abs/1808.08127).\n",
    "        decoder_interpolation: Interpolation mode used in decoder of the model. Available options are\n",
    "            **\"nearest\"**, **\"bilinear\"**, **\"bicubic\"**, **\"area\"**, **\"nearest-exact\"**. Default is **\"nearest\"**.\n",
    "        in_channels: A number of input channels for the model, default is 3 (RGB images)\n",
    "        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n",
    "        activation: An activation function to apply after the final convolution layer.\n",
    "            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**,\n",
    "            **callable** and **None**. Default is **None**.\n",
    "        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build\n",
    "            on top of encoder if **aux_params** is not **None** (default). Supported params:\n",
    "                - classes (int): A number of classes\n",
    "                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n",
    "                - dropout (float): Dropout factor in [0, 1)\n",
    "                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\"\n",
    "                    (could be **None** to return logits)\n",
    "        kwargs: Arguments passed to the encoder class ``__init__()`` function. Applies only to ``timm`` models. Keys with ``None`` values are pruned before passing.\n",
    "\n",
    "    Returns:\n",
    "        ``torch.nn.Module``: Unet\n",
    "\n",
    "    Example:\n",
    "        .. code-block:: python\n",
    "\n",
    "            import torch\n",
    "            import segmentation_models_pytorch as smp\n",
    "\n",
    "            model = smp.Unet(\"resnet18\", encoder_weights=\"imagenet\", classes=5)\n",
    "            model.eval()\n",
    "\n",
    "            # generate random images\n",
    "            images = torch.rand(2, 3, 256, 256)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                mask = model(images)\n",
    "\n",
    "            print(mask.shape)\n",
    "            # torch.Size([2, 5, 256, 256])\n",
    "\n",
    "    .. _Unet:\n",
    "        https://arxiv.org/abs/1505.04597\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    requires_divisible_input_shape = False\n",
    "\n",
    "    @supports_config_loading\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_name: str = \"resnet34\",\n",
    "        encoder_depth: int = 5,\n",
    "        encoder_weights: Optional[str] = \"imagenet\",\n",
    "        decoder_use_norm: Union[bool, str, Dict[str, Any]] = \"batchnorm\",\n",
    "        decoder_last_channel: int = 16,\n",
    "        decoder_attention_type: Optional[str] = None,\n",
    "        decoder_interpolation: str = \"nearest\",\n",
    "        in_channels: int = 3,\n",
    "        classes: int = 1,\n",
    "        activation: Optional[Union[str, Callable]] = None,\n",
    "        aux_params: Optional[dict] = None,\n",
    "        **kwargs: dict[str, Any],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        decoder_use_batchnorm = kwargs.pop(\"decoder_use_batchnorm\", None)\n",
    "        if decoder_use_batchnorm is not None:\n",
    "            warnings.warn(\n",
    "                \"The usage of decoder_use_batchnorm is deprecated. Please modify your code for decoder_use_norm\",\n",
    "                DeprecationWarning,\n",
    "                stacklevel=2,\n",
    "            )\n",
    "            decoder_use_norm = decoder_use_batchnorm\n",
    "\n",
    "        self.encoder = get_encoder(\n",
    "            encoder_name,\n",
    "            in_channels=in_channels,\n",
    "            depth=encoder_depth,\n",
    "            weights=encoder_weights,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        add_center_block = encoder_name.startswith(\"vgg\")\n",
    "\n",
    "        self.decoder = FCNDecoder(\n",
    "            encoder_channels=self.encoder.out_channels,\n",
    "            decoder_last_channel=decoder_last_channel,\n",
    "            n_blocks=encoder_depth,\n",
    "            use_norm=decoder_use_norm,\n",
    "            add_center_block=add_center_block,\n",
    "            attention_type=decoder_attention_type,\n",
    "            interpolation_mode=decoder_interpolation,\n",
    "        )\n",
    "\n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=decoder_last_channel,\n",
    "            out_channels=classes,\n",
    "            activation=activation,\n",
    "            kernel_size=3,\n",
    "        )\n",
    "\n",
    "        if aux_params is not None:\n",
    "            self.classification_head = ClassificationHead(\n",
    "                in_channels=decoder_last_channel, **aux_params\n",
    "            )\n",
    "        else:\n",
    "            self.classification_head = None\n",
    "\n",
    "        self.name = \"fcn-{}\".format(encoder_name)\n",
    "        self.initialize()\n",
    "\n",
    "#model = FCN(decoder_last_channel=32)\n",
    "#ret = model(torch.randn(1, 3, 224, 224))\n",
    "#ret.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4035171",
   "metadata": {},
   "source": [
    "# Фабрики для создания моделей по конфигурациям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1960f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relace_input_layer(model:nn.Module, config:dict):\n",
    "    pass\n",
    "\n",
    "def create_weights_from_avg_ch(weight, new_in_channels):\n",
    "    return torch.cat([weight.mean(dim=1).unsqueeze(1)]*new_in_channels, dim=1)\n",
    "\n",
    "def cerate_weights_from_repeated_ch(weight, in_channels, new_in_channels):\n",
    "    ch_multiple = new_in_channels//in_channels\n",
    "    reminded_channels = new_in_channels%in_channels\n",
    "    # сначала набираем новые каналы путем подставления друг за другом (stack) каналов изначального изображения,\n",
    "    # а затем, если количество новых каналов не делится без остатка на количество изначальных, \n",
    "    # то набираем оставшиеся новые каналы из оставшихся изначальных    \n",
    "    new_weight = torch.cat(\n",
    "        [weight]*ch_multiple + [weight[:,:reminded_channels]], dim=1)\n",
    "    return new_weight\n",
    "\n",
    "def create_model(config_dict, segmentation_nns_factory_dict):\n",
    "    model_name = config_dict['segmentation_nn']['nn_architecture']\n",
    "    # создаем нейронную сеть из фабрики\n",
    "    model = segmentation_nns_factory_dict[model_name](**config_dict['segmentation_nn']['params'])\n",
    "    multispecter_bands_indices = config_dict['multispecter_bands_indices']\n",
    "    in_channels = len(multispecter_bands_indices)\n",
    "    # замена входного слоя, если кол-во каналов изображения не равно трем\n",
    "    input_conv = model.get_submodule(\n",
    "        config_dict['segmentation_nn']['input_layer_config']['layer_path']\n",
    "        )\n",
    "    if 'channels' in config_dict['segmentation_nn']['input_layer_config']['replace_type']:\n",
    "        if in_channels != 3:\n",
    "            # получаем входной слой, специфический для конкретной нейронной сети\n",
    "            \n",
    "            new_input_conv = nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=input_conv.out_channels,\n",
    "                kernel_size=input_conv.kernel_size,\n",
    "                #stride=conv1.stride,\n",
    "                stride=config_dict['segmentation_nn']['input_layer_config']['params']['stride'],\n",
    "                #padding=conv1.padding,\n",
    "                padding=config_dict['segmentation_nn']['input_layer_config']['params']['padding'],\n",
    "                dilation=input_conv.dilation,\n",
    "                groups=input_conv.groups,\n",
    "                bias=input_conv.bias is not None\n",
    "            )\n",
    "            if config_dict['segmentation_nn']['params']['encoder_weights'] is not None:\n",
    "                # выбор типа обнолвления весов\n",
    "                if config_dict['segmentation_nn']['input_layer_config']['weight_update_type'] == 'average_all':\n",
    "                    \n",
    "                    #new_weight = torch.cat([input_conv.weight.mean(dim=1).unsqueeze(1)]*in_channels, dim=1)\n",
    "                    new_weight = create_weights_from_avg_ch(input_conv.weight, in_channels)\n",
    "                    input_conv.weight = nn.Parameter(new_weight)\n",
    "\n",
    "                elif config_dict['segmentation_nn']['input_layer_config']['weight_update_type'] == 'repeate':\n",
    "                    '''\n",
    "                    ch_multiple = in_channels//input_conv.in_channels\n",
    "                    reminded_channels = in_channels%input_conv.in_channels\n",
    "                    new_weight = torch.cat(\n",
    "                        [input_conv.weight]*ch_multiple + [input_conv.weight[:,:reminded_channels]], dim=1)\n",
    "                    '''\n",
    "                    new_weight = cerate_weights_from_repeated_ch(input_conv.weight, input_conv.in_channels, in_channels)\n",
    "                    \n",
    "                if input_conv.bias is not None:\n",
    "                    new_input_conv.bias = input_conv.bias\n",
    "\n",
    "            # перезаписываем входной слой исходя из специфики оригинальной сети\n",
    "            model.set_submodule(\n",
    "                config_dict['segmentation_nn']['input_layer_config']['layer_path'],\n",
    "                new_input_conv\n",
    "                )\n",
    "    elif 'multisize_conv' in config_dict['segmentation_nn']['input_layer_config']['replace_type']:\n",
    "        multisize_params = config_dict['segmentation_nn']['input_layer_config']['params']\n",
    "        new_input_conv = MultisizeConv(**multisize_params)\n",
    "\n",
    "        # Если мы модифицируем входной слой.\n",
    "        if config_dict['segmentation_nn']['params']['encoder_weights'] is not None:\n",
    "            # вычленяем словрь с параметрами размеров ядер сверток.\n",
    "            kernel_sizes_dict = config_dict['segmentation_nn']['input_layer_config']['params']['kernel_size']\n",
    "            interpolated_kernels_dict = {}\n",
    "            # выполняем интерполяцию ядер свертки для каждого набора из новых ядер\n",
    "            for name, kernel_size in kernel_sizes_dict.items():\n",
    "                if isinstance(kernel_size, int):\n",
    "                    kernel_size = (kernel_size, kernel_size)\n",
    "                # получаем интерполированную версию ядер свертки\n",
    "                interpolated_kernels_dict[name] = [\n",
    "                    F.interpolate(input_conv.weight, size=kernel_size, mode='bicubic', antialias=True),\n",
    "                    input_conv.bias]\n",
    "                '''            \n",
    "                out_channels_dict = config_dict['segmentation_nn']['input_layer_config']['params']['out_channels']\n",
    "                for name, out_channels in out_channels_dict.items():\n",
    "                    weights = interpolated_kernels_dict[name][0]\n",
    "                    weights = create_weights_from_avg_ch(weights, in_channels)\n",
    "                    interpolated_kernels_dict[name][0] = weights\n",
    "                '''\n",
    "            #out_channels_dict = config_dict['segmentation_nn']['input_layer_config']['params']['out_channels']\n",
    "            for name in interpolated_kernels_dict.keys():\n",
    "                weights = interpolated_kernels_dict[name][0]\n",
    "                if config_dict['segmentation_nn']['input_layer_config']['weight_update_type'] == 'average_all':\n",
    "                    weights = create_weights_from_avg_ch(weights, new_in_channels=in_channels)\n",
    "                elif config_dict['segmentation_nn']['input_layer_config']['weight_update_type'] == 'repeat':\n",
    "                    weights = cerate_weights_from_repeated_ch(weights, in_channels=input_conv.in_channels, new_in_channels=in_channels)\n",
    "                \n",
    "                interpolated_kernels_dict[name][0] = weights\n",
    "                        \n",
    "            new_input_conv.update_weights(new_weights_dict=interpolated_kernels_dict)\n",
    "        if config_dict['segmentation_nn']['input_layer_config']['params']['aggregation_type'] == 'cat':\n",
    "            # Если тип агрегации выхода MultisizeConv - это конкатенация, то изменяем также второй сверточный слой,\n",
    "            # чтобы число его входных каналов соответствовало числу выходных первого слоя \n",
    "            raise NotImplementedError\n",
    "        # заменяем сходной слой по заранее определенному пути, который может варьировать в зависимости от архитектуры энкодера\n",
    "        model.set_submodule(\n",
    "                config_dict['segmentation_nn']['input_layer_config']['layer_path'],\n",
    "                new_input_conv\n",
    "                )\n",
    "    return model\n",
    "\n",
    "class MultisizeConv(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels:int,\n",
    "            out_channels:dict,\n",
    "            kernel_size:dict,\n",
    "            stride:dict,\n",
    "            padding:dict,\n",
    "            dilation:dict,\n",
    "            groups:dict,\n",
    "            bias:dict,\n",
    "            aggregation_type:str,\n",
    "            ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.aggregation_type = aggregation_type\n",
    "        self.multisize_convs = nn.ModuleDict()\n",
    "        for conv_name in kernel_size.keys():\n",
    "            self.multisize_convs[conv_name] = nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels[conv_name],\n",
    "                kernel_size=kernel_size[conv_name],\n",
    "                stride=stride[conv_name],\n",
    "                padding=padding[conv_name],\n",
    "                dilation=dilation[conv_name],\n",
    "                groups=groups[conv_name],\n",
    "                bias=bias[conv_name]\n",
    "                )\n",
    "        \n",
    "    def update_weights(self, new_weights_dict):\n",
    "        '''\n",
    "        На вход принимается словрь со структурой {'имя_свертки': (weight, bias)}\n",
    "        '''\n",
    "        for conv_name, (weight, bias) in new_weights_dict.items():\n",
    "            self.multisize_convs[conv_name].weight = nn.Parameter(weight)\n",
    "            if self.multisize_convs[conv_name].bias is not None:\n",
    "                self.multisize_convs[conv_name].bias = nn.Parameter(bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for conv_name in self.multisize_convs.keys():\n",
    "            out = self.multisize_convs[conv_name](x)\n",
    "            #print(out.shape)\n",
    "            outputs.append(out)\n",
    "        \n",
    "        if self.aggregation_type == 'add':\n",
    "            outputs = torch.stack(outputs, dim=0)\n",
    "            outputs = outputs.sum(dim=0)\n",
    "        elif self.aggregation_type == 'cat':\n",
    "            outputs = torch.cat(outputs, dim=1)\n",
    "        else:\n",
    "            raise ValueError(f'self.aggregation_type should be either \"add\" or \"cat\". Got {self.aggregation_type}')\n",
    "        return outputs\n",
    "\n",
    "segmentation_nns_factory_dict = {\n",
    "    'unet': smp.Unet,\n",
    "    'fpn': smp.FPN,\n",
    "    'custom_fpn': FPNMod,\n",
    "    'unet++': smp.UnetPlusPlus,\n",
    "    'fcn': FCN,\n",
    "    'custom_manet': MAnetMod,\n",
    "}\n",
    "\n",
    "criterion_factory_dict = {\n",
    "    'crossentropy': nn.CrossEntropyLoss,\n",
    "    'dice_crossentropy': DiceCELoss,\n",
    "    'dice': smp.losses.DiceLoss\n",
    "}\n",
    "\n",
    "optimizers_factory_dict = {\n",
    "    'adam': torch.optim.Adam,\n",
    "    'adamw': torch.optim.AdamW\n",
    "}\n",
    "\n",
    "lr_schedulers_factory_dict = {\n",
    "    'cosine_warm_restarts': torch.optim.lr_scheduler.CosineAnnealingWarmRestarts,\n",
    "    'plateau': torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    'cosine': torch.optim.lr_scheduler.CosineAnnealingLR,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4680fb11",
   "metadata": {},
   "source": [
    "# Конфигурации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2291919a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name_postfix': None,\n",
       " 'segmentation_nn': {'nn_architecture': 'unet',\n",
       "  'params': {'encoder_name': 'tu-cspresnext50',\n",
       "   'encoder_depth': 5,\n",
       "   'encoder_weights': 'imagenet',\n",
       "   'decoder_use_norm': 'batchnorm',\n",
       "   'decoder_channels': (256, 128, 128, 128, 128),\n",
       "   'decoder_attention_type': None,\n",
       "   'decoder_interpolation': 'nearest',\n",
       "   'in_channels': 3,\n",
       "   'classes': 11,\n",
       "   'activation': None,\n",
       "   'aux_params': None},\n",
       "  'input_layer_config': {'layer_path': 'encoder.model.stem_conv1.conv',\n",
       "   'replace_type': 'channels+stride',\n",
       "   'weight_update_type': 'average_all',\n",
       "   'params': {'stride': (1, 1), 'padding': (1, 1)}}},\n",
       " 'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
       " 'input_image_size': 96,\n",
       " 'loss': {'type': 'crossentropy',\n",
       "  'params': {'weight': None,\n",
       "   'ignore_index': -100,\n",
       "   'reduction': 'mean',\n",
       "   'label_smoothing': 0.15}},\n",
       " 'optimizer': {'type': 'adam', 'args': {}},\n",
       " 'lr_scheduler': {'type': 'cosine_warm_restarts',\n",
       "  'args': {'T_0': 25, 'T_mult': 1, 'eta_min': 0, 'last_epoch': -1},\n",
       "  'params': {'interval': 'epoch',\n",
       "   'frequency': 1,\n",
       "   'monitor': 'val_loss',\n",
       "   'strict': True,\n",
       "   'name': None}},\n",
       " 'device': 'cuda:0',\n",
       " 'batch_size': 16,\n",
       " 'epoch_num': 200,\n",
       " 'path_to_dataset_root': 'C:\\\\Users\\\\admin\\\\python_programming\\\\DATA\\\\DATA_FOR_TRAINIG_96'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet_config_dict = {\n",
    "    'name_postfix': None,\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'unet',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': (256, 128, 128, 128, 128),\n",
    "            'decoder_attention_type': None,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':200,\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96'\n",
    "}\n",
    "\n",
    "att_unet_config_dict = {\n",
    "    'name_postfix': 'att',\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'unet',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': (256, 128, 128, 128, 128),\n",
    "            'decoder_attention_type': 'scse',\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'average_all',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':200,\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96'\n",
    "}\n",
    "\n",
    "unet_mit_config_dict = {\n",
    "    'name_postfix': None,\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'unet',\n",
    "        'params': {\n",
    "            'encoder_name': \"mit_b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': (256, 128, 128, 128, 128),\n",
    "            'decoder_attention_type': None,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder.patch_embed1.proj',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'average_all',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (3, 3),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':200,\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96'\n",
    "}\n",
    "\n",
    "unet_maxvit_config_dict = {\n",
    "    'name_postfix': None,\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'unet',\n",
    "        'params': {\n",
    "            'encoder_name': 'tu-maxvit_tiny_rw_224',\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': (256, 128, 128, 128, 128),\n",
    "            'decoder_attention_type': None,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "            'img_size':96,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder.model.stem.conv1',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'average_all',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':200,\n",
    "    #'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    'path_to_dataset_root': r'/home/mikhail_u/develop/DATA/DATA_FOR_TRAINIG_96'\n",
    "}\n",
    "\n",
    "net_hgnetv2_config_dict = {\n",
    "    'name_postfix': None,\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'unet',\n",
    "        'params': {\n",
    "            'encoder_name': 'tu-hgnetv2_b1',\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': (256, 128, 128, 128, 128),\n",
    "            'decoder_attention_type': None,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "            #'img_size':96,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder.model.stem.stem1.conv',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'average_all',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':200,\n",
    "    #'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    'path_to_dataset_root': r'/home/mikhail_u/develop/DATA/DATA_FOR_TRAINIG_96'\n",
    "}\n",
    "\n",
    "net_mambaout_config_dict = {\n",
    "    'name_postfix': None,\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'unet',\n",
    "        'params': {\n",
    "            'encoder_name': 'tu-mambaout_small',\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': (256, 128, 128, 128, 128),\n",
    "            'decoder_attention_type': None,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "            #'img_size':96,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder.model.stem.conv1',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'average_all',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':200,\n",
    "    #'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    'path_to_dataset_root': r'/home/mikhail_u/develop/DATA/DATA_FOR_TRAINIG_96'\n",
    "}\n",
    "\n",
    "unet_hrnet_config_dict = {\n",
    "    'name_postfix': None,\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'unet',\n",
    "        'params': {\n",
    "            'encoder_name': 'tu-hrnet_w18',\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': (256, 128, 128, 128, 128),\n",
    "            'decoder_attention_type': None,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "            #'img_size':96,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder.model.conv1',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'average_all',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':200,\n",
    "    #'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    'path_to_dataset_root': r'/home/mikhail_u/develop/DATA/DATA_FOR_TRAINIG_96'\n",
    "}\n",
    "\n",
    "unet_cspresnext_config_dict = {\n",
    "    'name_postfix': None,\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'unet',\n",
    "        'params': {\n",
    "            'encoder_name': 'tu-cspresnext50',\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': (256, 128, 128, 128, 128),\n",
    "            'decoder_attention_type': None,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "            #'img_size':96,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder.model.stem_conv1.conv',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'average_all',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':200,\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'/home/mikhail_u/develop/DATA/DATA_FOR_TRAINIG_96'\n",
    "}\n",
    "\n",
    "fpn_config_dict = {\n",
    "    'name_postfix': None,\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'custom_fpn',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_pyramid_channels': 128,\n",
    "            'decoder_segmentation_channels': 128,\n",
    "            'decoder_merge_policy': \"add\",\n",
    "            'decoder_dropout': 0.2,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'upsampling': 0,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                },\n",
    "        },\n",
    "    },\n",
    "    'multispecter_bands_indices': [1, 2, 3, 7, 'ndvi'],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'},\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':200,\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "}\n",
    "\n",
    "fcn_config_dict = {\n",
    "    'name_postfix': None,\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'fcn',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': 'batchnorm',\n",
    "            'decoder_last_channel': 16,\n",
    "            'decoder_attention_type': None,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation':  None,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                },\n",
    "        },\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "        #'params': {},\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':200,\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "}\n",
    "\n",
    "fpn_config_dict_dice_ce = {\n",
    "    'name_postfix': None,\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'custom_fpn',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_pyramid_channels': 128,\n",
    "            'decoder_segmentation_channels': 128,\n",
    "            'decoder_merge_policy': \"add\",\n",
    "            'decoder_dropout': 0.2,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'upsampling': 0,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                },\n",
    "        },\n",
    "    },\n",
    "    'multispecter_bands_indices': [1, 2, 3, 7, 'ndvi'],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'dice_crossentropy',\n",
    "        #'params': {'weight': 'classes'},\n",
    "        'params': {\n",
    "            'ce_weight':None,\n",
    "            'ce_ignore_index':-100,\n",
    "            'ce_reducion':'mean',\n",
    "            'ce_label_smoothing':0.15,\n",
    "            'dice_mode':'multiclass',\n",
    "            'dice_classes': None,\n",
    "            'dice_log_loss':False,\n",
    "            'dice_from_logits':True,\n",
    "            'dice_smooth':0.15,\n",
    "            'dice_ignore_index':-100,\n",
    "            'dice_eps': 1e-7,\n",
    "            'losses_weight': [0.5, 0.5],\n",
    "            'is_trainable_weights': True,\n",
    "            'weights_processing_type': 'softmax',\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':200,\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "}\n",
    "\n",
    "fpn_config_dict_dice = {\n",
    "    'name_postfix': None,\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'custom_fpn',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_pyramid_channels': 128,\n",
    "            'decoder_segmentation_channels': 128,\n",
    "            'decoder_merge_policy': \"add\",\n",
    "            'decoder_dropout': 0.2,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'upsampling': 0,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                },\n",
    "        },\n",
    "    },\n",
    "    'multispecter_bands_indices': [1, 2, 3, 7, 'ndvi'],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'dice',\n",
    "        #'params': {'weight': 'classes'},\n",
    "        'params': {\n",
    "            'mode':'multiclass',\n",
    "            'classes': None,\n",
    "            'log_loss':False,\n",
    "            'from_logits':True,\n",
    "            'smooth':0.15,\n",
    "            'ignore_index':-100,\n",
    "            'eps': 1e-7,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':200,\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "}\n",
    "\n",
    "fpn_config_dict_ce = {\n",
    "    'name_postfix': None,\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'custom_fpn',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_pyramid_channels': 128,\n",
    "            'decoder_segmentation_channels': 128,\n",
    "            'decoder_merge_policy': \"add\",\n",
    "            'decoder_dropout': 0.2,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'upsampling': 0,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'average_all',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                },\n",
    "        },\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'},\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size':16,\n",
    "    'epoch_num':200,\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "}\n",
    "\n",
    "fpn_multisize_input_config_dict_ce = {\n",
    "    'name_postfix': None,\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'custom_fpn',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_pyramid_channels': 128,\n",
    "            'decoder_segmentation_channels': 128,\n",
    "            'decoder_merge_policy': \"add\",\n",
    "            'decoder_dropout': 0.2,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'upsampling': 0,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'multisize_conv',\n",
    "            'weight_update_type': 'repeat', # avearge_all OR repeate\n",
    "            'params':{\n",
    "                'in_channels': 3,\n",
    "                'out_channels': {\n",
    "                    '1x1': 32,\n",
    "                    '3x3': 32,\n",
    "                    #'5x5': 32, \n",
    "                },\n",
    "                'kernel_size': {\n",
    "                    '1x1': 1,\n",
    "                    '3x3': 3,\n",
    "                    #'5x5': 5, \n",
    "                },\n",
    "                'stride': {\n",
    "                    '1x1': 1,\n",
    "                    '3x3': 1,\n",
    "                    #'5x5': 1, \n",
    "                },\n",
    "                'padding': {\n",
    "                    '1x1': 0,\n",
    "                    '3x3': 1,\n",
    "                    #'5x5': 2, \n",
    "                },\n",
    "                'dilation': {\n",
    "                    '1x1': 1,\n",
    "                    '3x3': 1,\n",
    "                    #'5x5': 1, \n",
    "                },\n",
    "                'groups': {\n",
    "                    '1x1': 1,\n",
    "                    '3x3': 1,\n",
    "                    #'5x5': 1, \n",
    "                },\n",
    "                'bias': {\n",
    "                    '1x1': False,\n",
    "                    '3x3': False,\n",
    "                    #'5x5': False, \n",
    "                },\n",
    "                'aggregation_type': 'add',\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'},\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':200,\n",
    "    #'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "}\n",
    "\n",
    "fpn_mit_config_dict_ce = {\n",
    "    'name_postfix': None,\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'custom_fpn',\n",
    "        'params': {\n",
    "            'encoder_name': \"mit_b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_pyramid_channels': 128,\n",
    "            'decoder_segmentation_channels': 128,\n",
    "            'decoder_merge_policy': \"add\",\n",
    "            'decoder_dropout': 0.2,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'upsampling': 0,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                },\n",
    "        },\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'},\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':200,\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "}\n",
    "\n",
    "custom_manet_config_dict = {\n",
    "    'name_postfix': None,\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'custom_manet',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': [256, 128, 64, 64, 64],\n",
    "            'decoder_pab_channels': 64,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                },\n",
    "        },\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'},\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size':8,\n",
    "    'epoch_num':200,\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "}\n",
    "\n",
    "config_dict = unet_cspresnext_config_dict\n",
    "\n",
    "config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "85ae8eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_save = r'training_configs/unet_cspresnext.yaml'\n",
    "with open(path_to_save, 'w', encoding='utf-8') as fd:\n",
    "    yaml.dump(config_dict, fd, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d8ee3a",
   "metadata": {},
   "source": [
    "# Создание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5037b82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 13, 96, 96]) torch.Size([16, 11, 96, 96])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'unet_tu-cspresnext50 2025-09-17T21-18-04'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_dataset_root = config_dict['path_to_dataset_root']\n",
    "\n",
    "path_to_dataset_info_csv = os.path.join(path_to_dataset_root, 'data_info_table.csv')\n",
    "path_to_surface_classes_json = os.path.join(path_to_dataset_root, 'surface_classes.json')\n",
    "\n",
    "input_image_size = config_dict['input_image_size']\n",
    "multispecter_bands_indices = config_dict['multispecter_bands_indices']\n",
    "device = config_dict['device']\n",
    "\n",
    "# чтение списка имен классов поверхностей\n",
    "with open(path_to_surface_classes_json) as fd:\n",
    "    surface_classes_list = json.load(fd)\n",
    "# чтение таблицы с информацией о каждом изображении в выборке\n",
    "images_df = pd.read_csv(path_to_dataset_info_csv)\n",
    "\n",
    "path_to_partition_json = os.path.join(path_to_dataset_root, 'dataset_partition.json')\n",
    "# чтение словаря со списками квадратов, находящихся в обучающей и тестовой выборке\n",
    "with open(path_to_partition_json) as fd:\n",
    "    partition_dict = json.load(fd)\n",
    "\n",
    "# формирование pandas DataFrame-ов с информацией об изображениях обучающей и тестовой выборках\n",
    "train_images_df = []\n",
    "for train_square in partition_dict['train_squares']:\n",
    "    train_images_df.append(images_df[images_df['square_id']==train_square])\n",
    "train_images_df = pd.concat(train_images_df, ignore_index=True)\n",
    "\n",
    "test_images_df = []\n",
    "for test_square in partition_dict['test_squares']:\n",
    "    test_images_df.append(images_df[images_df['square_id']==test_square])\n",
    "test_images_df = pd.concat(test_images_df, ignore_index=True)\n",
    "\n",
    "#train_images_df, test_images_df = train_test_split(images_df, test_size=0.3, random_state=0)\n",
    "\n",
    "class_num = images_df['class_num'].iloc[0]\n",
    "\n",
    "# формирование словаря, отображающейго имя класса поверхности в индекс класса\n",
    "class_name2idx_dict = {n:i for i, n in enumerate(surface_classes_list)}\n",
    "\n",
    "# вычисление распределений пикселей в классах поверхностей \n",
    "classes_pixels_distribution_df = images_df[surface_classes_list]\n",
    "classes_pixels_num = classes_pixels_distribution_df.sum()\n",
    "classes_weights = classes_pixels_num / classes_pixels_num.sum()\n",
    "classes_weights = classes_weights[surface_classes_list].to_numpy().astype(np.float32)\n",
    "\n",
    "input_image_size = config_dict['input_image_size']\n",
    "'''\n",
    "train_transforms = v2.Compose(\n",
    "    [v2.Resize((input_image_size,input_image_size), antialias=True),v2.ToDtype(torch.float32, scale=True)])\n",
    "test_transforms = v2.Compose(\n",
    "    [v2.Resize((input_image_size,input_image_size), antialias=True),v2.ToDtype(torch.float32, scale=True)])\n",
    "'''\n",
    "train_transforms = nn.Identity()\n",
    "test_transforms = nn.Identity()\n",
    "# если ф-ция потерь перекрестная энтропия, то проверяем, есть ли там веса классов\n",
    "if config_dict['loss']['type'] == 'crossentropy':\n",
    "    # если в параметрах функции потерь стоит строка 'classes', надо передать в функцию вектор весов классов\n",
    "    if 'weight' in config_dict['loss']['params']:\n",
    "        if isinstance(config_dict['loss']['params']['weight'], (list, tuple)):\n",
    "            config_dict['loss']['params']['weight'] = torch.tensor(config_dict['loss']['params']['weight'])\n",
    "        \n",
    "        elif config_dict['loss']['params']['weight'] is not None:\n",
    "            config_dict['loss']['params']['weight'] = torch.tensor(classes_weights)\n",
    "\n",
    "# создание функции потерь\n",
    "criterion = criterion_factory_dict[config_dict['loss']['type']](**config_dict['loss']['params'])\n",
    "\n",
    "# если ф-ция потерь перекрестная энтропия, то проверяем, есть ли там веса классов\n",
    "if config_dict['loss']['type'] == 'crossentropy':\n",
    "    # если в параметрах функции потерь стоит строка 'classes', надо передать в функцию вектор весов классов\n",
    "    if 'weight' in config_dict['loss']['params']:\n",
    "        if isinstance(config_dict['loss']['params']['weight'], torch.Tensor):\n",
    "            config_dict['loss']['params']['weight'] = config_dict['loss']['params']['weight'].cpu().tolist()\n",
    "\n",
    "model = create_model(config_dict, segmentation_nns_factory_dict)\n",
    "model = model.to(device)\n",
    "\n",
    "# создаем датасеты и даталоадеры\n",
    "train_dataset = SegmentationDataset(path_to_dataset_root=path_to_dataset_root, samples_df=train_images_df, channel_indices=multispecter_bands_indices, transforms=train_transforms, dtype=torch.float32, device=device)\n",
    "test_dataset = SegmentationDataset(path_to_dataset_root=path_to_dataset_root, samples_df=test_images_df, channel_indices=multispecter_bands_indices, transforms=test_transforms, dtype=torch.float32, device=device)\n",
    "#train_dataset = SegmentationDatasetApplSurf(path_to_dataset_root=path_to_dataset_root, samples_df=test_images_df, channel_indices=channel_indices, name2class_idx_dict=class_name2idx_dict, applicable_surfaces_dict=applicable_surfaces_dict, transforms=test_transforms, device=device)\n",
    "#test_dataset = SegmentationDatasetApplSurf(path_to_dataset_root=path_to_dataset_root, samples_df=test_images_df, channel_indices=channel_indices, name2class_idx_dict=class_name2idx_dict, applicable_surfaces_dict=applicable_surfaces_dict, transforms=test_transforms, device=device)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config_dict['batch_size'], shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=config_dict['batch_size'])\n",
    "\n",
    "# тестовое чтение данных\n",
    "for data, labels in test_loader:\n",
    "    break\n",
    "    pred = model(data)\n",
    "    loss = criterion(pred, labels)\n",
    "    loss.backward()\n",
    "    \n",
    "\n",
    "# тестовая обработка данных нейронной сетью\n",
    "ret = model(data)\n",
    "print(data.shape, ret.shape)\n",
    "\n",
    "createion_time_str = datetime.now().strftime('%Y-%m-%dT%H-%M-%S')\n",
    "nn_arch_str = config_dict[\"segmentation_nn\"][\"nn_architecture\"]\n",
    "nn_encoder_str = config_dict[\"segmentation_nn\"][\"params\"][\"encoder_name\"]\n",
    "name_postfix = config_dict[\"name_postfix\"]\n",
    "if name_postfix is not None:\n",
    "    model_name = f'{nn_arch_str}_{nn_encoder_str}_{name_postfix} {createion_time_str}'\n",
    "else:\n",
    "    model_name = f'{nn_arch_str}_{nn_encoder_str} {createion_time_str}'\n",
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f4c339",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e6b7cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############################\n",
      "unet_tu-cspresnext50 2025-09-17T21-18-04\n",
      "#############################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\admin\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:654: Checkpoint directory C:\\Users\\admin\\python_programming\\MultispectralSegmentation\\saving_dir\\unet_tu-cspresnext50 2025-09-17T21-18-04 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | Unet             | 28.5 M | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "28.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "28.5 M    Total params\n",
      "114.085   Total estimated model params size (MB)\n",
      "427       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:  16%|█▌        | 17/106 [00:21<01:54,  0.78it/s, v_num=0]"
     ]
    }
   ],
   "source": [
    "epoch_num = config_dict['epoch_num']\n",
    "\n",
    "print('#############################')\n",
    "print(model_name)\n",
    "print('#############################')\n",
    "print()\n",
    "\n",
    "# создаем список словарей с информацией о вычисляемых метриках с помощью multiclass confusion matrix\n",
    "# см. подробнее ддокументацию к функции compute_metric_from_confusion\n",
    "metrics_dict = {\n",
    "    'train': {\n",
    "        'iou': classification.JaccardIndex(task='multiclass', average='none', num_classes=len(class_name2idx_dict)).to(device),\n",
    "        'precision': classification.Precision(task='multiclass', average='none', num_classes=len(class_name2idx_dict)).to(device),\n",
    "        'recall': classification.Precision(task='multiclass', average='none', num_classes=len(class_name2idx_dict)).to(device),\n",
    "        #'confusion': classification.ConfusionMatrix(task='multiclass', num_classes=len(class_name2idx_dict)).to(device),\n",
    "    },\n",
    "    'val': {\n",
    "        'iou': classification.JaccardIndex(task='multiclass', average='none', num_classes=len(class_name2idx_dict)).to(device),\n",
    "        'precision': classification.Precision(task='multiclass', average='none', num_classes=len(class_name2idx_dict)).to(device),\n",
    "        'recall': classification.Precision(task='multiclass', average='none', num_classes=len(class_name2idx_dict)).to(device),\n",
    "        #'confusion': classification.ConfusionMatrix(task='multiclass', num_classes=len(class_name2idx_dict)).to(device),\n",
    "    }\n",
    "}\n",
    "\n",
    "optimizer_cfg = {\n",
    "    'optmizer': optimizers_factory_dict[config_dict['optimizer']['type']],\n",
    "    'optimizer_args':config_dict['optimizer']['args'],\n",
    "    'lr_scheduler': lr_schedulers_factory_dict[config_dict['lr_scheduler']['type']],\n",
    "    'lr_scheduler_args': config_dict['lr_scheduler']['args'],\n",
    "    'lr_scheduler_params': config_dict['lr_scheduler']['params']\n",
    "\n",
    "}\n",
    "\n",
    "# Создаем модуль Lightning\n",
    "segmentation_module = LightningSegmentationModule(model, criterion, optimizer_cfg, metrics_dict, class_name2idx_dict)\n",
    "\n",
    "# задаем путь до папки с логгерами и создаем логгер, записывающий результаты в csv\n",
    "path_to_saving_dir = 'saving_dir'\n",
    "csv_logger = CSVLogger(\n",
    "    save_dir = path_to_saving_dir,\n",
    "    name=model_name, \n",
    "    flush_logs_every_n_steps=1,\n",
    "    )\n",
    "\n",
    "# создаем объект, записывающий в чекпоинт лучшую модель\n",
    "path_to_save_model_dir = os.path.join(path_to_saving_dir, model_name)\n",
    "os.makedirs(path_to_save_model_dir, exist_ok=True)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    mode=\"max\",\n",
    "    filename=model_name+\"-{epoch:02d}-{val_iou_mean:.3}\",\n",
    "    dirpath=path_to_save_model_dir, \n",
    "    save_top_k=1, monitor=\"val_iou_mean\"\n",
    "    )\n",
    "\n",
    "trainer = L.Trainer(logger=[csv_logger],\n",
    "        max_epochs=epoch_num, \n",
    "        callbacks=[checkpoint_callback],\n",
    "        accelerator = 'gpu'\n",
    "        )\n",
    "\n",
    "# сохраняем конфигурацию\n",
    "path_to_config = os.path.join(path_to_save_model_dir, 'training_config.yaml')\n",
    "with open(path_to_config, 'w', encoding='utf-8') as fd:\n",
    "    #json.dump(config_dict, fd, indent=4)\n",
    "    yaml.dump(config_dict, fd, indent=4)\n",
    "\n",
    "trainer.fit(segmentation_module , train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ea282e",
   "metadata": {},
   "source": [
    "# Черновики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5674eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.MAnet()\n",
    "model.encoder.conv1.stride=(1,1)\n",
    "\n",
    "def custom_forward(self, x):\n",
    "    \"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\n",
    "    features = self.encoder(x)\n",
    "    '''\n",
    "    for f in features:\n",
    "        print(f.shape)\n",
    "    print()\n",
    "    '''\n",
    "\n",
    "    decoder_output = self.decoder(features)\n",
    "    #print(decoder_output.shape)\n",
    "\n",
    "    masks = self.segmentation_head(decoder_output)\n",
    "\n",
    "    if self.classification_head is not None:\n",
    "        labels = self.classification_head(features[-1])\n",
    "        return masks, labels\n",
    "\n",
    "    return masks\n",
    "\n",
    "\n",
    "def decoder_custom_forward(self, features: List[torch.Tensor]) -> torch.Tensor:\n",
    "    features = features[1:]  # remove first skip with same spatial resolution\n",
    "    features = features[::-1]  # reverse channels to start from head of encoder\n",
    "\n",
    "    head = features[0]\n",
    "    skips = features[1:]\n",
    "    print('Features shapes')\n",
    "    print(f'head:{head.shape}')\n",
    "    print('Skips:')\n",
    "    for skip in skips:\n",
    "        print(skip.shape)\n",
    "\n",
    "    print('----------------------')\n",
    "\n",
    "    x = self.center(head)\n",
    "    print(f'Center:{x.shape}')\n",
    "\n",
    "    for i, decoder_block in enumerate(self.blocks):\n",
    "        skip = skips[i] if i < len(skips) else None\n",
    "        x = decoder_block(x, skip)\n",
    "        if skip is not None:\n",
    "            print(f'x:{x.shape}; skip:{skip.shape}')\n",
    "        else:\n",
    "            print(f'x:{x.shape}; skip:{skip}')\n",
    "\n",
    "    return x\n",
    "\n",
    "model.forward = types.MethodType(custom_forward, model)\n",
    "model.decoder.forward = types.MethodType(decoder_custom_forward, model.decoder)\n",
    "\n",
    "ret = model(torch.randn(1, 3, 96, 96))\n",
    "ret.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7f0022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "model = smp.Unet(encoder_name='resnet34')\n",
    "#input_weight = model.encoder._conv_stem.weight.detach()#.numpy()\n",
    "input_weight = model.encoder.conv1.weight.detach()#.numpy()\n",
    "interpolated_weight = F.interpolate(input_weight, size=(11, 11), mode='bicubic', antialias=True, align_corners=False)\n",
    "\n",
    "print(input_weight.shape)\n",
    "print(interpolated_weight.shape)\n",
    "\n",
    "filter_index = 9\n",
    "conv_filter = input_weight[filter_index]\n",
    "interpolated_conv_filter = interpolated_weight[filter_index]\n",
    "fig1, axs1 = plt.subplots(1,3)\n",
    "for idx, img in enumerate(conv_filter):\n",
    "    axs1[idx].imshow(img)\n",
    "\n",
    "fig2, axs2 = plt.subplots(1,3)\n",
    "for idx, img in enumerate(interpolated_conv_filter):\n",
    "    axs2[idx].imshow(img)\n",
    "\n",
    "\n",
    "filter_index += 1\n",
    "conv_filter = input_weight[filter_index]\n",
    "interpolated_conv_filter = interpolated_weight[filter_index]\n",
    "fig3, axs3 = plt.subplots(1,3)\n",
    "for idx, img in enumerate(conv_filter):\n",
    "    axs3[idx].imshow(img)\n",
    "\n",
    "fig4, axs4 = plt.subplots(1,3)\n",
    "for idx, img in enumerate(interpolated_conv_filter):\n",
    "    axs4[idx].imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "35c0200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [1, 2, 3, 4, 5, 6, 7, 8, 11, 12, 'ndvi', 'ndbi', 'ndwi', 'ndre']\n",
    "basic_indices = [1, 2, 3, 7]\n",
    "rest_indices = set(indices) - set(basic_indices)\n",
    "rest_indices = list(rest_indices)\n",
    "\n",
    "rest_indices = [x for x in rest_indices if isinstance(x, int)] + [x for x in rest_indices if isinstance(x, str)]\n",
    "#for combination in combinations()\n",
    "for k in range(len(rest_indices)):\n",
    "    k+=1\n",
    "    for combination_of_indices in combinations(rest_indices, k):\n",
    "        indices_to_test = basic_indices + list(combination_of_indices)\n",
    "        config_dict['multispecter_bands_indices'] = indices_to_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "2c05f400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6, 8, 11, 12, 'ndre', 'ndvi', 'ndwi', 'ndbi']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rest_indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330f7a52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
