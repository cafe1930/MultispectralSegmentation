{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0cede1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import tv_tensors\n",
    "from torchvision import models\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "from torchmetrics import classification\n",
    "from torchmetrics import segmentation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e94f93be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 1, 0, 0, 0],\n",
       "        [0, 2, 0, 0, 0],\n",
       "        [1, 0, 1, 0, 0],\n",
       "        [0, 0, 0, 0, 1],\n",
       "        [0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "confusion = classification.ConfusionMatrix(task='multiclass', num_classes=5)\n",
    "target = torch.tensor([2, 1, 0, 0])\n",
    "preds = torch.tensor([2, 1, 0, 1])\n",
    "confusion.update(preds, target)\n",
    "\n",
    "target = torch.tensor([3, 4, 2, 1])\n",
    "preds = torch.tensor([4, 3, 0, 1])\n",
    "confusion.update(preds, target)\n",
    "\n",
    "confusion.compute()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "84bf9960",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'torchmetrics' has no attribute 'segmentation'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mtorchmetrics\u001b[49m\u001b[43m.\u001b[49m\u001b[43msegmentation\u001b[49m\n",
      "\u001b[31mAttributeError\u001b[39m: module 'torchmetrics' has no attribute 'segmentation'"
     ]
    }
   ],
   "source": [
    "\n",
    "torchmetrics.segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "03c4d21c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 1., 0., 0.])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "iou = segmentation.DiceScore(average='none', num_classes=5, input_format='index')\n",
    "\n",
    "target = torch.tensor([[2, 1, 0, 1]])\n",
    "preds = torch.tensor([[2, 1, 0, 1]])\n",
    "iou.update(preds, target)\n",
    "\n",
    "target = torch.tensor([[3, 4, 2, 1]])\n",
    "preds = torch.tensor([[4, 3, 0, 1]])\n",
    "#iou.update(preds, target)\n",
    "iou.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "363572d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([3, 4, 2, 1])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target.reshape(-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e32608",
   "metadata": {},
   "source": [
    "# Работа с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "678fd044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "class SegmentationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path_to_dataset_root:str, samples_df:pd.DataFrame, channel_indices:list, transforms:v2._transform.Transform, device:torch.device):\n",
    "        '''\n",
    "        In:\n",
    "            path_to_dataset_root - путь до корневой папки с датасетом\n",
    "            samples_df - pandas.DataFrame с информацией о файлах\n",
    "            channel_indices - список с номерами каналов мультиспектрального изображения\n",
    "            transforms - аугментация изображений\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.path_to_dataset_root = path_to_dataset_root\n",
    "        self.samples_df = samples_df\n",
    "        self.channel_indices = channel_indices\n",
    "        self.transforms = transforms\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples_df.iloc[idx]\n",
    "\n",
    "        file_name = sample['file_name']\n",
    "\n",
    "        path_to_image = os.path.join(self.path_to_dataset_root, 'images', f'{file_name}.npy')\n",
    "        path_to_labels = os.path.join(self.path_to_dataset_root, 'labels', f'{file_name}.npy')\n",
    "\n",
    "        image = torch.as_tensor(np.load(path_to_image), dtype=torch.int16)[self.channel_indices]\n",
    "        #image = np.load(path_to_image)\n",
    "        # метки читаем как одноканальное изображение\n",
    "        label = np.load(path_to_labels)\n",
    "        label = np.where(label >= 0, label, 0)\n",
    "        #label = torch.as_tensor(np.load(path_to_labels), dtype=torch.uint8).long()\n",
    "        label = torch.as_tensor(label, dtype=torch.uint8).long()\n",
    "        \n",
    "        \n",
    "        image = tv_tensors.Image(image, device=self.device)\n",
    "        label = tv_tensors.Mask(label, device=self.device)\n",
    "\n",
    "        transforms_dict = {'image':image, 'mask':label}\n",
    "        transformed = self.transforms(transforms_dict)\n",
    "        return transformed['image'], transformed['mask']#, image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fac0331",
   "metadata": {},
   "source": [
    "# Описание нейронных сетей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "7082272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pred_mask(pred):\n",
    "    '''\n",
    "    Определение маски классов на основе сгенерированной softmax маски\n",
    "    '''\n",
    "    #pred = pred.detach()\n",
    "    _, pred_mask = pred.max(dim=1)\n",
    "    return pred_mask#.cpu().numpy()\n",
    "\n",
    "class SegmentationModule(L.LightningModule):\n",
    "    def __init__(self, model:nn.Module, criterion:nn.Module, optimizer_cfg:dict, metrics_dict:dict, name2class_idx_dict:dict) -> None:\n",
    "        '''\n",
    "        Модуль Lightning для обучения сегментационной сети\n",
    "        In:\n",
    "            model - нейронная сеть\n",
    "            criterion - функция потерь\n",
    "            \n",
    "            name2class_idx_dict - словарь с отображением {class_name(str): class_idx(int)}\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer_cfg = optimizer_cfg\n",
    "        self.metrics_dict = metrics_dict\n",
    "        \n",
    "        self.name2class_idx_dict = name2class_idx_dict\n",
    "        # словарь, выполняющий обратное отображение class_idx в class_name\n",
    "        self.class_idx2name_dict = {v:k for k, v in name2class_idx_dict.items()}\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self.optimizer_cfg['optmizer'](self.parameters(), **self.optimizer_cfg['optimizer_params'])\n",
    "        ret_dict = {'optimizer': optimizer}\n",
    "        if self.optimizer_cfg['lr_scheduler'] is not None:\n",
    "            scheduler = self.optimizer_cfg['lr_scheduler'](optimizer, **self.optimizer_cfg['lr_scheduler'])\n",
    "            ret_dict['lr_scheduler'] = scheduler\n",
    "        \n",
    "        return ret_dict\n",
    "\n",
    "    def compute_metrics(self, pred_labels, true_labels, mode):\n",
    "        metrics_names_list = self.metrics_dict[mode].keys()\n",
    "        for metric_name in metrics_names_list:\n",
    "            if 'dice' in metric_name.lower():\n",
    "                self.metrics_dict[mode][metric_name].update(pred_labels, true_labels)\n",
    "            else:\n",
    "                self.metrics_dict[mode][metric_name].update(pred_labels.reshape(-1), true_labels.reshape(-1))\n",
    "        \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, true_labels = batch\n",
    "        pred = self.model(data)\n",
    "        loss = self.criterion(pred, true_labels)\n",
    "        # вычисление сгенерированной маски\n",
    "        pred_labels = compute_pred_mask(pred)\n",
    "        #true_labels = true_labels.detach().cpu().numpy()\n",
    "        \n",
    "        self.compute_metrics(pred_labels=pred_labels, true_labels=true_labels, mode='train')\n",
    "\n",
    "        # т.к. мы вычисляем общую ошибку на всей эпохе, то записываем в лог только значение функции потерь\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data, true_labels = batch\n",
    "        pred = self.model(data)\n",
    "        loss = self.criterion(pred, true_labels)\n",
    "        pred_labels = compute_pred_mask(pred)\n",
    "        self.compute_metrics(pred_labels=pred_labels, true_labels=true_labels, mode='val')\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def log_metrics(self, mode):\n",
    "        for metric_name, metric in self.metrics_dict[mode].items():\n",
    "            metric_val = metric.compute()\n",
    "            if 'confusion' in metric_name.lower():\n",
    "                disp_name = f'{mode}_{metric_name}'\n",
    "                self.log(disp_name, metric_val, on_step=False, on_epoch=True, prog_bar=False)\n",
    "            else:\n",
    "                for i, value in enumerate(metric_val):\n",
    "                    class_name = self.class_idx2name_dict[i]\n",
    "                    disp_name = f'{mode}_{metric_name}_{class_name}'\n",
    "                    self.log(disp_name, value, on_step=False, on_epoch=True, prog_bar=True)\n",
    "                disp_name = f'{mode}_{metric_name}_mean'\n",
    "                self.log(disp_name, metric_val.mean(), on_step=False, on_epoch=True, prog_bar=True)\n",
    "            self.metrics_dict[mode][metric_name].reset()\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        '''\n",
    "        Декодирование результатов тренировочной эпохи и запись их в лог\n",
    "        '''\n",
    "        self.log_metrics(mode='train')\n",
    " \n",
    "    def on_validation_epoch_end(self):\n",
    "        '''\n",
    "        Декодирование результатов тестовой эпохи и запись их в лог\n",
    "        (работает точно также, как и )\n",
    "        '''\n",
    "        self.log_metrics(mode='val')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "617f9a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(0.47513042695143004)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lst = [0.8390797972679138,0.6945005655288696,0.28147169947624207,0.16609252989292145,0.36803194880485535,0.7777265906333923,0.0514686144888401,0.4750562012195587,0.5050733685493469,0.26440125703811646,0.8035321235656738]\n",
    "np.mean(lst)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4035171",
   "metadata": {},
   "source": [
    "# Фабрики для создания моделей по конфигурациям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1960f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(config_dict, segmentation_nns_factory_dict):\n",
    "    model_name = config_dict['segmentation_nn']['nn_architecture']\n",
    "    # создаем нейронную сеть из фабрики\n",
    "    model = segmentation_nns_factory_dict[model_name](**config_dict['segmentation_nn']['params'])\n",
    "    multispecter_bands_indices = config_dict['multispecter_bands_indices']\n",
    "    in_channels = len(multispecter_bands_indices)\n",
    "    # замена входного слоя, если кол-во каналов изображения не равно трем\n",
    "    if in_channels != 3:\n",
    "        # получаем входной слой, специфический для конкретной нейронной сети\n",
    "        input_conv = model.get_submodule(\n",
    "            config_dict['segmentation_nn']['input_layer_params']['layer_path']\n",
    "            )\n",
    "        new_input_conv = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=input_conv.out_channels,\n",
    "            kernel_size=input_conv.kernel_size,\n",
    "            #stride=conv1.stride,\n",
    "            stride=config_dict['segmentation_nn']['input_layer_params']['stride'],\n",
    "            #padding=conv1.padding,\n",
    "            padding=config_dict['segmentation_nn']['input_layer_params']['padding'],\n",
    "            dilation=input_conv.dilation,\n",
    "            groups=input_conv.groups,\n",
    "            bias=input_conv.bias is not None\n",
    "        )\n",
    "        if config_dict['segmentation_nn']['params']['encoder_weights'] is not None:\n",
    "            new_weight = torch.cat([input_conv.weight.mean(dim=1).unsqueeze(1)]*in_channels, dim=1)\n",
    "            input_conv.weight = nn.Parameter(new_weight)\n",
    "            if input_conv.bias is not None:\n",
    "                new_input_conv.bias = input_conv.bias\n",
    "        # перезаписываем входной слой исходя из специфики оригинальной сети\n",
    "        model.set_submodule(\n",
    "            config_dict['segmentation_nn']['input_layer_params']['layer_path'],\n",
    "            new_input_conv\n",
    "            )\n",
    "        \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4680fb11",
   "metadata": {},
   "source": [
    "# Конфигурации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "2291919a",
   "metadata": {},
   "outputs": [],
   "source": [
    "segmentation_nns_factory_dict = {\n",
    "    'unet': smp.Unet,\n",
    "    'fpn': smp.FPN,\n",
    "}\n",
    "\n",
    "criterion_factory_dict = {\n",
    "    'crossentropy': nn.CrossEntropyLoss,\n",
    "}\n",
    "\n",
    "\n",
    "config_dict = {\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'fpn',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': (256, 128, 64, 32, 16),\n",
    "            'decoder_attention_type': None,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_params': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'stride': (2, 2),\n",
    "            'padding': (1, 1),\n",
    "            'layers_num': 1,\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [1, 2, 3, 4, 5, 6, 7, ],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        'params': {'weight': 'classes'}\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\MULTISPECTRAL_DATA_FOR_TRAINIG_NEW'\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d8ee3a",
   "metadata": {},
   "source": [
    "# Создание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "5037b82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 7, 96, 96]) torch.Size([16, 11, 96, 96])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'fpn_efficientnet-b0 2025-09-04-18-27-31'"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_dataset_root = config_dict['path_to_dataset_root']\n",
    "\n",
    "path_to_dataset_info_csv = os.path.join(path_to_dataset_root, 'data_info_table.csv')\n",
    "path_to_surface_classes_json = os.path.join(path_to_dataset_root, 'surface_classes.json')\n",
    "\n",
    "input_image_size = config_dict['input_image_size']\n",
    "multispecter_bands_indices = config_dict['multispecter_bands_indices']\n",
    "device = config_dict['device']\n",
    "\n",
    "# чтение списка имен классов поверхностей\n",
    "with open(path_to_surface_classes_json) as fd:\n",
    "    surface_classes_list = json.load(fd)\n",
    "# чтение таблицы с информацией о каждом изображении в выборке\n",
    "images_df = pd.read_csv(path_to_dataset_info_csv)\n",
    "\n",
    "path_to_partition_json = os.path.join(path_to_dataset_root, 'dataset_partition.json')\n",
    "# чтение словаря со списками квадратов, находящихся в обучающей и тестовой выборке\n",
    "with open(path_to_partition_json) as fd:\n",
    "    partition_dict = json.load(fd)\n",
    "\n",
    "# формирование pandas DataFrame-ов с информацией об изображениях обучающей и тестовой выборках\n",
    "train_images_df = []\n",
    "for train_square in partition_dict['train_squares']:\n",
    "    train_images_df.append(images_df[images_df['square_id']==train_square])\n",
    "train_images_df = pd.concat(train_images_df, ignore_index=True)\n",
    "\n",
    "test_images_df = []\n",
    "for test_square in partition_dict['test_squares']:\n",
    "    test_images_df.append(images_df[images_df['square_id']==test_square])\n",
    "test_images_df = pd.concat(test_images_df, ignore_index=True)\n",
    "\n",
    "#train_images_df, test_images_df = train_test_split(images_df, test_size=0.3, random_state=0)\n",
    "\n",
    "class_num = images_df['class_num'].iloc[0]\n",
    "\n",
    "# формирование словаря, отображающейго имя класса поверхности в индекс класса\n",
    "class_name2idx_dict = {n:i for i, n in enumerate(surface_classes_list)}\n",
    "\n",
    "# вычисление распределений пикселей в классах поверхностей \n",
    "classes_pixels_distribution_df = images_df[surface_classes_list]\n",
    "classes_pixels_num = classes_pixels_distribution_df.sum()\n",
    "classes_weights = classes_pixels_num / classes_pixels_num.sum()\n",
    "classes_weights = classes_weights[surface_classes_list].to_numpy().astype(np.float32)\n",
    "\n",
    "input_image_size = config_dict['input_image_size']\n",
    "train_transforms = v2.Compose(\n",
    "    [v2.Resize((input_image_size,input_image_size), antialias=True),v2.ToDtype(torch.float32, scale=True)])\n",
    "test_transforms = v2.Compose(\n",
    "    [v2.Resize((input_image_size,input_image_size), antialias=True),v2.ToDtype(torch.float32, scale=True)])\n",
    "\n",
    "# если ф-ция потерь перекрестная энтропия, то проверяем, есть ли там веса классов\n",
    "if config_dict['loss']['type'] == 'crossentropy':\n",
    "    # если в параметрах функции потерь стоит строка 'classes', надо передать в функцию вектор весов классов\n",
    "    if 'weight' in config_dict['loss']['params']:\n",
    "        if config_dict['loss']['params']['weight'] == 'classes':\n",
    "            config_dict['loss']['params']['weight'] = torch.tensor(classes_weights)\n",
    "\n",
    "# создание функции потерь\n",
    "criterion = criterion_factory_dict[config_dict['loss']['type']](**config_dict['loss']['params'])\n",
    "\n",
    "model = create_model(config_dict, segmentation_nns_factory_dict)\n",
    "model = model.to(device)\n",
    "\n",
    "# создаем датасеты и даталоадеры\n",
    "train_dataset = SegmentationDataset(path_to_dataset_root=path_to_dataset_root, samples_df=train_images_df, channel_indices=multispecter_bands_indices, transforms=train_transforms, device=device)\n",
    "test_dataset = SegmentationDataset(path_to_dataset_root=path_to_dataset_root, samples_df=test_images_df, channel_indices=multispecter_bands_indices, transforms=test_transforms, device=device)\n",
    "#train_dataset = SegmentationDatasetApplSurf(path_to_dataset_root=path_to_dataset_root, samples_df=test_images_df, channel_indices=channel_indices, name2class_idx_dict=class_name2idx_dict, applicable_surfaces_dict=applicable_surfaces_dict, transforms=test_transforms, device=device)\n",
    "#test_dataset = SegmentationDatasetApplSurf(path_to_dataset_root=path_to_dataset_root, samples_df=test_images_df, channel_indices=channel_indices, name2class_idx_dict=class_name2idx_dict, applicable_surfaces_dict=applicable_surfaces_dict, transforms=test_transforms, device=device)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# тестовое чтение данных\n",
    "for data, labels in test_loader:\n",
    "    break\n",
    "    pred = model(data)\n",
    "    loss = criterion(pred, labels)\n",
    "\n",
    "# тестовая обработка данных нейронной сетью\n",
    "ret = model(data)\n",
    "print(data.shape, ret.shape)\n",
    "\n",
    "createion_time_str = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "nn_arch_str = config_dict[\"segmentation_nn\"][\"nn_architecture\"]\n",
    "nn_encoder_str = config_dict[\"segmentation_nn\"][\"params\"][\"encoder_name\"]\n",
    "model_name = f'{nn_arch_str}_{nn_encoder_str} {createion_time_str}'\n",
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f4c339",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "9e6b7cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############################\n",
      "unet_efficientnet-b0 2025-09-03-19-54-39\n",
      "#############################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\admin\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:654: Checkpoint directory C:\\Users\\admin\\python_programming\\MultispectralSegmentation\\saving_dir\\unet_efficientnet-b0 2025-09-03-19-54-39 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | Unet             | 6.3 M  | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "6.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.3 M     Total params\n",
      "25.016    Total estimated model params size (MB)\n",
      "324       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 88/88 [01:01<00:00,  1.44it/s, v_num=0, val_loss=1.650, val_iou_UNLABELED=0.785, val_iou_buildings_territory=0.662, val_iou_natural_ground=0.0957, val_iou_natural_grow=0.114, val_iou_natural_wetland=0.227, val_iou_natural_wood=0.533, val_iou_quasi_natural_ground=0.0302, val_iou_quasi_natural_grow=0.421, val_iou_quasi_natural_wetland=0.0479, val_iou_transport=0.139, val_iou_water=0.812, val_iou_mean=0.352, val_precision_UNLABELED=0.923, val_precision_buildings_territory=0.845, val_precision_natural_ground=0.101, val_precision_natural_grow=0.408, val_precision_natural_wetland=0.260, val_precision_natural_wood=0.854, val_precision_quasi_natural_ground=0.040, val_precision_quasi_natural_grow=0.580, val_precision_quasi_natural_wetland=0.0489, val_precision_transport=0.646, val_precision_water=0.906, val_precision_mean=0.510, val_recall_UNLABELED=0.923, val_recall_buildings_territory=0.845, val_recall_natural_ground=0.101, val_recall_natural_grow=0.408, val_recall_natural_wetland=0.260, val_recall_natural_wood=0.854, val_recall_quasi_natural_ground=0.040, val_recall_quasi_natural_grow=0.580, val_recall_quasi_natural_wetland=0.0489, val_recall_transport=0.646, val_recall_water=0.906, val_recall_mean=0.510, train_loss=0.169, train_iou_UNLABELED=0.908, train_iou_buildings_territory=0.909, train_iou_natural_ground=0.734, train_iou_natural_grow=0.817, train_iou_natural_wetland=0.895, train_iou_natural_wood=0.936, train_iou_quasi_natural_ground=0.847, train_iou_quasi_natural_grow=0.924, train_iou_quasi_natural_wetland=0.910, train_iou_transport=0.580, train_iou_water=0.852, train_iou_mean=0.847, train_precision_UNLABELED=0.964, train_precision_buildings_territory=0.946, train_precision_natural_ground=0.871, train_precision_natural_grow=0.903, train_precision_natural_wetland=0.943, train_precision_natural_wood=0.963, train_precision_quasi_natural_ground=0.912, train_precision_quasi_natural_grow=0.958, train_precision_quasi_natural_wetland=0.932, train_precision_transport=0.761, train_precision_water=0.939, train_precision_mean=0.917, train_recall_UNLABELED=0.964, train_recall_buildings_territory=0.946, train_recall_natural_ground=0.871, train_recall_natural_grow=0.903, train_recall_natural_wetland=0.943, train_recall_natural_wood=0.963, train_recall_quasi_natural_ground=0.912, train_recall_quasi_natural_grow=0.958, train_recall_quasi_natural_wetland=0.932, train_recall_transport=0.761, train_recall_water=0.939, train_recall_mean=0.917]                        "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 88/88 [01:01<00:00,  1.44it/s, v_num=0, val_loss=1.650, val_iou_UNLABELED=0.785, val_iou_buildings_territory=0.662, val_iou_natural_ground=0.0957, val_iou_natural_grow=0.114, val_iou_natural_wetland=0.227, val_iou_natural_wood=0.533, val_iou_quasi_natural_ground=0.0302, val_iou_quasi_natural_grow=0.421, val_iou_quasi_natural_wetland=0.0479, val_iou_transport=0.139, val_iou_water=0.812, val_iou_mean=0.352, val_precision_UNLABELED=0.923, val_precision_buildings_territory=0.845, val_precision_natural_ground=0.101, val_precision_natural_grow=0.408, val_precision_natural_wetland=0.260, val_precision_natural_wood=0.854, val_precision_quasi_natural_ground=0.040, val_precision_quasi_natural_grow=0.580, val_precision_quasi_natural_wetland=0.0489, val_precision_transport=0.646, val_precision_water=0.906, val_precision_mean=0.510, val_recall_UNLABELED=0.923, val_recall_buildings_territory=0.845, val_recall_natural_ground=0.101, val_recall_natural_grow=0.408, val_recall_natural_wetland=0.260, val_recall_natural_wood=0.854, val_recall_quasi_natural_ground=0.040, val_recall_quasi_natural_grow=0.580, val_recall_quasi_natural_wetland=0.0489, val_recall_transport=0.646, val_recall_water=0.906, val_recall_mean=0.510, train_loss=0.169, train_iou_UNLABELED=0.908, train_iou_buildings_territory=0.909, train_iou_natural_ground=0.734, train_iou_natural_grow=0.817, train_iou_natural_wetland=0.895, train_iou_natural_wood=0.936, train_iou_quasi_natural_ground=0.847, train_iou_quasi_natural_grow=0.924, train_iou_quasi_natural_wetland=0.910, train_iou_transport=0.580, train_iou_water=0.852, train_iou_mean=0.847, train_precision_UNLABELED=0.964, train_precision_buildings_territory=0.946, train_precision_natural_ground=0.871, train_precision_natural_grow=0.903, train_precision_natural_wetland=0.943, train_precision_natural_wood=0.963, train_precision_quasi_natural_ground=0.912, train_precision_quasi_natural_grow=0.958, train_precision_quasi_natural_wetland=0.932, train_precision_transport=0.761, train_precision_water=0.939, train_precision_mean=0.917, train_recall_UNLABELED=0.964, train_recall_buildings_territory=0.946, train_recall_natural_ground=0.871, train_recall_natural_grow=0.903, train_recall_natural_wetland=0.943, train_recall_natural_wood=0.963, train_recall_quasi_natural_ground=0.912, train_recall_quasi_natural_grow=0.958, train_recall_quasi_natural_wetland=0.932, train_recall_transport=0.761, train_recall_water=0.939, train_recall_mean=0.917]\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 100\n",
    "\n",
    "print('#############################')\n",
    "print(model_name)\n",
    "print('#############################')\n",
    "print()\n",
    "\n",
    "# создаем список словарей с информацией о вычисляемых метриках с помощью multiclass confusion matrix\n",
    "# см. подробнее ддокументацию к функции compute_metric_from_confusion\n",
    "metrics_dict = {\n",
    "    'train': {\n",
    "        'iou': classification.JaccardIndex(task='multiclass', average='none', num_classes=len(class_name2idx_dict)).to(device),\n",
    "        'precision': classification.Precision(task='multiclass', average='none', num_classes=len(class_name2idx_dict)).to(device),\n",
    "        'recall': classification.Precision(task='multiclass', average='none', num_classes=len(class_name2idx_dict)).to(device),\n",
    "    },\n",
    "    'val': {\n",
    "        'iou': classification.JaccardIndex(task='multiclass', average='none', num_classes=len(class_name2idx_dict)).to(device),\n",
    "        'precision': classification.Precision(task='multiclass', average='none', num_classes=len(class_name2idx_dict)).to(device),\n",
    "        'recall': classification.Precision(task='multiclass', average='none', num_classes=len(class_name2idx_dict)).to(device),\n",
    "    }\n",
    "}\n",
    "\n",
    "optimizer_cfg = {\n",
    "    'optmizer': torch.optim.Adam,\n",
    "    'optimizer_params':{},\n",
    "    'lr_scheduler': None,\n",
    "    'lr_scheduler_params': {},\n",
    "\n",
    "}\n",
    "\n",
    "# Создаем модуль Lightning\n",
    "segmentation_module = SegmentationModule(model, criterion, optimizer_cfg, metrics_dict, class_name2idx_dict)\n",
    "\n",
    "# задаем путь до папки с логгерами и создаем логгер, записывающий результаты в csv\n",
    "path_to_saving_dir = 'saving_dir'\n",
    "logger = CSVLogger(\n",
    "    save_dir = path_to_saving_dir,\n",
    "    name=model_name, \n",
    "    flush_logs_every_n_steps=1,\n",
    "    )\n",
    "\n",
    "# создаем объект, записывающий в чекпоинт лучшую модель\n",
    "path_to_save_model_dir = os.path.join(path_to_saving_dir, model_name)\n",
    "os.makedirs(path_to_save_model_dir, exist_ok=True)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    mode=\"max\",\n",
    "    filename=model_name+\"-{epoch:02d}-{val_iou_mean:.3}\",\n",
    "    dirpath=path_to_save_model_dir, \n",
    "    save_top_k=1, monitor=\"val_iou_mean\"\n",
    "    )\n",
    "\n",
    "trainer = L.Trainer(logger=logger,\n",
    "        max_epochs=epoch_num, \n",
    "        callbacks=[checkpoint_callback],\n",
    "        accelerator = 'gpu'\n",
    "        )\n",
    "\n",
    "# сохраняем конфигурацию\n",
    "path_to_config = os.path.join(path_to_save_model_dir, 'config.json')\n",
    "with open(path_to_config, 'w', encoding='utf-8') as fd:\n",
    "    json.dump(config_dict, fd, indent=4)\n",
    "\n",
    "trainer.fit(segmentation_module , train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12570f8b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e871bfdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cd5d12d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
