{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f0cede1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import json\n",
    "import yaml\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import tv_tensors\n",
    "from torchvision import models\n",
    "\n",
    "import types\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.base import (\n",
    "    ClassificationHead,\n",
    "    SegmentationHead,\n",
    "    SegmentationModel,\n",
    ")\n",
    "\n",
    "from segmentation_models_pytorch.base import modules as md\n",
    "\n",
    "from segmentation_models_pytorch.encoders import get_encoder\n",
    "from segmentation_models_pytorch.base.hub_mixin import supports_config_loading\n",
    "from typing import Any, Dict, Optional, Union, Callable, Sequence, List, Literal\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger, Logger\n",
    "\n",
    "from torchmetrics import classification\n",
    "from torchmetrics import segmentation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import einops as eo\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e32608",
   "metadata": {},
   "source": [
    "# Работа с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "678fd044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "class SegmentationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path_to_dataset_root:str, samples_df:pd.DataFrame, channel_indices:list, transforms:v2._transform.Transform, dtype:torch.dtype, device:torch.device):\n",
    "        '''\n",
    "        In:\n",
    "            path_to_dataset_root - путь до корневой папки с датасетом\n",
    "            samples_df - pandas.DataFrame с информацией о файлах\n",
    "            channel_indices - список с номерами каналов мультиспектрального изображения\n",
    "            transforms - аугментация изображений\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.path_to_dataset_root = path_to_dataset_root\n",
    "        self.samples_df = samples_df\n",
    "        self.specter_bands_list = [i for i in channel_indices if isinstance(i, int)]\n",
    "        self.specter_indices_names = [s for s in channel_indices if isinstance(s, str)]\n",
    "        self.dtype_trasform = v2.ToDtype(dtype=dtype, scale=True)\n",
    "        self.other_transforms = transforms\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples_df)\n",
    "    @staticmethod\n",
    "    def compute_spectral_index(index_name, image):\n",
    "        if index_name.lower() == 'ndvi':\n",
    "            b0 = image[7] # NIR, B8\n",
    "            b1 = image[3] # RED, B4\n",
    "            \n",
    "        elif index_name.lower() == 'ndbi':\n",
    "            b0 = image[10] #SWIR, B11\n",
    "            b1 = image[7] #NIR, B8\n",
    "\n",
    "        elif index_name.lower() == 'ndwi':\n",
    "            b0 = image[2] #green, B3\n",
    "            b1 = image[7] #NIR, B8\n",
    "\n",
    "        elif index_name.lower() == 'ndre':\n",
    "            b0 = image[7] #NIR, B8\n",
    "            b1 = image[5] #Red Edge, B6\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            index = (b0 - b1)/(b0 + b1)\n",
    "            \n",
    "        index = np.nan_to_num(index, nan=-5)\n",
    "\n",
    "        return index    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples_df.iloc[idx]\n",
    "\n",
    "        file_name = sample['file_name']\n",
    "\n",
    "        path_to_image = os.path.join(self.path_to_dataset_root, 'images', f'{file_name}.npy')\n",
    "        path_to_labels = os.path.join(self.path_to_dataset_root, 'labels', f'{file_name}.npy')\n",
    "\n",
    "        image = np.load(path_to_image)\n",
    "        spectral_indices = []\n",
    "        # вычисляем спектральные индексы\n",
    "        if len(self.specter_indices_names) > 0:\n",
    "            for sp_index_name in self.specter_indices_names:\n",
    "                spectral_index = self.compute_spectral_index(sp_index_name, image)\n",
    "                spectral_index = torch.as_tensor(spectral_index)\n",
    "                spectral_indices.append(spectral_index.unsqueeze(0))\n",
    "\n",
    "            spectral_indices = torch.cat(spectral_indices)\n",
    "            spectral_indices = self.dtype_trasform(spectral_indices)\n",
    "\n",
    "        image = torch.as_tensor(image[self.specter_bands_list], dtype=torch.int16)\n",
    "        image = self.dtype_trasform(image)\n",
    "        # добавляем спектральные индексы\n",
    "        if len(self.specter_indices_names) > 0:\n",
    "            image = torch.cat([image, spectral_indices], dim=0) \n",
    "        #image = np.load(path_to_image)\n",
    "        # метки читаем как одноканальное изображение\n",
    "        label = np.load(path_to_labels)\n",
    "        label = np.where(label >= 0, label, 0)\n",
    "        #label = torch.as_tensor(np.load(path_to_labels), dtype=torch.uint8).long()\n",
    "        label = torch.as_tensor(label, dtype=torch.uint8).long()\n",
    "        \n",
    "        image = tv_tensors.Image(image, device=self.device)\n",
    "        label = tv_tensors.Mask(label, device=self.device)\n",
    "\n",
    "        transforms_dict = {'image':image, 'mask':label}\n",
    "        transformed = self.other_transforms(transforms_dict)\n",
    "        return transformed['image'], transformed['mask']#, image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fac0331",
   "metadata": {},
   "source": [
    "# Описание модуля Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7082272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pred_mask(pred):\n",
    "    '''\n",
    "    Определение маски классов на основе сгенерированной softmax маски\n",
    "    '''\n",
    "    #pred = pred.detach()\n",
    "    _, pred_mask = pred.max(dim=1)\n",
    "    return pred_mask#.cpu().numpy()\n",
    "\n",
    "class LightningSegmentationModule(L.LightningModule):\n",
    "    def __init__(self, model:nn.Module, criterion:nn.Module, optimizer_cfg:dict, metrics_dict:dict, name2class_idx_dict:dict) -> None:\n",
    "        '''\n",
    "        Модуль Lightning для обучения сегментационной сети\n",
    "        In:\n",
    "            model - нейронная сеть\n",
    "            criterion - функция потерь\n",
    "            \n",
    "            name2class_idx_dict - словарь с отображением {class_name(str): class_idx(int)}\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer_cfg = optimizer_cfg\n",
    "        self.metrics_dict = metrics_dict\n",
    "        \n",
    "        self.name2class_idx_dict = name2class_idx_dict\n",
    "        # словарь, выполняющий обратное отображение class_idx в class_name\n",
    "        self.class_idx2name_dict = {v:k for k, v in name2class_idx_dict.items()}\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self.optimizer_cfg['optmizer'](self.parameters(), **self.optimizer_cfg['optimizer_args'])\n",
    "        ret_dict = {'optimizer': optimizer}\n",
    "        if self.optimizer_cfg['lr_scheduler'] is not None:\n",
    "            scheduler = self.optimizer_cfg['lr_scheduler'](optimizer, **self.optimizer_cfg['lr_scheduler_args'])\n",
    "            ret_dict['lr_scheduler'] = {'scheduler': scheduler}\n",
    "            ret_dict['lr_scheduler'].update(self.optimizer_cfg['lr_scheduler_params'])\n",
    "        \n",
    "        return ret_dict\n",
    "\n",
    "    def compute_metrics(self, pred_labels, true_labels, mode):\n",
    "        metrics_names_list = self.metrics_dict[mode].keys()\n",
    "        for metric_name in metrics_names_list:\n",
    "            if 'dice' in metric_name.lower():\n",
    "                self.metrics_dict[mode][metric_name].update(pred_labels, true_labels)\n",
    "            else:\n",
    "                self.metrics_dict[mode][metric_name].update(pred_labels.reshape(-1), true_labels.reshape(-1))\n",
    "        \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, true_labels = batch\n",
    "        pred = self.model(data)\n",
    "        loss = self.criterion(pred, true_labels)\n",
    "        # вычисление сгенерированной маски\n",
    "        pred_labels = compute_pred_mask(pred)\n",
    "        #true_labels = true_labels.detach().cpu().numpy()\n",
    "        \n",
    "        self.compute_metrics(pred_labels=pred_labels, true_labels=true_labels, mode='train')\n",
    "\n",
    "        # т.к. мы вычисляем общую ошибку на всей эпохе, то записываем в лог только значение функции потерь\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data, true_labels = batch\n",
    "        pred = self.model(data)\n",
    "        loss = self.criterion(pred, true_labels)\n",
    "        pred_labels = compute_pred_mask(pred)\n",
    "        self.compute_metrics(pred_labels=pred_labels, true_labels=true_labels, mode='val')\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def log_metrics(self, mode):\n",
    "        for metric_name, metric in self.metrics_dict[mode].items():\n",
    "            metric_val = metric.compute()\n",
    "            if 'confusion' in metric_name.lower():\n",
    "                disp_name = f'{mode}_{metric_name}'\n",
    "                self.log(disp_name, metric_val.cpu().tolist(), on_step=False, on_epoch=True, prog_bar=False)\n",
    "            else:\n",
    "                for i, value in enumerate(metric_val):\n",
    "                    class_name = self.class_idx2name_dict[i]\n",
    "                    disp_name = f'{mode}_{metric_name}_{class_name}'\n",
    "                    self.log(disp_name, value, on_step=False, on_epoch=True, prog_bar=True)\n",
    "                disp_name = f'{mode}_{metric_name}_mean'\n",
    "                self.log(disp_name, metric_val.mean(), on_step=False, on_epoch=True, prog_bar=True)\n",
    "            self.metrics_dict[mode][metric_name].reset()\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        '''\n",
    "        Декодирование результатов тренировочной эпохи и запись их в лог\n",
    "        '''\n",
    "        self.log_metrics(mode='train')\n",
    " \n",
    "    def on_validation_epoch_end(self):\n",
    "        '''\n",
    "        Декодирование результатов тестовой эпохи и запись их в лог\n",
    "        (работает точно также, как и )\n",
    "        '''\n",
    "        self.log_metrics(mode='val')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b4de78",
   "metadata": {},
   "source": [
    "# Новые функции потерь (Dice-Crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5889a22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceCELoss(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            ce_weight,\n",
    "            ce_ignore_index,\n",
    "            ce_reducion,\n",
    "            ce_label_smoothing,\n",
    "            dice_mode,\n",
    "            dice_classes,\n",
    "            dice_log_loss,\n",
    "            dice_from_logits,\n",
    "            dice_smooth,\n",
    "            dice_ignore_index,\n",
    "            dice_eps,\n",
    "            losses_weight: List = [0.5, 0.5],\n",
    "            is_trainable_weights: bool = False,\n",
    "            weights_processing_type: str = None,\n",
    "            ):\n",
    "        super().__init__()\n",
    "        self.dice = smp.losses.DiceLoss(\n",
    "            mode=dice_mode,\n",
    "            classes=dice_classes,\n",
    "            log_loss=dice_log_loss,\n",
    "            from_logits=dice_from_logits,\n",
    "            smooth=dice_smooth,\n",
    "            ignore_index=dice_ignore_index,\n",
    "            eps=dice_eps\n",
    "            )\n",
    "        self.ce = nn.CrossEntropyLoss(\n",
    "            weight=ce_weight,\n",
    "            ignore_index=ce_ignore_index,\n",
    "            reduction=ce_reducion,\n",
    "            label_smoothing=ce_label_smoothing,\n",
    "        )\n",
    "        self.loss_weights = torch.tensor(losses_weight)\n",
    "        if is_trainable_weights:\n",
    "            self.loss_weights = nn.Parameter(self.loss_weights)\n",
    "        self.weights_processing_type = weights_processing_type\n",
    "\n",
    "    def forward(self, pred, true):\n",
    "        weights = self.loss_weights\n",
    "        if self.weights_processing_type == 'softmax':\n",
    "            weights = weights.softmax(dim=0)\n",
    "        elif self.weights_processing_type == 'sigmoid':\n",
    "            weights = weights.softmax(dim=0)\n",
    "\n",
    "        ce_loss = self.ce(pred, true) * weights[0]\n",
    "        dice_loss = self.dice(pred, true) * weights[1]\n",
    "        return ce_loss + dice_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaa745e",
   "metadata": {},
   "source": [
    "# Адаптация UNet++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "7124811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetppDecoderBlockMod(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        skip_channels: int,\n",
    "        out_channels: int,\n",
    "        use_norm: Union[bool, str, Dict[str, Any]] = \"batchnorm\",\n",
    "        attention_type: Optional[str] = None,\n",
    "        interpolation_mode: str = \"nearest\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = md.Conv2dReLU(\n",
    "            in_channels + skip_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_norm=use_norm,\n",
    "        )\n",
    "        self.attention1 = md.Attention(\n",
    "            attention_type, in_channels=in_channels + skip_channels\n",
    "        )\n",
    "        self.conv2 = md.Conv2dReLU(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_norm=use_norm,\n",
    "        )\n",
    "        self.attention2 = md.Attention(attention_type, in_channels=out_channels)\n",
    "        self.interpolation_mode = interpolation_mode\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, skip: Optional[torch.Tensor] = None, scale_factor: float = 2.0\n",
    "    ) -> torch.Tensor:\n",
    "        if scale_factor != 1:\n",
    "            x = F.interpolate(x, scale_factor=scale_factor, mode=self.interpolation_mode)\n",
    "        if skip is not None:\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "            x = self.attention1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.attention2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UnetppCenterBlock(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        use_norm: Union[bool, str, Dict[str, Any]] = \"batchnorm\",\n",
    "    ):\n",
    "        conv1 = md.Conv2dReLU(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_norm=use_norm,\n",
    "        )\n",
    "        conv2 = md.Conv2dReLU(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_norm=use_norm,\n",
    "        )\n",
    "        super().__init__(conv1, conv2)\n",
    "\n",
    "\n",
    "class UnetPlusPlusDecoderMod(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_channels: Sequence[int],\n",
    "        decoder_channels: Sequence[int],\n",
    "        n_blocks: int = 5,\n",
    "        use_norm: Union[bool, str, Dict[str, Any]] = \"batchnorm\",\n",
    "        attention_type: Optional[str] = None,\n",
    "        interpolation_mode: str = \"nearest\",\n",
    "        center: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if n_blocks != len(decoder_channels):\n",
    "            raise ValueError(\n",
    "                f\"Model depth is {n_blocks}, but you provide `decoder_channels` for {len(decoder_channels)} blocks.\"\n",
    "            )\n",
    "\n",
    "        # remove first skip with same spatial resolution\n",
    "        encoder_channels = encoder_channels[1:]\n",
    "        # reverse channels to start from head of encoder\n",
    "        encoder_channels = encoder_channels[::-1]\n",
    "\n",
    "        # computing blocks input and output channels\n",
    "        head_channels = encoder_channels[0]\n",
    "        self.in_channels = [head_channels] + list(decoder_channels[:-1])\n",
    "        self.skip_channels = list(encoder_channels[1:]) + [0]\n",
    "        self.out_channels = decoder_channels\n",
    "        if center:\n",
    "            self.center = UnetppCenterBlock(\n",
    "                head_channels,\n",
    "                head_channels,\n",
    "                use_norm=use_norm,\n",
    "            )\n",
    "        else:\n",
    "            self.center = nn.Identity()\n",
    "\n",
    "        # combine decoder keyword arguments\n",
    "        kwargs = dict(\n",
    "            use_norm=use_norm,\n",
    "            attention_type=attention_type,\n",
    "            interpolation_mode=interpolation_mode,\n",
    "        )\n",
    "\n",
    "        blocks = {}\n",
    "        for layer_idx in range(len(self.in_channels) - 1):\n",
    "            for depth_idx in range(layer_idx + 1):\n",
    "                if depth_idx == 0:\n",
    "                    in_ch = self.in_channels[layer_idx]\n",
    "                    skip_ch = self.skip_channels[layer_idx] * (layer_idx + 1)\n",
    "                    out_ch = self.out_channels[layer_idx]\n",
    "                else:\n",
    "                    out_ch = self.skip_channels[layer_idx]\n",
    "                    skip_ch = self.skip_channels[layer_idx] * (\n",
    "                        layer_idx + 1 - depth_idx\n",
    "                    )\n",
    "                    in_ch = self.skip_channels[layer_idx - 1]\n",
    "                blocks[f\"x_{depth_idx}_{layer_idx}\"] = UnetppDecoderBlockMod(\n",
    "                    in_ch, skip_ch, out_ch, **kwargs\n",
    "                )\n",
    "        blocks[f\"x_{0}_{len(self.in_channels) - 1}\"] = UnetppDecoderBlockMod(\n",
    "            self.in_channels[-1], 0, self.out_channels[-1], **kwargs\n",
    "        )\n",
    "        self.blocks = nn.ModuleDict(blocks)\n",
    "        self.depth = len(self.in_channels) - 1\n",
    "\n",
    "    def forward(self, features: List[torch.Tensor]) -> torch.Tensor:\n",
    "        # если \n",
    "        bs, channels, img_rows, img_cols = features[0].shape\n",
    "        _, _, feat1_rows, feat1_cols = features[1].shape\n",
    "        #upsample_scale_factors_list = [2.0 for i in range(len(self.blocks))]\n",
    "        if (img_rows, img_cols) == (feat1_rows, feat1_cols):\n",
    "            output_upsample_scaling_factor = 1.0\n",
    "        else:\n",
    "            output_upsample_scaling_factor = 2.0\n",
    "\n",
    "        features = features[1:]  # remove first skip with same spatial resolution\n",
    "        features = features[::-1]  # reverse channels to start from head of encoder\n",
    "\n",
    "        # start building dense connections\n",
    "        dense_x = {}\n",
    "        for layer_idx in range(len(self.in_channels) - 1):\n",
    "            for depth_idx in range(self.depth - layer_idx):\n",
    "                if layer_idx == 0:\n",
    "                    output = self.blocks[f\"x_{depth_idx}_{depth_idx}\"](\n",
    "                        features[depth_idx], features[depth_idx + 1]\n",
    "                    )\n",
    "                    dense_x[f\"x_{depth_idx}_{depth_idx}\"] = output\n",
    "                else:\n",
    "                    dense_l_i = depth_idx + layer_idx\n",
    "                    cat_features = [\n",
    "                        dense_x[f\"x_{idx}_{dense_l_i}\"]\n",
    "                        for idx in range(depth_idx + 1, dense_l_i + 1)\n",
    "                    ]\n",
    "                    cat_features = torch.cat(\n",
    "                        cat_features + [features[dense_l_i + 1]], dim=1\n",
    "                    )\n",
    "                    dense_x[f\"x_{depth_idx}_{dense_l_i}\"] = self.blocks[\n",
    "                        f\"x_{depth_idx}_{dense_l_i}\"\n",
    "                    ](dense_x[f\"x_{depth_idx}_{dense_l_i - 1}\"], cat_features)\n",
    "        \n",
    "        dense_x[f\"x_{0}_{self.depth}\"] = self.blocks[f\"x_{0}_{self.depth}\"](\n",
    "            dense_x[f\"x_{0}_{self.depth - 1}\"], scale_factor=output_upsample_scaling_factor\n",
    "        )\n",
    "        return dense_x[f\"x_{0}_{self.depth}\"]\n",
    "\n",
    "class UnetPlusPlusMod(SegmentationModel):\n",
    "    \"\"\"Unet++ is a fully convolution neural network for image semantic segmentation. Consist of *encoder*\n",
    "    and *decoder* parts connected with *skip connections*. Encoder extract features of different spatial\n",
    "    resolution (skip connections) which are used by decoder to define accurate segmentation mask. Decoder of\n",
    "    Unet++ is more complex than in usual Unet.\n",
    "\n",
    "    Args:\n",
    "        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n",
    "            to extract features of different spatial resolution\n",
    "        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features\n",
    "            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n",
    "            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n",
    "            Default is 5\n",
    "        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and\n",
    "            other pretrained weights (see table with available weights for each encoder_name)\n",
    "        decoder_channels: List of integers which specify **in_channels** parameter for convolutions used in decoder.\n",
    "            Length of the list should be the same as **encoder_depth**\n",
    "        decoder_use_norm:     Specifies normalization between Conv2D and activation.\n",
    "            Accepts the following types:\n",
    "            - **True**: Defaults to `\"batchnorm\"`.\n",
    "            - **False**: No normalization (`nn.Identity`).\n",
    "            - **str**: Specifies normalization type using default parameters. Available values:\n",
    "              `\"batchnorm\"`, `\"identity\"`, `\"layernorm\"`, `\"instancenorm\"`, `\"inplace\"`.\n",
    "            - **dict**: Fully customizable normalization settings. Structure:\n",
    "              ```python\n",
    "              {\"type\": <norm_type>, **kwargs}\n",
    "              ```\n",
    "              where `norm_name` corresponds to normalization type (see above), and `kwargs` are passed directly to the normalization layer as defined in PyTorch documentation.\n",
    "\n",
    "            **Example**:\n",
    "            ```python\n",
    "            decoder_use_norm={\"type\": \"layernorm\", \"eps\": 1e-2}\n",
    "            ```\n",
    "        decoder_attention_type: Attention module used in decoder of the model.\n",
    "            Available options are **None** and **scse** (https://arxiv.org/abs/1808.08127).\n",
    "        decoder_interpolation: Interpolation mode used in decoder of the model. Available options are\n",
    "            **\"nearest\"**, **\"bilinear\"**, **\"bicubic\"**, **\"area\"**, **\"nearest-exact\"**. Default is **\"nearest\"**.\n",
    "        in_channels: A number of input channels for the model, default is 3 (RGB images)\n",
    "        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n",
    "        activation: An activation function to apply after the final convolution layer.\n",
    "            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**,\n",
    "            **callable** and **None**. Default is **None**.\n",
    "        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build\n",
    "            on top of encoder if **aux_params** is not **None** (default). Supported params:\n",
    "                - classes (int): A number of classes\n",
    "                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n",
    "                - dropout (float): Dropout factor in [0, 1)\n",
    "                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\"\n",
    "                    (could be **None** to return logits)\n",
    "        kwargs: Arguments passed to the encoder class ``__init__()`` function. Applies only to ``timm`` models. Keys with ``None`` values are pruned before passing.\n",
    "\n",
    "    Returns:\n",
    "        ``torch.nn.Module``: **Unet++**\n",
    "\n",
    "    Reference:\n",
    "        https://arxiv.org/abs/1807.10165\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    _is_torch_scriptable = False\n",
    "\n",
    "    @supports_config_loading\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_name: str = \"resnet34\",\n",
    "        encoder_depth: int = 5,\n",
    "        encoder_weights: Optional[str] = \"imagenet\",\n",
    "        decoder_use_norm: Union[bool, str, Dict[str, Any]] = \"batchnorm\",\n",
    "        decoder_channels: Sequence[int] = (256, 128, 64, 32, 16),\n",
    "        decoder_attention_type: Optional[str] = None,\n",
    "        decoder_interpolation: str = \"nearest\",\n",
    "        in_channels: int = 3,\n",
    "        classes: int = 1,\n",
    "        activation: Optional[Union[str, Callable]] = None,\n",
    "        aux_params: Optional[dict] = None,\n",
    "        **kwargs: dict[str, Any],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if encoder_name.startswith(\"mit_b\"):\n",
    "            raise ValueError(\n",
    "                \"UnetPlusPlus is not support encoder_name={}\".format(encoder_name)\n",
    "            )\n",
    "\n",
    "        decoder_use_batchnorm = kwargs.pop(\"decoder_use_batchnorm\", None)\n",
    "        if decoder_use_batchnorm is not None:\n",
    "            warnings.warn(\n",
    "                \"The usage of decoder_use_batchnorm is deprecated. Please modify your code for decoder_use_norm\",\n",
    "                DeprecationWarning,\n",
    "                stacklevel=2,\n",
    "            )\n",
    "            decoder_use_norm = decoder_use_batchnorm\n",
    "\n",
    "        self.encoder = get_encoder(\n",
    "            encoder_name,\n",
    "            in_channels=in_channels,\n",
    "            depth=encoder_depth,\n",
    "            weights=encoder_weights,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.decoder = UnetPlusPlusDecoderMod(\n",
    "            encoder_channels=self.encoder.out_channels,\n",
    "            decoder_channels=decoder_channels,\n",
    "            n_blocks=encoder_depth,\n",
    "            use_norm=decoder_use_norm,\n",
    "            center=True if encoder_name.startswith(\"vgg\") else False,\n",
    "            attention_type=decoder_attention_type,\n",
    "            interpolation_mode=decoder_interpolation,\n",
    "        )\n",
    "\n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=decoder_channels[-1],\n",
    "            out_channels=classes,\n",
    "            activation=activation,\n",
    "            kernel_size=3,\n",
    "        )\n",
    "\n",
    "        if aux_params is not None:\n",
    "            self.classification_head = ClassificationHead(\n",
    "                in_channels=self.encoder.out_channels[-1], **aux_params\n",
    "            )\n",
    "        else:\n",
    "            self.classification_head = None\n",
    "\n",
    "        self.name = \"unetplusplus-{}\".format(encoder_name)\n",
    "        self.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7444b3b3",
   "metadata": {},
   "source": [
    "# Адаптация MANet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c55c40b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFABBlockMod(smp.decoders.manet.decoder.MFABBlock):\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, skip: Optional[torch.Tensor] = None, scale_factor=2.0,\n",
    "    ) -> torch.Tensor:\n",
    "        x = self.hl_conv(x)\n",
    "        x = F.interpolate(x, scale_factor=scale_factor, mode=self.interpolation_mode)\n",
    "        attention_hl = self.SE_hl(x)\n",
    "        if skip is not None:\n",
    "            attention_ll = self.SE_ll(skip)\n",
    "            attention_hl = attention_hl + attention_ll\n",
    "            x = x * attention_hl\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "class DecoderBlockMod(smp.decoders.manet.decoder.DecoderBlock):\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, skip: Optional[torch.Tensor] = None, scale_factor=2.0,\n",
    "    ) -> torch.Tensor:\n",
    "        x = F.interpolate(x, scale_factor=scale_factor, mode=self.interpolation_mode)\n",
    "        if skip is not None:\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "class MAnetDecoderMod(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_channels: List[int],\n",
    "        decoder_channels: List[int],\n",
    "        n_blocks: int = 5,\n",
    "        reduction: int = 16,\n",
    "        use_norm: Union[bool, str, Dict[str, Any]] = \"batchnorm\",\n",
    "        pab_channels: int = 64,\n",
    "        interpolation_mode: str = \"nearest\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if n_blocks != len(decoder_channels):\n",
    "            raise ValueError(\n",
    "                \"Model depth is {}, but you provide `decoder_channels` for {} blocks.\".format(\n",
    "                    n_blocks, len(decoder_channels)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # remove first skip with same spatial resolution\n",
    "        encoder_channels = encoder_channels[1:]\n",
    "\n",
    "        # reverse channels to start from head of encoder\n",
    "        encoder_channels = encoder_channels[::-1]\n",
    "\n",
    "        # computing blocks input and output channels\n",
    "        head_channels = encoder_channels[0]\n",
    "        in_channels = [head_channels] + list(decoder_channels[:-1])\n",
    "        skip_channels = list(encoder_channels[1:]) + [0]\n",
    "        out_channels = decoder_channels\n",
    "\n",
    "        self.center = smp.decoders.manet.decoder.PABBlock(head_channels, pab_channels=pab_channels)\n",
    "\n",
    "        # combine decoder keyword arguments\n",
    "        kwargs = dict(\n",
    "            use_norm=use_norm, interpolation_mode=interpolation_mode\n",
    "        )  # no attention type here\n",
    "        blocks = [\n",
    "            MFABBlockMod(in_ch, skip_ch, out_ch, reduction=reduction, **kwargs)\n",
    "            if skip_ch > 0\n",
    "            else DecoderBlockMod(in_ch, skip_ch, out_ch, **kwargs)\n",
    "            for in_ch, skip_ch, out_ch in zip(in_channels, skip_channels, out_channels)\n",
    "        ]\n",
    "        # for the last we dont have skip connection -> use simple decoder block\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "\n",
    "    def forward(self, features: List[torch.Tensor]) -> torch.Tensor:\n",
    "        bs, channels, img_rows, img_cols = features[0].shape\n",
    "        _, _, feat1_rows, feat1_cols = features[1].shape\n",
    "        upsample_scale_factors_list = [2.0 for i in range(len(self.blocks))]\n",
    "        if (img_rows, img_cols) == (feat1_rows, feat1_cols):\n",
    "            upsample_scale_factors_list[-1] = 1.0\n",
    "\n",
    "        features = features[1:]  # remove first skip with same spatial resolution\n",
    "        features = features[::-1]  # reverse channels to start from head of encoder\n",
    "\n",
    "        head = features[0]\n",
    "        skips = features[1:]\n",
    "\n",
    "\n",
    "        x = self.center(head)\n",
    "        for i, (decoder_block, scale_factor) in enumerate(zip(self.blocks, upsample_scale_factors_list)):\n",
    "            skip = skips[i] if i < len(skips) else None\n",
    "            x = decoder_block(x, skip, scale_factor)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class MAnetMod(SegmentationModel):\n",
    "    \"\"\"MAnet_ :  Multi-scale Attention Net. The MA-Net can capture rich contextual dependencies based on\n",
    "    the attention mechanism, using two blocks:\n",
    "     - Position-wise Attention Block (PAB), which captures the spatial dependencies between pixels in a global view\n",
    "     - Multi-scale Fusion Attention Block (MFAB), which  captures the channel dependencies between any feature map by\n",
    "       multi-scale semantic feature fusion\n",
    "\n",
    "    Args:\n",
    "        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n",
    "            to extract features of different spatial resolution\n",
    "        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features\n",
    "            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n",
    "            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n",
    "            Default is 5\n",
    "        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and\n",
    "            other pretrained weights (see table with available weights for each encoder_name)\n",
    "        decoder_channels: List of integers which specify **in_channels** parameter for convolutions used in decoder.\n",
    "            Length of the list should be the same as **encoder_depth**\n",
    "        decoder_use_norm: Specifies normalization between Conv2D and activation.\n",
    "            Accepts the following types:\n",
    "            - **True**: Defaults to `\"batchnorm\"`.\n",
    "            - **False**: No normalization (`nn.Identity`).\n",
    "            - **str**: Specifies normalization type using default parameters. Available values:\n",
    "              `\"batchnorm\"`, `\"identity\"`, `\"layernorm\"`, `\"instancenorm\"`, `\"inplace\"`.\n",
    "            - **dict**: Fully customizable normalization settings. Structure:\n",
    "              ```python\n",
    "              {\"type\": <norm_type>, **kwargs}\n",
    "              ```\n",
    "              where `norm_name` corresponds to normalization type (see above), and `kwargs` are passed directly to the normalization layer as defined in PyTorch documentation.\n",
    "\n",
    "            **Example**:\n",
    "            ```python\n",
    "            decoder_use_norm={\"type\": \"layernorm\", \"eps\": 1e-2}\n",
    "            ```\n",
    "        decoder_pab_channels: A number of channels for PAB module in decoder.\n",
    "            Default is 64.\n",
    "        decoder_interpolation: Interpolation mode used in decoder of the model. Available options are\n",
    "            **\"nearest\"**, **\"bilinear\"**, **\"bicubic\"**, **\"area\"**, **\"nearest-exact\"**. Default is **\"nearest\"**.\n",
    "        in_channels: A number of input channels for the model, default is 3 (RGB images)\n",
    "        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n",
    "        activation: An activation function to apply after the final convolution layer.\n",
    "            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**,\n",
    "            **callable** and **None**. Default is **None**.\n",
    "        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build\n",
    "            on top of encoder if **aux_params** is not **None** (default). Supported params:\n",
    "                - classes (int): A number of classes\n",
    "                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n",
    "                - dropout (float): Dropout factor in [0, 1)\n",
    "                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\"\n",
    "                    (could be **None** to return logits)\n",
    "        kwargs: Arguments passed to the encoder class ``__init__()`` function. Applies only to ``timm`` models. Keys with ``None`` values are pruned before passing.\n",
    "\n",
    "    Returns:\n",
    "        ``torch.nn.Module``: **MAnet**\n",
    "\n",
    "    .. _MAnet:\n",
    "        https://ieeexplore.ieee.org/abstract/document/9201310\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    @supports_config_loading\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_name: str = \"resnet34\",\n",
    "        encoder_depth: int = 5,\n",
    "        encoder_weights: Optional[str] = \"imagenet\",\n",
    "        decoder_use_norm: Union[bool, str, Dict[str, Any]] = \"batchnorm\",\n",
    "        decoder_channels: Sequence[int] = (256, 128, 64, 32, 16),\n",
    "        decoder_pab_channels: int = 64,\n",
    "        decoder_interpolation: str = \"nearest\",\n",
    "        in_channels: int = 3,\n",
    "        classes: int = 1,\n",
    "        activation: Optional[Union[str, Callable]] = None,\n",
    "        aux_params: Optional[dict] = None,\n",
    "        **kwargs: dict[str, Any],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        decoder_use_batchnorm = kwargs.pop(\"decoder_use_batchnorm\", None)\n",
    "        if decoder_use_batchnorm is not None:\n",
    "            warnings.warn(\n",
    "                \"The usage of decoder_use_batchnorm is deprecated. Please modify your code for decoder_use_norm\",\n",
    "                DeprecationWarning,\n",
    "                stacklevel=2,\n",
    "            )\n",
    "            decoder_use_norm = decoder_use_batchnorm\n",
    "\n",
    "        self.encoder = get_encoder(\n",
    "            encoder_name,\n",
    "            in_channels=in_channels,\n",
    "            depth=encoder_depth,\n",
    "            weights=encoder_weights,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.decoder = MAnetDecoderMod(\n",
    "            encoder_channels=self.encoder.out_channels,\n",
    "            decoder_channels=decoder_channels,\n",
    "            n_blocks=encoder_depth,\n",
    "            use_norm=decoder_use_norm,\n",
    "            pab_channels=decoder_pab_channels,\n",
    "            interpolation_mode=decoder_interpolation,\n",
    "        )\n",
    "\n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=decoder_channels[-1],\n",
    "            out_channels=classes,\n",
    "            activation=activation,\n",
    "            kernel_size=3,\n",
    "        )\n",
    "\n",
    "        if aux_params is not None:\n",
    "            self.classification_head = ClassificationHead(\n",
    "                in_channels=self.encoder.out_channels[-1], **aux_params\n",
    "            )\n",
    "        else:\n",
    "            self.classification_head = None\n",
    "\n",
    "        self.name = \"manet-{}\".format(encoder_name)\n",
    "        self.initialize()\n",
    "\n",
    "\n",
    "#model = MAnetMod()\n",
    "#model.encoder.conv1.stride = 1\n",
    "#ret = model(torch.randn(1, 3, 96, 96))\n",
    "#ret.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200468db",
   "metadata": {},
   "source": [
    "# Адаптация FPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "617f9a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPNMod(SegmentationModel):\n",
    "    \"\"\"FPN_ is a fully convolution neural network for image semantic segmentation.\n",
    "\n",
    "    Args:\n",
    "        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n",
    "            to extract features of different spatial resolution\n",
    "        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features\n",
    "            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n",
    "            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n",
    "            Default is 5\n",
    "        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and\n",
    "            other pretrained weights (see table with available weights for each encoder_name)\n",
    "        decoder_pyramid_channels: A number of convolution filters in Feature Pyramid of FPN_\n",
    "        decoder_segmentation_channels: A number of convolution filters in segmentation blocks of FPN_\n",
    "        decoder_merge_policy: Determines how to merge pyramid features inside FPN. Available options are **add**\n",
    "            and **cat**\n",
    "        decoder_dropout: Spatial dropout rate in range (0, 1) for feature pyramid in FPN_\n",
    "        decoder_interpolation: Interpolation mode used in decoder of the model. Available options are\n",
    "            **\"nearest\"**, **\"bilinear\"**, **\"bicubic\"**, **\"area\"**, **\"nearest-exact\"**. Default is **\"nearest\"**.\n",
    "        in_channels: A number of input channels for the model, default is 3 (RGB images)\n",
    "        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n",
    "        activation: An activation function to apply after the final convolution layer.\n",
    "            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**,\n",
    "            **callable** and **None**. Default is **None**.\n",
    "        upsampling: Final upsampling factor. Default is 4 to preserve input-output spatial shape identity\n",
    "        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build\n",
    "            on top of encoder if **aux_params** is not **None** (default). Supported params:\n",
    "                - classes (int): A number of classes\n",
    "                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n",
    "                - dropout (float): Dropout factor in [0, 1)\n",
    "                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\"\n",
    "                    (could be **None** to return logits)\n",
    "        kwargs: Arguments passed to the encoder class ``__init__()`` function. Applies only to ``timm`` models. Keys with ``None`` values are pruned before passing.\n",
    "\n",
    "    Returns:\n",
    "        ``torch.nn.Module``: **FPN**\n",
    "\n",
    "    .. _FPN:\n",
    "        http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    @supports_config_loading\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_name: str = \"resnet34\",\n",
    "        encoder_depth: int = 5,\n",
    "        encoder_weights: Optional[str] = \"imagenet\",\n",
    "        encoder_type:str = 'conv',\n",
    "        decoder_pyramid_channels: int = 256,\n",
    "        decoder_segmentation_channels: int = 128,\n",
    "        decoder_merge_policy: str = \"add\",\n",
    "        decoder_dropout: float = 0.2,\n",
    "        decoder_interpolation: str = \"nearest\",\n",
    "        in_channels: int = 3,\n",
    "        classes: int = 1,\n",
    "        activation: Optional[str] = None,\n",
    "        upsampling: int = 4,\n",
    "        aux_params: Optional[dict] = None,\n",
    "        **kwargs: dict[str, Any],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # validate input params\n",
    "        if encoder_name.startswith(\"mit_b\") and encoder_depth != 5:\n",
    "            raise ValueError(\n",
    "                \"Encoder {} support only encoder_depth=5\".format(encoder_name)\n",
    "            )\n",
    "\n",
    "        self.encoder = get_encoder(\n",
    "            encoder_name,\n",
    "            in_channels=in_channels,\n",
    "            depth=encoder_depth,\n",
    "            weights=encoder_weights,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.decoder = FPNDecoderMod(\n",
    "            encoder_channels=self.encoder.out_channels,\n",
    "            encoder_depth=encoder_depth,\n",
    "            pyramid_channels=decoder_pyramid_channels,\n",
    "            segmentation_channels=decoder_segmentation_channels,\n",
    "            dropout=decoder_dropout,\n",
    "            merge_policy=decoder_merge_policy,\n",
    "            interpolation_mode=decoder_interpolation,\n",
    "            encoder_type=encoder_type,\n",
    "        )\n",
    "\n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=self.decoder.out_channels,\n",
    "            out_channels=classes,\n",
    "            activation=activation,\n",
    "            kernel_size=1,\n",
    "            upsampling=upsampling,\n",
    "        )\n",
    "\n",
    "        if aux_params is not None:\n",
    "            self.classification_head = ClassificationHead(\n",
    "                in_channels=self.encoder.out_channels[-1], **aux_params\n",
    "            )\n",
    "        else:\n",
    "            self.classification_head = None\n",
    "\n",
    "        self.name = \"fpn-{}\".format(encoder_name)\n",
    "        self.initialize()\n",
    "\n",
    "class FPNModBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pyramid_channels: int,\n",
    "        skip_channels: int,\n",
    "        interpolation_mode: str = \"nearest\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.skip_conv = nn.Conv2d(skip_channels, pyramid_channels, kernel_size=1)\n",
    "        self.interpolation_mode = interpolation_mode\n",
    "\n",
    "    def forward(self, x: torch.Tensor, skip: torch.Tensor, scale_factor: float) -> torch.Tensor:\n",
    "        x = F.interpolate(x, scale_factor=scale_factor, mode=self.interpolation_mode)\n",
    "        if skip.size(1) != 0:\n",
    "            #print(x.shape, skip.shape)\n",
    "            skip = self.skip_conv(skip)\n",
    "            x = x + skip\n",
    "        return x\n",
    "\n",
    "class FPNDecoderMod(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_channels: List[int],\n",
    "        encoder_depth: int = 5,\n",
    "        pyramid_channels: int = 256,\n",
    "        segmentation_channels: int = 128,\n",
    "        dropout: float = 0.2,\n",
    "        merge_policy: Literal[\"add\", \"cat\"] = \"add\",\n",
    "        interpolation_mode: str = \"nearest\",\n",
    "        encoder_type:str = 'conv',\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.out_channels = (\n",
    "            segmentation_channels\n",
    "            if merge_policy == \"add\"\n",
    "            else segmentation_channels * 4\n",
    "        )\n",
    "        #print(self.out_channels)\n",
    "        if encoder_depth < 3:\n",
    "            raise ValueError(\n",
    "                \"Encoder depth for FPN decoder cannot be less than 3, got {}.\".format(\n",
    "                    encoder_depth\n",
    "                )\n",
    "            )\n",
    "\n",
    "        encoder_channels = encoder_channels[::-1]\n",
    "        encoder_channels = encoder_channels[: encoder_depth + 1]\n",
    "        \n",
    "        self.p6 = nn.Conv2d(encoder_channels[0], pyramid_channels, kernel_size=1)\n",
    "        '''\n",
    "        self.p5 = smp.decoders.fpn.decoder.FPNBlock(pyramid_channels, encoder_channels[1], interpolation_mode)\n",
    "        self.p4 = smp.decoders.fpn.decoder.FPNBlock(pyramid_channels, encoder_channels[2], interpolation_mode)\n",
    "        self.p3 = smp.decoders.fpn.decoder.FPNBlock(pyramid_channels, encoder_channels[3], interpolation_mode)\n",
    "        self.p2 = smp.decoders.fpn.decoder.FPNBlock(pyramid_channels, encoder_channels[4], interpolation_mode)\n",
    "        '''\n",
    "        self.p5 = FPNModBlock(pyramid_channels, encoder_channels[1], interpolation_mode)\n",
    "        self.p4 = FPNModBlock(pyramid_channels, encoder_channels[2], interpolation_mode)\n",
    "        self.p3 = FPNModBlock(pyramid_channels, encoder_channels[3], interpolation_mode)\n",
    "        self.p2 = FPNModBlock(pyramid_channels, encoder_channels[4], interpolation_mode)\n",
    "        \n",
    "        if encoder_type == 'conv':\n",
    "            upsamples_list = [4, 3, 2, 1, 0]\n",
    "        elif encoder_type == 'vit':\n",
    "            upsamples_list = [3, 2, 1, 0, 0]\n",
    "\n",
    "\n",
    "        self.seg_blocks = nn.ModuleList(\n",
    "            [\n",
    "                smp.decoders.fpn.decoder.SegmentationBlock(\n",
    "                    pyramid_channels, segmentation_channels, n_upsamples=n_upsamples\n",
    "                )\n",
    "                for n_upsamples in upsamples_list\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.merge = smp.decoders.fpn.decoder.MergeBlock(merge_policy)\n",
    "        self.dropout = nn.Dropout2d(p=dropout, inplace=True)\n",
    "\n",
    "    def forward(self, features: List[torch.Tensor]) -> torch.Tensor:\n",
    "        c2, c3, c4, c5, c6 = features[-5:]\n",
    "\n",
    "        #print([c2.shape, c3.shape, c4.shape, c5.shape, c6.shape])\n",
    "        #print([f.shape for f in features])\n",
    "        #print()\n",
    "        #print(f'c6:{c6.shape}')\n",
    "        p6 = self.p6(c6)\n",
    "        #print(f'p6:{p6.shape};c5:{c5.shape}')\n",
    "        p5 = self.p5(p6, c5, scale_factor=2.0)\n",
    "        #print(f'p5:{p5.shape};c4:{c4.shape}')\n",
    "        p4 = self.p4(p5, c4, scale_factor=2.0)\n",
    "        #print(f'p4:{p4.shape};c3:{c3.shape}')\n",
    "        p3 = self.p3(p4, c3, scale_factor=2.0)\n",
    "        #print(f'p3:{p3.shape};c2:{c2.shape}')\n",
    "        p2 = self.p2(p3, c2, scale_factor=2.0)\n",
    "        #print(f'p2:{p4.shape}')\n",
    "\n",
    "        s6 = self.seg_blocks[0](p6)\n",
    "        s5 = self.seg_blocks[1](p5)\n",
    "        s4 = self.seg_blocks[2](p4)\n",
    "        s3 = self.seg_blocks[3](p3)\n",
    "        s2 = self.seg_blocks[4](p2)\n",
    "\n",
    "        feature_pyramid = [s6, s5, s4, s3, s2]\n",
    "\n",
    "        #print([f.shape for f in feature_pyramid])\n",
    "\n",
    "        x = self.merge(feature_pyramid)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "#model = FPNMod(encoder_name='mit_b0', upsampling=0, encoder_type='vit', image_size=(96, 96))\n",
    "#model.encoder.patch_embed1.proj.stride=1\n",
    "\n",
    "#model = FPNMod(encoder_name='resnet34', upsampling=0)\n",
    "#model.encoder.conv1.stride=(1,1)\n",
    "\n",
    "#ret = model(torch.randn(1, 3, 96, 96))\n",
    "#ret.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaf945c",
   "metadata": {},
   "source": [
    "# UNet with attention"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7003e5d4",
   "metadata": {},
   "source": [
    "## Attention and supplementary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0276c8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    # query, key, value are (batch_size, num_heads, seq_len, head_dim)\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / (d_k ** 0.5)\n",
    "\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9) # Apply mask for padding or causal attention\n",
    "\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    return output, attention_weights\n",
    "\n",
    "class ConvMultiheadAttention(nn.Module):\n",
    "    '''\n",
    "    Class for convolutional multihead self-attention Wq, Wk, Wv are replaced from fully connected to convolutional layers \n",
    "    '''\n",
    "    def __init__(\n",
    "            self,\n",
    "            q_in_channels,\n",
    "            k_in_channels,\n",
    "            v_in_channels,\n",
    "            q_out_channels,\n",
    "            k_out_channels,\n",
    "            v_out_channels,\n",
    "            in_kernel_size,\n",
    "            in_padding,\n",
    "            in_stride,\n",
    "            head_row_dim,\n",
    "            head_col_dim,\n",
    "            head_ch_dim,\n",
    "            norm=nn.LayerNorm\n",
    "            ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_row_dim = head_row_dim\n",
    "        self.head_col_dim = head_col_dim\n",
    "        self.head_ch_dim = head_ch_dim\n",
    "\n",
    "        self.input_norm = norm(head_ch_dim*head_row_dim*head_col_dim, eps=1e-6)\n",
    "        \n",
    "        self.conv_q = nn.Conv2d(\n",
    "            in_channels=q_in_channels,\n",
    "            out_channels=q_out_channels,\n",
    "            kernel_size=in_kernel_size,\n",
    "            padding=in_padding,\n",
    "            stride=in_stride\n",
    "            )\n",
    "        \n",
    "        self.conv_k = nn.Conv2d(\n",
    "            in_channels=k_in_channels,\n",
    "            out_channels=k_out_channels,\n",
    "            kernel_size=in_kernel_size,\n",
    "            padding=in_padding,\n",
    "            stride=in_stride\n",
    "            )\n",
    "        \n",
    "        self.conv_v = nn.Conv2d(\n",
    "            in_channels=v_in_channels,\n",
    "            out_channels=v_out_channels,\n",
    "            kernel_size=in_kernel_size,\n",
    "            padding=in_padding,\n",
    "            stride=in_stride\n",
    "            )\n",
    "        \n",
    "        #self.out_conv = nn.Conv2d(in_channels=q_out_channels,out_channels=out_channels,kernel_size=out_kernel_size,padding=out_padding,stride=out_stride)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        bs, ch, rows, cols = value.shape\n",
    "        head_row_num = rows // self.head_row_dim\n",
    "        head_col_num = cols // self.head_col_dim\n",
    "        head_ch_num = ch // self.head_ch_dim\n",
    "\n",
    "        #print(head_row_num, head_col_num)\n",
    "\n",
    "        q = self.conv_q(query)\n",
    "        k = self.conv_k(key)\n",
    "        v = self.conv_v(value)\n",
    "        qkv_rearrangement_str = 'bs (head_ch_dim head_ch_num) (head_rdim head_rnum) (head_cdim head_cnum) -> bs (head_rnum head_cnum head_ch_num) (head_rdim head_cdim head_ch_dim) '\n",
    "        \n",
    "        q = eo.rearrange(\n",
    "            q,\n",
    "            qkv_rearrangement_str,\n",
    "            head_rdim=self.head_row_dim, head_cdim=self.head_col_dim, head_ch_dim=self.head_ch_dim)\n",
    "\n",
    "        k = eo.rearrange(\n",
    "            k,\n",
    "            qkv_rearrangement_str,\n",
    "            head_rdim=self.head_row_dim, head_cdim=self.head_col_dim, head_ch_dim=self.head_ch_dim\n",
    "            )\n",
    "        \n",
    "        v = eo.rearrange(\n",
    "            v,\n",
    "            qkv_rearrangement_str,\n",
    "            head_rdim=self.head_row_dim, head_cdim=self.head_col_dim, head_ch_dim=self.head_ch_dim)\n",
    "        #print(f'q:{q.shape};k:{k.shape};v:{v.shape}')\n",
    "        \n",
    "        weighted_v = F.scaled_dot_product_attention(query=q, key=k, value=v)\n",
    "        #print(f'v_w:{weighted_v.shape}')\n",
    "\n",
    "        weighted_v = eo.rearrange(\n",
    "            weighted_v,\n",
    "            'bs (head_rnum head_cnum head_ch_num) (head_rdim head_cdim head_ch_dim) -> bs (head_ch_dim head_ch_num) (head_rdim head_rnum) (head_cdim head_cnum)',\n",
    "            head_rdim=self.head_row_dim, head_cdim=self.head_col_dim, head_rnum=head_row_num, head_cnum=head_col_num,\n",
    "        )\n",
    "        #print(f'v_w_ra:{weighted_v.shape}')\n",
    "        return weighted_v\n",
    "\n",
    "        out = self.out_conv(weighted_v)\n",
    "        return out\n",
    "    \n",
    "class ConvMSABlock(nn.Module):\n",
    "    '''\n",
    "    Implementatuion of convolutional multihead self-attention block\n",
    "    '''\n",
    "    def __init__(\n",
    "            self,\n",
    "            msa_in_channels,\n",
    "            msa_intermediate_channels,\n",
    "            msa_in_kernel_size,\n",
    "            msa_in_padding,\n",
    "            msa_in_stride,\n",
    "            msa_out_channels,\n",
    "            \n",
    "            msa_head_row_dim,\n",
    "            msa_head_col_dim,\n",
    "            msa_head_ch_dim,\n",
    "\n",
    "            dropout,\n",
    "\n",
    "            out_conv_hidden_channels,\n",
    "            out_conv_kernel_size,\n",
    "            out_conv_padding,\n",
    "            out_conv_stride,\n",
    "\n",
    "            out_conv_out_channels,\n",
    "\n",
    "            out_conv_act,\n",
    "            \n",
    "            norm_layer: Callable[..., torch.nn.Module],\n",
    "            ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm1 = norm_layer(msa_in_channels)\n",
    "        self.self_att = ConvMultiheadAttention(\n",
    "            q_in_channels=msa_in_channels,\n",
    "            k_in_channels=msa_in_channels,\n",
    "            v_in_channels=msa_in_channels,\n",
    "            q_out_channels=msa_intermediate_channels,\n",
    "            k_out_channels=msa_intermediate_channels,\n",
    "            v_out_channels=msa_intermediate_channels,\n",
    "            in_kernel_size=msa_in_kernel_size,\n",
    "            in_padding=msa_in_padding,\n",
    "            in_stride=msa_in_stride,\n",
    "            head_row_dim=msa_head_row_dim,\n",
    "            head_col_dim=msa_head_col_dim,\n",
    "            head_ch_dim=msa_head_ch_dim,\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout2d(dropout)\n",
    "\n",
    "        self.out_conv = nn.Sequential(\n",
    "            torchvision.ops.Conv2dNormActivation(\n",
    "                in_channels=msa_out_channels,\n",
    "                out_channels=out_conv_hidden_channels,\n",
    "                kernel_size=out_conv_kernel_size,\n",
    "                padding=out_conv_padding,\n",
    "                stride=out_conv_stride,\n",
    "                activation_layer=out_conv_act\n",
    "            ),\n",
    "            torchvision.ops.Conv2dNormActivation(\n",
    "                in_channels=out_conv_hidden_channels,\n",
    "                out_channels=out_conv_out_channels,\n",
    "                kernel_size=out_conv_kernel_size,\n",
    "                padding=out_conv_padding,\n",
    "                stride=out_conv_stride,\n",
    "                activation_layer=out_conv_act\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.norm2 = norm_layer(msa_out_channels)\n",
    "    def forward(self, input_features):\n",
    "        \n",
    "        x = self.norm1(input_features)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = self.self_att(query=x, key=x, value=x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = x + input_features\n",
    "        x = self.norm2(x)\n",
    "        #print(x.shape)\n",
    "        y = self.out_conv(x)\n",
    "        return x + y\n",
    "    \n",
    "\n",
    "class ConvCrossAttentionBlock(nn.Module):\n",
    "    '''\n",
    "    Implementation of convolutional cross-attention block\n",
    "    '''\n",
    "    def __init__(\n",
    "            self,\n",
    "            q_in_channels,\n",
    "            k_in_channels,\n",
    "            v_in_channels,\n",
    "            q_out_channels,\n",
    "            k_out_channels,\n",
    "            v_out_channels,\n",
    "            in_kernel_size,\n",
    "            in_padding,\n",
    "            in_stride,\n",
    "            \n",
    "            head_row_dim,\n",
    "            head_col_dim,\n",
    "            head_ch_dim,\n",
    "\n",
    "            dropout,\n",
    "            \n",
    "            norm_layer: Callable[..., torch.nn.Module],\n",
    "            ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.kv_inp_norm = norm_layer(k_in_channels)\n",
    "        self.q_inp_norm = norm_layer(q_in_channels)\n",
    "\n",
    "        self.cross_att = ConvMultiheadAttention(\n",
    "            q_in_channels=q_in_channels,\n",
    "            k_in_channels=k_in_channels,\n",
    "            v_in_channels=v_in_channels,\n",
    "            q_out_channels=q_out_channels,\n",
    "            k_out_channels=k_out_channels,\n",
    "            v_out_channels=v_out_channels,\n",
    "            in_kernel_size=in_kernel_size,\n",
    "            in_padding=in_padding,\n",
    "            in_stride=in_stride,\n",
    "            \n",
    "            head_row_dim=head_row_dim,\n",
    "            head_col_dim=head_col_dim,\n",
    "            head_ch_dim=head_ch_dim,\n",
    "        )\n",
    "        self.dropout = nn.Dropout2d(dropout)\n",
    "        self.out_norm = norm_layer(q_out_channels)\n",
    "\n",
    "    def forward(self, q, kv):\n",
    "        \n",
    "        x = self.kv_inp_norm(kv)\n",
    "        q = self.q_inp_norm(q)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = self.cross_att(query=q, key=x, value=x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        #print(x.shape, q.shape)\n",
    "        x = x + q\n",
    "        x = self.out_norm(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "class ConcatDim1(nn.Module):\n",
    "    '''\n",
    "    Implementation of concatenation. It is nececcary for various aggreagation strategies in UNet decoder\n",
    "    '''\n",
    "    def forward(self, *tensors):\n",
    "        return torch.cat(tensors, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0e5384",
   "metadata": {},
   "source": [
    "## Код для UnetAtt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0d3e9abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetAtt(SegmentationModel):\n",
    "    \"\"\"\n",
    "    U-Net is a fully convolutional neural network architecture designed for semantic image segmentation.\n",
    "\n",
    "    It consists of two main parts:\n",
    "\n",
    "    1. An encoder (downsampling path) that extracts increasingly abstract features\n",
    "    2. A decoder (upsampling path) that gradually recovers spatial details\n",
    "\n",
    "    The key is the use of skip connections between corresponding encoder and decoder layers.\n",
    "    These connections allow the decoder to access fine-grained details from earlier encoder layers,\n",
    "    which helps produce more precise segmentation masks.\n",
    "\n",
    "    The skip connections work by concatenating feature maps from the encoder directly into the decoder\n",
    "    at corresponding resolutions. This helps preserve important spatial information that would\n",
    "    otherwise be lost during the encoding process.\n",
    "\n",
    "    Args:\n",
    "        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n",
    "            to extract features of different spatial resolution\n",
    "        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features\n",
    "            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n",
    "            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n",
    "            Default is 5\n",
    "        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and\n",
    "            other pretrained weights (see table with available weights for each encoder_name)\n",
    "        decoder_channels: List of integers which specify **in_channels** parameter for convolutions used in decoder.\n",
    "            Length of the list should be the same as **encoder_depth**\n",
    "        decoder_use_norm:     Specifies normalization between Conv2D and activation.\n",
    "            Accepts the following types:\n",
    "            - **True**: Defaults to `\"batchnorm\"`.\n",
    "            - **False**: No normalization (`nn.Identity`).\n",
    "            - **str**: Specifies normalization type using default parameters. Available values:\n",
    "              `\"batchnorm\"`, `\"identity\"`, `\"layernorm\"`, `\"instancenorm\"`, `\"inplace\"`.\n",
    "            - **dict**: Fully customizable normalization settings. Structure:\n",
    "              ```python\n",
    "              {\"type\": <norm_type>, **kwargs}\n",
    "              ```\n",
    "              where `norm_name` corresponds to normalization type (see above), and `kwargs` are passed directly to the normalization layer as defined in PyTorch documentation.\n",
    "\n",
    "            **Example**:\n",
    "            ```python\n",
    "            decoder_use_norm={\"type\": \"layernorm\", \"eps\": 1e-2}\n",
    "            ```\n",
    "        decoder_attention_type: Attention module used in decoder of the model. Available options are\n",
    "            **None** and **scse** (https://arxiv.org/abs/1808.08127).\n",
    "        decoder_interpolation: Interpolation mode used in decoder of the model. Available options are\n",
    "            **\"nearest\"**, **\"bilinear\"**, **\"bicubic\"**, **\"area\"**, **\"nearest-exact\"**. Default is **\"nearest\"**.\n",
    "        in_channels: A number of input channels for the model, default is 3 (RGB images)\n",
    "        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n",
    "        activation: An activation function to apply after the final convolution layer.\n",
    "            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**,\n",
    "            **callable** and **None**. Default is **None**.\n",
    "        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build\n",
    "            on top of encoder if **aux_params** is not **None** (default). Supported params:\n",
    "                - classes (int): A number of classes\n",
    "                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n",
    "                - dropout (float): Dropout factor in [0, 1)\n",
    "                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\"\n",
    "                    (could be **None** to return logits)\n",
    "        kwargs: Arguments passed to the encoder class ``__init__()`` function. Applies only to ``timm`` models. Keys with ``None`` values are pruned before passing.\n",
    "\n",
    "    Returns:\n",
    "        ``torch.nn.Module``: Unet\n",
    "\n",
    "    Example:\n",
    "        .. code-block:: python\n",
    "\n",
    "            import torch\n",
    "            import segmentation_models_pytorch as smp\n",
    "\n",
    "            model = smp.Unet(\"resnet18\", encoder_weights=\"imagenet\", classes=5)\n",
    "            model.eval()\n",
    "\n",
    "            # generate random images\n",
    "            images = torch.rand(2, 3, 256, 256)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                mask = model(images)\n",
    "\n",
    "            print(mask.shape)\n",
    "            # torch.Size([2, 5, 256, 256])\n",
    "\n",
    "    .. _Unet:\n",
    "        https://arxiv.org/abs/1505.04597\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    requires_divisible_input_shape = False\n",
    "\n",
    "    @supports_config_loading\n",
    "    def __init__(\n",
    "        self,\n",
    "        decoder_layers_configs: Sequence,\n",
    "        encoder_name: str = \"resnet34\",\n",
    "        encoder_depth: int = 5,\n",
    "        encoder_weights: Optional[str] = \"imagenet\",\n",
    "        decoder_use_norm: Union[bool, str, Dict[str, Any]] = \"batchnorm\",\n",
    "        decoder_channels: Sequence[int] = (256, 128, 64, 32, 16),\n",
    "        decoder_interpolation: str = \"nearest\",\n",
    "        in_channels: int = 3,\n",
    "        classes: int = 1,\n",
    "        activation: Optional[Union[str, Callable]] = None,\n",
    "        aux_params: Optional[dict] = None,\n",
    "        **kwargs: dict[str, Any],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        decoder_use_batchnorm = kwargs.pop(\"decoder_use_batchnorm\", None)\n",
    "        if decoder_use_batchnorm is not None:\n",
    "            warnings.warn(\n",
    "                \"The usage of decoder_use_batchnorm is deprecated. Please modify your code for decoder_use_norm\",\n",
    "                DeprecationWarning,\n",
    "                stacklevel=2,\n",
    "            )\n",
    "            decoder_use_norm = decoder_use_batchnorm\n",
    "\n",
    "        self.encoder = get_encoder(\n",
    "            encoder_name,\n",
    "            in_channels=in_channels,\n",
    "            depth=encoder_depth,\n",
    "            weights=encoder_weights,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        add_center_block = encoder_name.startswith(\"vgg\")\n",
    "\n",
    "        self.decoder = UnetDecoderAtt(\n",
    "            encoder_channels=self.encoder.out_channels,\n",
    "            decoder_channels=decoder_channels,\n",
    "            decoder_layers_configs=decoder_layers_configs,\n",
    "            n_blocks=encoder_depth,\n",
    "            use_norm=decoder_use_norm,\n",
    "            add_center_block=add_center_block,\n",
    "            interpolation_mode=decoder_interpolation,\n",
    "        )\n",
    "\n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=decoder_channels[-1],\n",
    "            out_channels=classes,\n",
    "            activation=activation,\n",
    "            kernel_size=3,\n",
    "        )\n",
    "\n",
    "        if aux_params is not None:\n",
    "            self.classification_head = ClassificationHead(\n",
    "                in_channels=self.encoder.out_channels[-1], **aux_params\n",
    "            )\n",
    "        else:\n",
    "            self.classification_head = None\n",
    "\n",
    "        self.name = \"u-{}\".format(encoder_name)\n",
    "        self.initialize()\n",
    "\n",
    "class UnetDecoderBlockAtt(nn.Module):\n",
    "    \"\"\"A decoder block in the U-Net architecture that performs upsampling and feature fusion.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.interpolation_mode = config['interpolation_mode']\n",
    "        in_channels = config['in_channels']\n",
    "        out_channels = config['out_channels']\n",
    "        skip_channels = config['skip_channels']\n",
    "\n",
    "        agg_type = config['aggregation_layer']['layer']\n",
    "        att1_type = config['attention1']['layer']\n",
    "        \n",
    "        # настраиваем cross-attention\n",
    "        if agg_type == 'conv_cross_att':\n",
    "            if skip_channels != 0:\n",
    "                config['aggregation_layer']['params']['q_in_channels'] = in_channels\n",
    "                config['aggregation_layer']['params']['k_in_channels'] = skip_channels\n",
    "                config['aggregation_layer']['params']['v_in_channels'] = skip_channels\n",
    "                config['aggregation_layer']['params']['q_out_channels'] = in_channels\n",
    "                config['aggregation_layer']['params']['k_out_channels'] = skip_channels\n",
    "                config['aggregation_layer']['params']['v_out_channels'] = skip_channels\n",
    "\n",
    "        # настраиваем multihead self-attention, в зависимости от параметров слоя объединения\n",
    "        if att1_type == 'conv_msa':\n",
    "            if agg_type == 'conv_cross_att':\n",
    "                # надо добавить выбор q=features OR q=skip\n",
    "                if skip_channels != 0:\n",
    "                    att_channels = in_channels\n",
    "                else:\n",
    "                    att_channels = in_channels\n",
    "                config['attention1']['params']['msa_in_channels'] = att_channels\n",
    "                config['attention1']['params']['msa_intermediate_channels'] = att_channels\n",
    "                config['attention1']['params']['msa_out_channels'] = att_channels\n",
    "                config['attention1']['params']['out_conv_out_channels'] = att_channels\n",
    "            else:\n",
    "                config['attention1']['params']['msa_in_channels'] = in_channels + skip_channels\n",
    "                config['attention1']['params']['msa_intermediate_channels'] = in_channels + skip_channels\n",
    "                config['attention1']['params']['msa_out_channels'] = in_channels + skip_channels\n",
    "                config['attention1']['params']['out_conv_out_channels'] = in_channels + skip_channels\n",
    "        # получаем метод создания слоя агрегации признаков\n",
    "        create_aggregation = unet_aggregation_factory_dict[agg_type]\n",
    "        if skip_channels != 0:\n",
    "            self.aggregation_layer = create_aggregation(**config['aggregation_layer']['params'])\n",
    "        else:\n",
    "            self.aggregation_layer = nn.Identity()\n",
    "        \n",
    "        # получаем метод создания слоя внимания после агрегации\n",
    "        create_attention = unet_attention_factory_dict[att1_type]\n",
    "        self.attention1 = create_attention(**config['attention1']['params'])\n",
    "        # создаем сверточные слои после слоя внимания\n",
    "        conv_layers = []\n",
    "        for idx, params in enumerate(config['conv']):\n",
    "            if idx == 0:\n",
    "                if agg_type == 'conv_cross_att' and skip_channels != 0:\n",
    "                    in_conv_ch = in_channels\n",
    "                else:\n",
    "                    in_conv_ch = in_channels + skip_channels\n",
    "            else:\n",
    "                in_conv_ch = out_channels\n",
    "\n",
    "            conv = torchvision.ops.Conv2dNormActivation(\n",
    "                in_channels=in_conv_ch,\n",
    "                out_channels=out_channels,\n",
    "                **params\n",
    "                )\n",
    "            conv_layers.append(conv)\n",
    "        self.conv_layers = nn.Sequential(*conv_layers)\n",
    "\n",
    "        att2_type = config['attention2']['layer']\n",
    "        if att2_type == 'conv_msa':\n",
    "            config['attention2']['params']['msa_in_channels'] = out_channels\n",
    "            config['attention2']['params']['msa_intermediate_channels'] = out_channels\n",
    "            config['attention2']['params']['msa_out_channels'] = out_channels\n",
    "            config['attention2']['params']['out_conv_out_channels'] = out_channels\n",
    "        create_attention = unet_attention_factory_dict[att2_type]\n",
    "        self.attention2 = create_attention(**config['attention2']['params'])\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        feature_map: torch.Tensor,\n",
    "        target_height: int,\n",
    "        target_width: int,\n",
    "        skip_connection: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        feature_map = F.interpolate(\n",
    "            feature_map,\n",
    "            size=(target_height, target_width),\n",
    "            mode=self.interpolation_mode,\n",
    "        )\n",
    "        #print('DECODER LAYER!!!')\n",
    "        if skip_connection is not None:\n",
    "            #print(f'feat:{feature_map.shape},skip:{skip_connection.shape}')\n",
    "            feature_map = self.aggregation_layer(feature_map, skip_connection)\n",
    "            feature_map = self.attention1(feature_map)\n",
    "        #print(f'att_feat:{feature_map.shape}')\n",
    "        feature_map = self.conv_layers(feature_map)\n",
    "        feature_map = self.attention2(feature_map)\n",
    "        return feature_map\n",
    "    \n",
    "class UnetDecoderAtt(nn.Module):\n",
    "    \"\"\"The decoder part of the U-Net architecture.\n",
    "\n",
    "    Takes encoded features from different stages of the encoder and progressively upsamples them while\n",
    "    combining with skip connections. This helps preserve fine-grained details in the final segmentation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_channels: Sequence[int],\n",
    "        decoder_channels: Sequence[int],\n",
    "        decoder_layers_configs: Sequence[Dict],\n",
    "        n_blocks: int = 5,\n",
    "        add_center_block: bool = False,\n",
    "        interpolation_mode: str = \"nearest\",\n",
    "        use_norm:str = \"batchnorm\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if n_blocks != len(decoder_channels):\n",
    "            raise ValueError(\n",
    "                \"Model depth is {}, but you provide `decoder_channels` for {} blocks.\".format(\n",
    "                    n_blocks, len(decoder_channels)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        if decoder_layers_configs is not None and (n_blocks != len(decoder_layers_configs)):\n",
    "            raise ValueError(\n",
    "                \"Model depth is {}, but you provide `attention_configs` for {} blocks.\".format(\n",
    "                    n_blocks, len(decoder_layers_configs)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # remove first skip with same spatial resolution\n",
    "        encoder_channels = encoder_channels[1:]\n",
    "        # reverse channels to start from head of encoder\n",
    "        encoder_channels = encoder_channels[::-1]\n",
    "\n",
    "        # computing blocks input and output channels\n",
    "        head_channels = encoder_channels[0]\n",
    "        in_channels = [head_channels] + list(decoder_channels[:-1])\n",
    "        skip_channels = list(encoder_channels[1:]) + [0]\n",
    "        out_channels = decoder_channels\n",
    "\n",
    "        if add_center_block:\n",
    "            self.center = smp.decoders.unet.decoder.UnetCenterBlock(\n",
    "                head_channels,\n",
    "                head_channels,\n",
    "                use_norm=use_norm,\n",
    "            )\n",
    "        else:\n",
    "            self.center = nn.Identity()\n",
    "\n",
    "        # combine decoder keyword arguments\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for block_in_channels, block_skip_channels, block_out_channels, decoder_layer_config in zip(\n",
    "            in_channels, skip_channels, out_channels, decoder_layers_configs\n",
    "        ):\n",
    "            #print(f'in:{block_in_channels}, skip:{block_skip_channels}, out:{block_out_channels}')\n",
    "            #print('-------------')\n",
    "            decoder_layer_config['in_channels'] = block_in_channels\n",
    "            decoder_layer_config['skip_channels'] = block_skip_channels\n",
    "            decoder_layer_config['out_channels'] = block_out_channels\n",
    "            block = UnetDecoderBlockAtt(\n",
    "                decoder_layer_config\n",
    "            )\n",
    "            self.blocks.append(block)\n",
    "\n",
    "    def forward(self, features: List[torch.Tensor]) -> torch.Tensor:\n",
    "        # spatial shapes of features: [hw, hw/2, hw/4, hw/8, ...]\n",
    "        spatial_shapes = [feature.shape[2:] for feature in features]\n",
    "        spatial_shapes = spatial_shapes[::-1]\n",
    "\n",
    "        features = features[1:]  # remove first skip with same spatial resolution\n",
    "        features = features[::-1]  # reverse channels to start from head of encoder\n",
    "\n",
    "        head = features[0]\n",
    "        skip_connections = features[1:]\n",
    "\n",
    "        x = self.center(head)\n",
    "\n",
    "        for i, decoder_block in enumerate(self.blocks):\n",
    "            # upsample to the next spatial shape\n",
    "            height, width = spatial_shapes[i + 1]\n",
    "            skip_connection = skip_connections[i] if i < len(skip_connections) else None\n",
    "            x = decoder_block(x, height, width, skip_connection=skip_connection)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079b9d59",
   "metadata": {},
   "source": [
    "# Адаптация FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "21b14254",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNDecoderBlock(nn.Module):\n",
    "    \"\"\"A decoder block in the FCN architecture that performs upsampling and feature fusion.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        use_norm: Union[bool, str, Dict[str, Any]] = \"batchnorm\",\n",
    "        attention_type: Optional[str] = None,\n",
    "        interpolation_mode: str = \"nearest\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.interpolation_mode = interpolation_mode\n",
    "        self.conv1 = md.Conv2dReLU(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_norm=use_norm,\n",
    "        )\n",
    "        self.attention1 = md.Attention(\n",
    "            attention_type, in_channels=in_channels\n",
    "        )\n",
    "        self.conv2 = md.Conv2dReLU(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_norm=use_norm,\n",
    "        )\n",
    "        self.attention2 = md.Attention(attention_type, in_channels=out_channels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        feature_map: torch.Tensor,\n",
    "        target_height: int,\n",
    "        target_width: int,\n",
    "        skip_connection: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        # сначала интерполяция и свертка\n",
    "        feature_map = F.interpolate(\n",
    "            feature_map,\n",
    "            size=(target_height, target_width),\n",
    "            mode=self.interpolation_mode,\n",
    "        )\n",
    "        feature_map = self.conv1(feature_map)\n",
    "        feature_map = self.attention1(feature_map)\n",
    "        \n",
    "        # потом сложение и выходная свертка\n",
    "        if skip_connection is not None:\n",
    "            feature_map = feature_map + skip_connection\n",
    "        feature_map = self.conv2(feature_map)\n",
    "        feature_map = self.attention2(feature_map)\n",
    "        \n",
    "        return feature_map\n",
    "    \n",
    "class FCNDecoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            encoder_channels: Sequence[int],\n",
    "            decoder_last_channel: Sequence[int],\n",
    "            n_blocks: int = 5,\n",
    "            use_norm: Union[bool, str, Dict[str, Any]] = \"batchnorm\",\n",
    "            attention_type: Optional[str] = None,\n",
    "            add_center_block: bool = False,\n",
    "            interpolation_mode: str = \"nearest\",\n",
    "        ):\n",
    "            super().__init__()\n",
    "            # remove first skip with same spatial resolution\n",
    "            encoder_channels = encoder_channels[1:]\n",
    "            # reverse channels to start from head of encoder\n",
    "            encoder_channels = encoder_channels[::-1]\n",
    "\n",
    "            # computing blocks input and output channels\n",
    "            head_channels = encoder_channels[0]\n",
    "            in_channels = encoder_channels\n",
    "            out_channels = encoder_channels[1:] + [decoder_last_channel]\n",
    "            \n",
    "            if add_center_block:\n",
    "                self.center = smp.decoders.unet.decoder.UnetCenterBlock(\n",
    "                    head_channels,\n",
    "                    head_channels//2,\n",
    "                    use_norm=use_norm,\n",
    "                )\n",
    "            else:\n",
    "                self.center = nn.Identity()\n",
    "\n",
    "            # combine decoder keyword arguments\n",
    "            self.blocks = nn.ModuleList()\n",
    "            for block_in_channels, block_out_channels in zip(\n",
    "                in_channels, out_channels\n",
    "            ):\n",
    "                block = FCNDecoderBlock(\n",
    "                    block_in_channels,\n",
    "                    block_out_channels,\n",
    "                    use_norm=use_norm,\n",
    "                    attention_type=attention_type,\n",
    "                    interpolation_mode=interpolation_mode,\n",
    "                )\n",
    "                self.blocks.append(block)\n",
    "\n",
    "    def forward(self, features: List[torch.Tensor]) -> torch.Tensor:\n",
    "        # spatial shapes of features: [hw, hw/2, hw/4, hw/8, ...]\n",
    "        spatial_shapes = [feature.shape[2:] for feature in features]\n",
    "        spatial_shapes = spatial_shapes[::-1]\n",
    "\n",
    "        features = features[1:]  # remove first skip with same spatial resolution\n",
    "        features = features[::-1]  # reverse channels to start from head of encoder\n",
    "        \n",
    "\n",
    "        head = features[0]\n",
    "        skip_connections = features[1:]\n",
    "        \n",
    "\n",
    "        x = self.center(head)\n",
    "\n",
    "        for i, decoder_block in enumerate(self.blocks):\n",
    "            # upsample to the next spatial shape\n",
    "            height, width = spatial_shapes[i + 1]\n",
    "            \n",
    "            skip_connection = skip_connections[i] if i < len(skip_connections) else None\n",
    "            \n",
    "            x = decoder_block(x, height, width, skip_connection=skip_connection)\n",
    "\n",
    "        return x\n",
    "\n",
    "class FCN(SegmentationModel):\n",
    "    \"\"\"\n",
    "    FCN is a fully convolutional neural network architecture designed for semantic image segmentation.\n",
    "\n",
    "    It consists of two main parts:\n",
    "\n",
    "    1. An encoder (downsampling path) that extracts increasingly abstract features\n",
    "    2. A decoder (upsampling path) that gradually recovers spatial details\n",
    "\n",
    "    The key is the use of skip connections between corresponding encoder and decoder layers.\n",
    "    These connections allow the decoder to access fine-grained details from earlier encoder layers,\n",
    "    which helps produce more precise segmentation masks.\n",
    "\n",
    "    The skip connections work by concatenating feature maps from the encoder directly into the decoder\n",
    "    at corresponding resolutions. This helps preserve important spatial information that would\n",
    "    otherwise be lost during the encoding process.\n",
    "\n",
    "    Args:\n",
    "        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n",
    "            to extract features of different spatial resolution\n",
    "        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features\n",
    "            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n",
    "            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n",
    "            Default is 5\n",
    "        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and\n",
    "            other pretrained weights (see table with available weights for each encoder_name)\n",
    "        decoder_channels: List of integers which specify **in_channels** parameter for convolutions used in decoder.\n",
    "            Length of the list should be the same as **encoder_depth**\n",
    "        decoder_use_norm:     Specifies normalization between Conv2D and activation.\n",
    "            Accepts the following types:\n",
    "            - **True**: Defaults to `\"batchnorm\"`.\n",
    "            - **False**: No normalization (`nn.Identity`).\n",
    "            - **str**: Specifies normalization type using default parameters. Available values:\n",
    "              `\"batchnorm\"`, `\"identity\"`, `\"layernorm\"`, `\"instancenorm\"`, `\"inplace\"`.\n",
    "            - **dict**: Fully customizable normalization settings. Structure:\n",
    "              ```python\n",
    "              {\"type\": <norm_type>, **kwargs}\n",
    "              ```\n",
    "              where `norm_name` corresponds to normalization type (see above), and `kwargs` are passed directly to the normalization layer as defined in PyTorch documentation.\n",
    "\n",
    "            **Example**:\n",
    "            ```python\n",
    "            decoder_use_norm={\"type\": \"layernorm\", \"eps\": 1e-2}\n",
    "            ```\n",
    "        decoder_attention_type: Attention module used in decoder of the model. Available options are\n",
    "            **None** and **scse** (https://arxiv.org/abs/1808.08127).\n",
    "        decoder_interpolation: Interpolation mode used in decoder of the model. Available options are\n",
    "            **\"nearest\"**, **\"bilinear\"**, **\"bicubic\"**, **\"area\"**, **\"nearest-exact\"**. Default is **\"nearest\"**.\n",
    "        in_channels: A number of input channels for the model, default is 3 (RGB images)\n",
    "        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n",
    "        activation: An activation function to apply after the final convolution layer.\n",
    "            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**,\n",
    "            **callable** and **None**. Default is **None**.\n",
    "        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build\n",
    "            on top of encoder if **aux_params** is not **None** (default). Supported params:\n",
    "                - classes (int): A number of classes\n",
    "                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n",
    "                - dropout (float): Dropout factor in [0, 1)\n",
    "                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\"\n",
    "                    (could be **None** to return logits)\n",
    "        kwargs: Arguments passed to the encoder class ``__init__()`` function. Applies only to ``timm`` models. Keys with ``None`` values are pruned before passing.\n",
    "\n",
    "    Returns:\n",
    "        ``torch.nn.Module``: Unet\n",
    "\n",
    "    Example:\n",
    "        .. code-block:: python\n",
    "\n",
    "            import torch\n",
    "            import segmentation_models_pytorch as smp\n",
    "\n",
    "            model = smp.Unet(\"resnet18\", encoder_weights=\"imagenet\", classes=5)\n",
    "            model.eval()\n",
    "\n",
    "            # generate random images\n",
    "            images = torch.rand(2, 3, 256, 256)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                mask = model(images)\n",
    "\n",
    "            print(mask.shape)\n",
    "            # torch.Size([2, 5, 256, 256])\n",
    "\n",
    "    .. _Unet:\n",
    "        https://arxiv.org/abs/1505.04597\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    requires_divisible_input_shape = False\n",
    "\n",
    "    @supports_config_loading\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_name: str = \"resnet34\",\n",
    "        encoder_depth: int = 5,\n",
    "        encoder_weights: Optional[str] = \"imagenet\",\n",
    "        decoder_use_norm: Union[bool, str, Dict[str, Any]] = \"batchnorm\",\n",
    "        decoder_last_channel: int = 16,\n",
    "        decoder_attention_type: Optional[str] = None,\n",
    "        decoder_interpolation: str = \"nearest\",\n",
    "        in_channels: int = 3,\n",
    "        classes: int = 1,\n",
    "        activation: Optional[Union[str, Callable]] = None,\n",
    "        aux_params: Optional[dict] = None,\n",
    "        **kwargs: dict[str, Any],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        decoder_use_batchnorm = kwargs.pop(\"decoder_use_batchnorm\", None)\n",
    "        if decoder_use_batchnorm is not None:\n",
    "            warnings.warn(\n",
    "                \"The usage of decoder_use_batchnorm is deprecated. Please modify your code for decoder_use_norm\",\n",
    "                DeprecationWarning,\n",
    "                stacklevel=2,\n",
    "            )\n",
    "            decoder_use_norm = decoder_use_batchnorm\n",
    "\n",
    "        self.encoder = get_encoder(\n",
    "            encoder_name,\n",
    "            in_channels=in_channels,\n",
    "            depth=encoder_depth,\n",
    "            weights=encoder_weights,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        add_center_block = encoder_name.startswith(\"vgg\")\n",
    "\n",
    "        self.decoder = FCNDecoder(\n",
    "            encoder_channels=self.encoder.out_channels,\n",
    "            decoder_last_channel=decoder_last_channel,\n",
    "            n_blocks=encoder_depth,\n",
    "            use_norm=decoder_use_norm,\n",
    "            add_center_block=add_center_block,\n",
    "            attention_type=decoder_attention_type,\n",
    "            interpolation_mode=decoder_interpolation,\n",
    "        )\n",
    "\n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=decoder_last_channel,\n",
    "            out_channels=classes,\n",
    "            activation=activation,\n",
    "            kernel_size=3,\n",
    "        )\n",
    "\n",
    "        if aux_params is not None:\n",
    "            self.classification_head = ClassificationHead(\n",
    "                in_channels=decoder_last_channel, **aux_params\n",
    "            )\n",
    "        else:\n",
    "            self.classification_head = None\n",
    "\n",
    "        self.name = \"fcn-{}\".format(encoder_name)\n",
    "        self.initialize()\n",
    "\n",
    "#model = FCN(decoder_last_channel=32)\n",
    "#ret = model(torch.randn(1, 3, 224, 224))\n",
    "#ret.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4035171",
   "metadata": {},
   "source": [
    "# Фабрики для создания моделей по конфигурациям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1960f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relace_input_layer(model:nn.Module, config:dict):\n",
    "    pass\n",
    "\n",
    "def create_weights_from_avg_ch(weight, new_in_channels):\n",
    "    return torch.cat([weight.mean(dim=1).unsqueeze(1)]*new_in_channels, dim=1)\n",
    "\n",
    "def cerate_weights_from_repeated_ch(weight, in_channels, new_in_channels):\n",
    "    ch_multiple = new_in_channels//in_channels\n",
    "    reminded_channels = new_in_channels%in_channels\n",
    "    # сначала набираем новые каналы путем подставления друг за другом (stack) каналов изначального изображения,\n",
    "    # а затем, если количество новых каналов не делится без остатка на количество изначальных, \n",
    "    # то набираем оставшиеся новые каналы из оставшихся изначальных    \n",
    "    new_weight = torch.cat(\n",
    "        [weight]*ch_multiple + [weight[:,:reminded_channels]], dim=1)\n",
    "    return new_weight\n",
    "\n",
    "def create_augmentation_transforms(transforms_dict:Dict[str, Dict]):\n",
    "    transforms_list = []\n",
    "    for name, transform_params in transforms_dict.items():\n",
    "        transform_creation_fn = transforms_factory_dict[name]\n",
    "        transforms_list.append(transform_creation_fn(**transform_params))\n",
    "    #return v2.Compose([v2.RandomOrder(transforms_list)])\n",
    "    return v2.RandomOrder(transforms_list)\n",
    "\n",
    "def create_model(config_dict, segmentation_nns_factory_dict):\n",
    "    model_name = config_dict['segmentation_nn']['nn_architecture']\n",
    "    # создаем нейронную сеть из фабрики\n",
    "    model = segmentation_nns_factory_dict[model_name](**config_dict['segmentation_nn']['params'])\n",
    "    multispecter_bands_indices = config_dict['multispecter_bands_indices']\n",
    "    in_channels = len(multispecter_bands_indices)\n",
    "    # замена входного слоя, если кол-во каналов изображения не равно трем\n",
    "    input_conv = model.get_submodule(\n",
    "        config_dict['segmentation_nn']['input_layer_config']['layer_path']\n",
    "        )\n",
    "    if 'channels' in config_dict['segmentation_nn']['input_layer_config']['replace_type']:\n",
    "        if in_channels != 3:\n",
    "            # получаем входной слой, специфический для конкретной нейронной сети\n",
    "            \n",
    "            new_input_conv = nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=input_conv.out_channels,\n",
    "                kernel_size=input_conv.kernel_size,\n",
    "                #stride=conv1.stride,\n",
    "                stride=config_dict['segmentation_nn']['input_layer_config']['params']['stride'],\n",
    "                #padding=conv1.padding,\n",
    "                padding=config_dict['segmentation_nn']['input_layer_config']['params']['padding'],\n",
    "                dilation=input_conv.dilation,\n",
    "                groups=input_conv.groups,\n",
    "                bias=input_conv.bias is not None\n",
    "            )\n",
    "            if config_dict['segmentation_nn']['params']['encoder_weights'] is not None:\n",
    "                # выбор типа обнолвления весов\n",
    "                if config_dict['segmentation_nn']['input_layer_config']['weight_update_type'] == 'average_all':\n",
    "                    \n",
    "                    #new_weight = torch.cat([input_conv.weight.mean(dim=1).unsqueeze(1)]*in_channels, dim=1)\n",
    "                    new_weight = create_weights_from_avg_ch(input_conv.weight, in_channels)\n",
    "                    input_conv.weight = nn.Parameter(new_weight)\n",
    "\n",
    "                elif config_dict['segmentation_nn']['input_layer_config']['weight_update_type'] == 'repeate':\n",
    "                    '''\n",
    "                    ch_multiple = in_channels//input_conv.in_channels\n",
    "                    reminded_channels = in_channels%input_conv.in_channels\n",
    "                    new_weight = torch.cat(\n",
    "                        [input_conv.weight]*ch_multiple + [input_conv.weight[:,:reminded_channels]], dim=1)\n",
    "                    '''\n",
    "                    new_weight = cerate_weights_from_repeated_ch(input_conv.weight, input_conv.in_channels, in_channels)\n",
    "                    \n",
    "                if input_conv.bias is not None:\n",
    "                    new_input_conv.bias = input_conv.bias\n",
    "\n",
    "            # перезаписываем входной слой исходя из специфики оригинальной сети\n",
    "            model.set_submodule(\n",
    "                config_dict['segmentation_nn']['input_layer_config']['layer_path'],\n",
    "                new_input_conv\n",
    "                )\n",
    "    elif 'multisize_conv' in config_dict['segmentation_nn']['input_layer_config']['replace_type']:\n",
    "        multisize_params = config_dict['segmentation_nn']['input_layer_config']['params']\n",
    "        new_input_conv = MultisizeConv(**multisize_params)\n",
    "\n",
    "        # Если мы модифицируем входной слой.\n",
    "        if config_dict['segmentation_nn']['params']['encoder_weights'] is not None:\n",
    "            # вычленяем словрь с параметрами размеров ядер сверток.\n",
    "            kernel_sizes_dict = config_dict['segmentation_nn']['input_layer_config']['params']['kernel_size']\n",
    "            interpolated_kernels_dict = {}\n",
    "            # выполняем интерполяцию ядер свертки для каждого набора из новых ядер\n",
    "            for name, kernel_size in kernel_sizes_dict.items():\n",
    "                if isinstance(kernel_size, int):\n",
    "                    kernel_size = (kernel_size, kernel_size)\n",
    "                # получаем интерполированную версию ядер свертки\n",
    "                interpolated_kernels_dict[name] = [\n",
    "                    F.interpolate(input_conv.weight, size=kernel_size, mode='bicubic', antialias=True),\n",
    "                    input_conv.bias]\n",
    "                '''            \n",
    "                out_channels_dict = config_dict['segmentation_nn']['input_layer_config']['params']['out_channels']\n",
    "                for name, out_channels in out_channels_dict.items():\n",
    "                    weights = interpolated_kernels_dict[name][0]\n",
    "                    weights = create_weights_from_avg_ch(weights, in_channels)\n",
    "                    interpolated_kernels_dict[name][0] = weights\n",
    "                '''\n",
    "            #out_channels_dict = config_dict['segmentation_nn']['input_layer_config']['params']['out_channels']\n",
    "            for name in interpolated_kernels_dict.keys():\n",
    "                weights = interpolated_kernels_dict[name][0]\n",
    "                if config_dict['segmentation_nn']['input_layer_config']['weight_update_type'] == 'average_all':\n",
    "                    weights = create_weights_from_avg_ch(weights, new_in_channels=in_channels)\n",
    "                elif config_dict['segmentation_nn']['input_layer_config']['weight_update_type'] == 'repeat':\n",
    "                    weights = cerate_weights_from_repeated_ch(weights, in_channels=input_conv.in_channels, new_in_channels=in_channels)\n",
    "                \n",
    "                interpolated_kernels_dict[name][0] = weights\n",
    "                        \n",
    "            new_input_conv.update_weights(new_weights_dict=interpolated_kernels_dict)\n",
    "        if config_dict['segmentation_nn']['input_layer_config']['params']['aggregation_type'] == 'cat':\n",
    "            # Если тип агрегации выхода MultisizeConv - это конкатенация, то изменяем также второй сверточный слой,\n",
    "            # чтобы число его входных каналов соответствовало числу выходных первого слоя \n",
    "            raise NotImplementedError\n",
    "        # заменяем сходной слой по заранее определенному пути, который может варьировать в зависимости от архитектуры энкодера\n",
    "        model.set_submodule(\n",
    "                config_dict['segmentation_nn']['input_layer_config']['layer_path'],\n",
    "                new_input_conv\n",
    "                )\n",
    "    return model\n",
    "\n",
    "class MultisizeConv(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels:int,\n",
    "            out_channels:dict,\n",
    "            kernel_size:dict,\n",
    "            stride:dict,\n",
    "            padding:dict,\n",
    "            dilation:dict,\n",
    "            groups:dict,\n",
    "            bias:dict,\n",
    "            aggregation_type:str,\n",
    "            ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.aggregation_type = aggregation_type\n",
    "        self.multisize_convs = nn.ModuleDict()\n",
    "        for conv_name in kernel_size.keys():\n",
    "            self.multisize_convs[conv_name] = nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels[conv_name],\n",
    "                kernel_size=kernel_size[conv_name],\n",
    "                stride=stride[conv_name],\n",
    "                padding=padding[conv_name],\n",
    "                dilation=dilation[conv_name],\n",
    "                groups=groups[conv_name],\n",
    "                bias=bias[conv_name]\n",
    "                )\n",
    "        \n",
    "    def update_weights(self, new_weights_dict):\n",
    "        '''\n",
    "        На вход принимается словрь со структурой {'имя_свертки': (weight, bias)}\n",
    "        '''\n",
    "        for conv_name, (weight, bias) in new_weights_dict.items():\n",
    "            self.multisize_convs[conv_name].weight = nn.Parameter(weight)\n",
    "            if self.multisize_convs[conv_name].bias is not None:\n",
    "                self.multisize_convs[conv_name].bias = nn.Parameter(bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for conv_name in self.multisize_convs.keys():\n",
    "            out = self.multisize_convs[conv_name](x)\n",
    "            #print(out.shape)\n",
    "            outputs.append(out)\n",
    "        \n",
    "        if self.aggregation_type == 'add':\n",
    "            outputs = torch.stack(outputs, dim=0)\n",
    "            outputs = outputs.sum(dim=0)\n",
    "        elif self.aggregation_type == 'cat':\n",
    "            outputs = torch.cat(outputs, dim=1)\n",
    "        else:\n",
    "            raise ValueError(f'self.aggregation_type should be either \"add\" or \"cat\". Got {self.aggregation_type}')\n",
    "        return outputs\n",
    "\n",
    "segmentation_nns_factory_dict = {\n",
    "    'unet': smp.Unet,\n",
    "    'att_unet': UnetAtt,\n",
    "    'fpn': smp.FPN,\n",
    "    'custom_fpn': FPNMod,\n",
    "    'unet++': UnetPlusPlusMod,\n",
    "    'fcn': FCN,\n",
    "    'custom_manet': MAnetMod,\n",
    "}\n",
    "\n",
    "unet_aggregation_factory_dict = {\n",
    "    'concat': ConcatDim1,\n",
    "    'conv_cross_att': ConvCrossAttentionBlock,\n",
    "}\n",
    "\n",
    "unet_attention_factory_dict = {\n",
    "    'conv_msa': ConvMSABlock,\n",
    "    'none': nn.Identity,\n",
    "}\n",
    "\n",
    "criterion_factory_dict = {\n",
    "    'crossentropy': nn.CrossEntropyLoss,\n",
    "    'dice_crossentropy': DiceCELoss,\n",
    "    'dice': smp.losses.DiceLoss\n",
    "}\n",
    "\n",
    "optimizers_factory_dict = {\n",
    "    'adam': torch.optim.Adam,\n",
    "    'adamw': torch.optim.AdamW\n",
    "}\n",
    "\n",
    "lr_schedulers_factory_dict = {\n",
    "    'cosine_warm_restarts': torch.optim.lr_scheduler.CosineAnnealingWarmRestarts,\n",
    "    'plateau': torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    'cosine': torch.optim.lr_scheduler.CosineAnnealingLR,\n",
    "}\n",
    "\n",
    "transforms_factory_dict = {\n",
    "    'affine': v2.RandomAffine,\n",
    "    'perspective': v2.RandomPerspective,\n",
    "    'horizontal_flip': v2.RandomHorizontalFlip,\n",
    "    'vertical_flip': v2.RandomVerticalFlip,\n",
    "    'crop': v2.RandomCrop,\n",
    "    'gauss_noise': v2.GaussianNoise,\n",
    "    'gauss_blur': v2.GaussianBlur,\n",
    "    'elastic': v2.ElasticTransform,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4680fb11",
   "metadata": {},
   "source": [
    "# Конфигурации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2291919a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name_postfix': None,\n",
       " 'segmentation_nn': {'nn_architecture': 'unet++',\n",
       "  'params': {'encoder_name': 'efficientnet-b0',\n",
       "   'encoder_depth': 5,\n",
       "   'encoder_weights': 'imagenet',\n",
       "   'decoder_use_norm': 'batchnorm',\n",
       "   'decoder_channels': [256, 128, 128, 128, 128],\n",
       "   'decoder_attention_type': None,\n",
       "   'decoder_interpolation': 'nearest',\n",
       "   'in_channels': 3,\n",
       "   'classes': 11,\n",
       "   'activation': None,\n",
       "   'aux_params': None},\n",
       "  'input_layer_config': {'layer_path': 'encoder._conv_stem',\n",
       "   'replace_type': 'channels+stride',\n",
       "   'weight_update_type': 'repeate',\n",
       "   'params': {'stride': (1, 1), 'padding': (1, 1)}}},\n",
       " 'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
       " 'input_image_size': 96,\n",
       " 'loss': {'type': 'crossentropy',\n",
       "  'params': {'weight': None,\n",
       "   'ignore_index': -100,\n",
       "   'reduction': 'mean',\n",
       "   'label_smoothing': 0.15}},\n",
       " 'optimizer': {'type': 'adam', 'args': {}},\n",
       " 'lr_scheduler': {'type': 'cosine_warm_restarts',\n",
       "  'args': {'T_0': 25, 'T_mult': 1, 'eta_min': 0, 'last_epoch': -1},\n",
       "  'params': {'interval': 'epoch',\n",
       "   'frequency': 1,\n",
       "   'monitor': 'val_loss',\n",
       "   'strict': True,\n",
       "   'name': None}},\n",
       " 'device': 'cuda:0',\n",
       " 'batch_size': 16,\n",
       " 'epoch_num': 300,\n",
       " 'train_augmentations': {'gauss_noise': {'mean': 0.0,\n",
       "   'sigma': 0.0008,\n",
       "   'clip': False},\n",
       "  'affine': {'degrees': [0, 45],\n",
       "   'translate': [0, 0.3],\n",
       "   'scale': [0.7, 1.5],\n",
       "   'shear': [0, 0.2],\n",
       "   'fill': [0]},\n",
       "  'perspective': {'distortion_scale': 0.2, 'p': 0.3, 'fill': [0]},\n",
       "  'horizontal_flip': {'p': 0.5},\n",
       "  'vertical_flip': {'p': 0.5}},\n",
       " 'path_to_dataset_root': 'C:\\\\Users\\\\admin\\\\python_programming\\\\DATA\\\\DATA_FOR_TRAINIG_96'}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet_config_dict = {\n",
    "    'name_postfix': None,\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'unet',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': (256, 128, 128, 128, 128),\n",
    "            'decoder_attention_type': None,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96'\n",
    "}\n",
    "\n",
    "att_cat_agg_2_conv_unet_config_dict = {\n",
    "    'name_postfix': 'cat_agg_2conv',\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'att_unet',\n",
    "        'params': {\n",
    "            'decoder_layers_configs': [\n",
    "                {\n",
    "                    'interpolation_mode': \"nearest\",\n",
    "                    'aggregation_layer':{\n",
    "                        'layer': 'concat',\n",
    "                        'params': {},\n",
    "                    },\n",
    "                    'attention1': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'conv': [\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                    ],\n",
    "                    'attention2': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'interpolation_mode': \"nearest\",\n",
    "                    'aggregation_layer':{\n",
    "                        'layer': 'conv_cross_att',\n",
    "                        'params': {\n",
    "                            'in_kernel_size':3,\n",
    "                            'in_padding':1,\n",
    "                            'in_stride':1,\n",
    "                            \"head_row_dim\":6,\n",
    "                            'head_col_dim':6,\n",
    "                            'head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'attention1': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'conv': [\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                    ],\n",
    "                    'attention2': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'interpolation_mode': \"nearest\",\n",
    "                    'aggregation_layer':{\n",
    "                        'layer': 'conv_cross_att',\n",
    "                        'params': {\n",
    "                            'in_kernel_size':3,\n",
    "                            'in_padding':1,\n",
    "                            'in_stride':1,\n",
    "                            \"head_row_dim\":6,\n",
    "                            'head_col_dim':6,\n",
    "                            'head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'attention1': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'conv': [\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                    ],\n",
    "                    'attention2': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'interpolation_mode': \"nearest\",\n",
    "                    'aggregation_layer':{\n",
    "                        'layer': 'conv_cross_att',\n",
    "                        'params': {\n",
    "                            'in_kernel_size':3,\n",
    "                            'in_padding':1,\n",
    "                            'in_stride':1,\n",
    "                            \"head_row_dim\":6,\n",
    "                            'head_col_dim':6,\n",
    "                            'head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'attention1': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'conv': [\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                    ],\n",
    "                    'attention2': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'interpolation_mode': \"nearest\",\n",
    "                    'aggregation_layer':{\n",
    "                        'layer': 'conv_cross_att',\n",
    "                        'params': {\n",
    "                            'in_kernel_size':3,\n",
    "                            'in_padding':1,\n",
    "                            'in_stride':1,\n",
    "                            \"head_row_dim\":6,\n",
    "                            'head_col_dim':6,\n",
    "                            'head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'attention1': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'conv': [\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                    ],\n",
    "                    'attention2': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    }\n",
    "                },\n",
    "            ],\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': (256, 128, 128, 64, 64),\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 4,\n",
    "    'epoch_num':4,\n",
    "    'train_augmentations': {\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    #'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96'    \n",
    "}\n",
    "\n",
    "att_cross_agg_2_conv_unet_config_dict = {\n",
    "    'name_postfix': 'cross_agg_2conv',\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'att_unet',\n",
    "        'params': {\n",
    "            'decoder_layers_configs': [\n",
    "                {\n",
    "                    'interpolation_mode': \"nearest\",\n",
    "                    'aggregation_layer':{\n",
    "                        'layer': 'conv_cross_att',\n",
    "                        'params': {\n",
    "                            'in_kernel_size':3,\n",
    "                            'in_padding':1,\n",
    "                            'in_stride':1,\n",
    "                            \"head_row_dim\":6,\n",
    "                            'head_col_dim':6,\n",
    "                            'head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'attention1': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'conv': [\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                    ],\n",
    "                    'attention2': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'interpolation_mode': \"nearest\",\n",
    "                    'aggregation_layer':{\n",
    "                        'layer': 'conv_cross_att',\n",
    "                        'params': {\n",
    "                            'in_kernel_size':3,\n",
    "                            'in_padding':1,\n",
    "                            'in_stride':1,\n",
    "                            \"head_row_dim\":6,\n",
    "                            'head_col_dim':6,\n",
    "                            'head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'attention1': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'conv': [\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                    ],\n",
    "                    'attention2': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'interpolation_mode': \"nearest\",\n",
    "                    'aggregation_layer':{\n",
    "                        'layer': 'conv_cross_att',\n",
    "                        'params': {\n",
    "                            'in_kernel_size':3,\n",
    "                            'in_padding':1,\n",
    "                            'in_stride':1,\n",
    "                            \"head_row_dim\":6,\n",
    "                            'head_col_dim':6,\n",
    "                            'head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'attention1': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'conv': [\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                    ],\n",
    "                    'attention2': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'interpolation_mode': \"nearest\",\n",
    "                    'aggregation_layer':{\n",
    "                        'layer': 'conv_cross_att',\n",
    "                        'params': {\n",
    "                            'in_kernel_size':3,\n",
    "                            'in_padding':1,\n",
    "                            'in_stride':1,\n",
    "                            \"head_row_dim\":6,\n",
    "                            'head_col_dim':6,\n",
    "                            'head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'attention1': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'conv': [\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                    ],\n",
    "                    'attention2': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'interpolation_mode': \"nearest\",\n",
    "                    'aggregation_layer':{\n",
    "                        'layer': 'conv_cross_att',\n",
    "                        'params': {\n",
    "                            'in_kernel_size':3,\n",
    "                            'in_padding':1,\n",
    "                            'in_stride':1,\n",
    "                            \"head_row_dim\":6,\n",
    "                            'head_col_dim':6,\n",
    "                            'head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'attention1': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'conv': [\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                    ],\n",
    "                    'attention2': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    }\n",
    "                },\n",
    "            ],\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': (256, 128, 128, 64, 64),\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 8,\n",
    "    'epoch_num': 300,\n",
    "    'train_augmentations': {\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    #'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96'    \n",
    "}\n",
    "\n",
    "unetpp_config_dict = {\n",
    "    'name_postfix': None,\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'unet++',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': [256, 128, 128, 128, 128],\n",
    "            'decoder_attention_type': None,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "       'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96'\n",
    "}\n",
    "\n",
    "scse_unet_config_dict = {\n",
    "    'name_postfix': 'scse_att',\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'unet',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': (256, 128, 128, 128, 128),\n",
    "            'decoder_attention_type': 'scse',\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'average_all',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "       'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96'\n",
    "}\n",
    "\n",
    "unet_mit_config_dict = {\n",
    "    'name_postfix': None,\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'unet',\n",
    "        'params': {\n",
    "            'encoder_name': \"mit_b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': (256, 128, 128, 128, 128),\n",
    "            'decoder_attention_type': None,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder.patch_embed1.proj',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'average_all',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (3, 3),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "       'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96'\n",
    "}\n",
    "\n",
    "unet_maxvit_config_dict = {\n",
    "    'name_postfix': None,\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'unet',\n",
    "        'params': {\n",
    "            'encoder_name': 'tu-maxvit_tiny_rw_224',\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': (256, 128, 128, 128, 128),\n",
    "            'decoder_attention_type': None,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "            'img_size':96,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder.model.stem.conv1',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'average_all',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "       'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    #'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    'path_to_dataset_root': r'/home/mikhail_u/develop/DATA/DATA_FOR_TRAINIG_96'\n",
    "}\n",
    "\n",
    "net_hgnetv2_config_dict = {\n",
    "    'name_postfix': None,\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'unet',\n",
    "        'params': {\n",
    "            'encoder_name': 'tu-hgnetv2_b1',\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': (256, 128, 128, 128, 128),\n",
    "            'decoder_attention_type': None,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "            #'img_size':96,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder.model.stem.stem1.conv',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'average_all',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "       'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    #'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    'path_to_dataset_root': r'/home/mikhail_u/develop/DATA/DATA_FOR_TRAINIG_96'\n",
    "}\n",
    "\n",
    "net_mambaout_config_dict = {\n",
    "    'name_postfix': None,\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'unet',\n",
    "        'params': {\n",
    "            'encoder_name': 'tu-mambaout_small',\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': (256, 128, 128, 128, 128),\n",
    "            'decoder_attention_type': None,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "            #'img_size':96,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder.model.stem.conv1',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'average_all',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "       'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    #'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    'path_to_dataset_root': r'/home/mikhail_u/develop/DATA/DATA_FOR_TRAINIG_96'\n",
    "}\n",
    "\n",
    "unet_hrnet_config_dict = {\n",
    "    'name_postfix': None,\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'unet',\n",
    "        'params': {\n",
    "            'encoder_name': 'tu-hrnet_w18',\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': (256, 128, 128, 128, 128),\n",
    "            'decoder_attention_type': None,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "            #'img_size':96,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder.model.conv1',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'average_all',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "       'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    #'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    'path_to_dataset_root': r'/home/mikhail_u/develop/DATA/DATA_FOR_TRAINIG_96'\n",
    "}\n",
    "\n",
    "unet_cspresnext_config_dict = {\n",
    "    'name_postfix': None,\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'unet',\n",
    "        'params': {\n",
    "            'encoder_name': 'tu-cspresnext50',\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': (256, 128, 128, 128, 128),\n",
    "            'decoder_attention_type': None,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "            #'img_size':96,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder.model.stem_conv1.conv',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'average_all',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "       'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'/home/mikhail_u/develop/DATA/DATA_FOR_TRAINIG_96'\n",
    "}\n",
    "\n",
    "fpn_config_dict = {\n",
    "    'name_postfix': None,\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'custom_fpn',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_pyramid_channels': 128,\n",
    "            'decoder_segmentation_channels': 128,\n",
    "            'decoder_merge_policy': \"add\",\n",
    "            'decoder_dropout': 0.2,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'upsampling': 0,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                },\n",
    "        },\n",
    "    },\n",
    "    'multispecter_bands_indices': [1, 2, 3, 7, 'ndvi'],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'},\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "       'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "}\n",
    "\n",
    "fcn_config_dict = {\n",
    "    'name_postfix': None,\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'fcn',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': 'batchnorm',\n",
    "            'decoder_last_channel': 16,\n",
    "            'decoder_attention_type': None,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation':  None,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                },\n",
    "        },\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "        #'params': {},\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "       'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "}\n",
    "\n",
    "fpn_config_dict_dice_ce = {\n",
    "    'name_postfix': None,\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'custom_fpn',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_pyramid_channels': 128,\n",
    "            'decoder_segmentation_channels': 128,\n",
    "            'decoder_merge_policy': \"add\",\n",
    "            'decoder_dropout': 0.2,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'upsampling': 0,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                },\n",
    "        },\n",
    "    },\n",
    "    'multispecter_bands_indices': [1, 2, 3, 7, 'ndvi'],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'dice_crossentropy',\n",
    "        #'params': {'weight': 'classes'},\n",
    "        'params': {\n",
    "            'ce_weight':None,\n",
    "            'ce_ignore_index':-100,\n",
    "            'ce_reducion':'mean',\n",
    "            'ce_label_smoothing':0.15,\n",
    "            'dice_mode':'multiclass',\n",
    "            'dice_classes': None,\n",
    "            'dice_log_loss':False,\n",
    "            'dice_from_logits':True,\n",
    "            'dice_smooth':0.15,\n",
    "            'dice_ignore_index':-100,\n",
    "            'dice_eps': 1e-7,\n",
    "            'losses_weight': [0.5, 0.5],\n",
    "            'is_trainable_weights': True,\n",
    "            'weights_processing_type': 'softmax',\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "       'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "}\n",
    "\n",
    "fpn_config_dict_dice = {\n",
    "    'name_postfix': None,\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'custom_fpn',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_pyramid_channels': 128,\n",
    "            'decoder_segmentation_channels': 128,\n",
    "            'decoder_merge_policy': \"add\",\n",
    "            'decoder_dropout': 0.2,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'upsampling': 0,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                },\n",
    "        },\n",
    "    },\n",
    "    'multispecter_bands_indices': [1, 2, 3, 7, 'ndvi'],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'dice',\n",
    "        #'params': {'weight': 'classes'},\n",
    "        'params': {\n",
    "            'mode':'multiclass',\n",
    "            'classes': None,\n",
    "            'log_loss':False,\n",
    "            'from_logits':True,\n",
    "            'smooth':0.15,\n",
    "            'ignore_index':-100,\n",
    "            'eps': 1e-7,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "       'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "}\n",
    "\n",
    "fpn_config_dict_ce = {\n",
    "    'name_postfix': None,\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'custom_fpn',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_pyramid_channels': 128,\n",
    "            'decoder_segmentation_channels': 128,\n",
    "            'decoder_merge_policy': \"add\",\n",
    "            'decoder_dropout': 0.2,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'upsampling': 0,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'average_all',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                },\n",
    "        },\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'},\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size':16,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "       'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "}\n",
    "\n",
    "fpn_multisize_input_config_dict_ce = {\n",
    "    'name_postfix': None,\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'custom_fpn',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_pyramid_channels': 128,\n",
    "            'decoder_segmentation_channels': 128,\n",
    "            'decoder_merge_policy': \"add\",\n",
    "            'decoder_dropout': 0.2,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'upsampling': 0,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'multisize_conv',\n",
    "            'weight_update_type': 'repeat', # avearge_all OR repeate\n",
    "            'params':{\n",
    "                'in_channels': 3,\n",
    "                'out_channels': {\n",
    "                    '1x1': 32,\n",
    "                    '3x3': 32,\n",
    "                    #'5x5': 32, \n",
    "                },\n",
    "                'kernel_size': {\n",
    "                    '1x1': 1,\n",
    "                    '3x3': 3,\n",
    "                    #'5x5': 5, \n",
    "                },\n",
    "                'stride': {\n",
    "                    '1x1': 1,\n",
    "                    '3x3': 1,\n",
    "                    #'5x5': 1, \n",
    "                },\n",
    "                'padding': {\n",
    "                    '1x1': 0,\n",
    "                    '3x3': 1,\n",
    "                    #'5x5': 2, \n",
    "                },\n",
    "                'dilation': {\n",
    "                    '1x1': 1,\n",
    "                    '3x3': 1,\n",
    "                    #'5x5': 1, \n",
    "                },\n",
    "                'groups': {\n",
    "                    '1x1': 1,\n",
    "                    '3x3': 1,\n",
    "                    #'5x5': 1, \n",
    "                },\n",
    "                'bias': {\n",
    "                    '1x1': False,\n",
    "                    '3x3': False,\n",
    "                    #'5x5': False, \n",
    "                },\n",
    "                'aggregation_type': 'add',\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'},\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "       'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    #'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "}\n",
    "\n",
    "fpn_mit_config_dict_ce = {\n",
    "    'name_postfix': None,\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'custom_fpn',\n",
    "        'params': {\n",
    "            'encoder_name': \"mit_b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_pyramid_channels': 128,\n",
    "            'decoder_segmentation_channels': 128,\n",
    "            'decoder_merge_policy': \"add\",\n",
    "            'decoder_dropout': 0.2,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'upsampling': 0,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                },\n",
    "        },\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'},\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "       'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "}\n",
    "\n",
    "custom_manet_config_dict = {\n",
    "    'name_postfix': None,\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'custom_manet',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': [256, 128, 64, 64, 64],\n",
    "            'decoder_pab_channels': 64,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                },\n",
    "        },\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'},\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size':8,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "       'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "}\n",
    "\n",
    "config_dict = unetpp_config_dict\n",
    "\n",
    "config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ae8eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_to_save = [\n",
    "    unet_config_dict,\n",
    "    att_cat_agg_2_conv_unet_config_dict,\n",
    "    unetpp_config_dict,\n",
    "    unet_mit_config_dict,\n",
    "    unet_maxvit_config_dict,\n",
    "    net_hgnetv2_config_dict,\n",
    "    net_mambaout_config_dict,\n",
    "    unet_hrnet_config_dict,\n",
    "    unet_cspresnext_config_dict,\n",
    "    fpn_config_dict,\n",
    "    fcn_config_dict,\n",
    "    fpn_config_dict_dice_ce,\n",
    "    fpn_config_dict_dice,\n",
    "    fpn_config_dict_ce,\n",
    "    fpn_multisize_input_config_dict_ce,\n",
    "    custom_manet_config_dict,\n",
    "]\n",
    "for config_dict in configs_to_save:\n",
    "    name_postfix = config_dict[\"name_postfix\"]\n",
    "    model_name = f'{config_dict[\"segmentation_nn\"][\"nn_architecture\"]}_{config_dict[\"segmentation_nn\"][\"params\"][\"encoder_name\"]}'\n",
    "    if name_postfix is not None:\n",
    "        model_name = f'{model_name}_{name_postfix}'\n",
    "        \n",
    "    path_to_save = os.path.join('training_configs', f'{model_name}.yaml')\n",
    "    with open(path_to_save, 'w', encoding='utf-8') as fd:\n",
    "        yaml.dump(config_dict, fd, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d8ee3a",
   "metadata": {},
   "source": [
    "# Создание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5037b82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_dataset_root = config_dict['path_to_dataset_root']\n",
    "\n",
    "path_to_dataset_info_csv = os.path.join(path_to_dataset_root, 'data_info_table.csv')\n",
    "path_to_surface_classes_json = os.path.join(path_to_dataset_root, 'surface_classes.json')\n",
    "\n",
    "input_image_size = config_dict['input_image_size']\n",
    "multispecter_bands_indices = config_dict['multispecter_bands_indices']\n",
    "device = config_dict['device']\n",
    "\n",
    "# чтение списка имен классов поверхностей\n",
    "with open(path_to_surface_classes_json) as fd:\n",
    "    surface_classes_list = json.load(fd)\n",
    "# чтение таблицы с информацией о каждом изображении в выборке\n",
    "images_df = pd.read_csv(path_to_dataset_info_csv)\n",
    "\n",
    "path_to_partition_json = os.path.join(path_to_dataset_root, 'dataset_partition.json')\n",
    "# чтение словаря со списками квадратов, находящихся в обучающей и тестовой выборке\n",
    "with open(path_to_partition_json) as fd:\n",
    "    partition_dict = json.load(fd)\n",
    "\n",
    "# формирование pandas DataFrame-ов с информацией об изображениях обучающей и тестовой выборках\n",
    "train_images_df = []\n",
    "for train_square in partition_dict['train_squares']:\n",
    "    train_images_df.append(images_df[images_df['square_id']==train_square])\n",
    "train_images_df = pd.concat(train_images_df, ignore_index=True)\n",
    "\n",
    "test_images_df = []\n",
    "for test_square in partition_dict['test_squares']:\n",
    "    test_images_df.append(images_df[images_df['square_id']==test_square])\n",
    "test_images_df = pd.concat(test_images_df, ignore_index=True)\n",
    "\n",
    "#train_images_df, test_images_df = train_test_split(images_df, test_size=0.3, random_state=0)\n",
    "\n",
    "class_num = images_df['class_num'].iloc[0]\n",
    "\n",
    "# формирование словаря, отображающейго имя класса поверхности в индекс класса\n",
    "class_name2idx_dict = {n:i for i, n in enumerate(surface_classes_list)}\n",
    "\n",
    "# вычисление распределений пикселей в классах поверхностей \n",
    "classes_pixels_distribution_df = images_df[surface_classes_list]\n",
    "classes_pixels_num = classes_pixels_distribution_df.sum()\n",
    "classes_weights = classes_pixels_num / classes_pixels_num.sum()\n",
    "classes_weights = classes_weights[surface_classes_list].to_numpy().astype(np.float32)\n",
    "\n",
    "input_image_size = config_dict['input_image_size']\n",
    "'''\n",
    "train_transforms = v2.Compose(\n",
    "    [v2.Resize((input_image_size,input_image_size), antialias=True),v2.ToDtype(torch.float32, scale=True)])\n",
    "test_transforms = v2.Compose(\n",
    "    [v2.Resize((input_image_size,input_image_size), antialias=True),v2.ToDtype(torch.float32, scale=True)])\n",
    "'''\n",
    "train_transforms = create_augmentation_transforms(config_dict['train_augmentations'])\n",
    "test_transforms = nn.Identity()\n",
    "# если ф-ция потерь перекрестная энтропия, то проверяем, есть ли там веса классов\n",
    "if config_dict['loss']['type'] == 'crossentropy':\n",
    "    # если в параметрах функции потерь стоит строка 'classes', надо передать в функцию вектор весов классов\n",
    "    if 'weight' in config_dict['loss']['params']:\n",
    "        if isinstance(config_dict['loss']['params']['weight'], (list, tuple)):\n",
    "            config_dict['loss']['params']['weight'] = torch.tensor(config_dict['loss']['params']['weight'])\n",
    "        \n",
    "        elif config_dict['loss']['params']['weight'] is not None:\n",
    "            config_dict['loss']['params']['weight'] = torch.tensor(classes_weights)\n",
    "\n",
    "# создание функции потерь\n",
    "criterion = criterion_factory_dict[config_dict['loss']['type']](**config_dict['loss']['params'])\n",
    "\n",
    "# если ф-ция потерь перекрестная энтропия, то проверяем, есть ли там веса классов\n",
    "if config_dict['loss']['type'] == 'crossentropy':\n",
    "    # если в параметрах функции потерь стоит строка 'classes', надо передать в функцию вектор весов классов\n",
    "    if 'weight' in config_dict['loss']['params']:\n",
    "        if isinstance(config_dict['loss']['params']['weight'], torch.Tensor):\n",
    "            config_dict['loss']['params']['weight'] = config_dict['loss']['params']['weight'].cpu().tolist()\n",
    "\n",
    "model = create_model(config_dict, segmentation_nns_factory_dict)\n",
    "model = model.to(device)\n",
    "\n",
    "# создаем датасеты и даталоадеры\n",
    "train_dataset = SegmentationDataset(path_to_dataset_root=path_to_dataset_root, samples_df=train_images_df, channel_indices=multispecter_bands_indices, transforms=train_transforms, dtype=torch.float32, device=device)\n",
    "test_dataset = SegmentationDataset(path_to_dataset_root=path_to_dataset_root, samples_df=test_images_df, channel_indices=multispecter_bands_indices, transforms=test_transforms, dtype=torch.float32, device=device)\n",
    "#train_dataset = SegmentationDatasetApplSurf(path_to_dataset_root=path_to_dataset_root, samples_df=test_images_df, channel_indices=channel_indices, name2class_idx_dict=class_name2idx_dict, applicable_surfaces_dict=applicable_surfaces_dict, transforms=test_transforms, device=device)\n",
    "#test_dataset = SegmentationDatasetApplSurf(path_to_dataset_root=path_to_dataset_root, samples_df=test_images_df, channel_indices=channel_indices, name2class_idx_dict=class_name2idx_dict, applicable_surfaces_dict=applicable_surfaces_dict, transforms=test_transforms, device=device)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config_dict['batch_size'], shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=config_dict['batch_size'])\n",
    "\n",
    "# тестовое чтение данных\n",
    "for data, labels in test_loader:\n",
    "    break\n",
    "    pred = model(data)\n",
    "    loss = criterion(pred, labels)\n",
    "    loss.backward()\n",
    "    \n",
    "# тестовая обработка данных нейронной сетью\n",
    "ret = model(data)\n",
    "print(data.shape, ret.shape)\n",
    "\n",
    "createion_time_str = datetime.now().strftime('%Y-%m-%dT%H-%M-%S')\n",
    "nn_arch_str = config_dict[\"segmentation_nn\"][\"nn_architecture\"]\n",
    "nn_encoder_str = config_dict[\"segmentation_nn\"][\"params\"][\"encoder_name\"]\n",
    "name_postfix = config_dict[\"name_postfix\"]\n",
    "if name_postfix is not None:\n",
    "    model_name = f'{nn_arch_str}_{nn_encoder_str}_{name_postfix} {createion_time_str}'\n",
    "else:\n",
    "    model_name = f'{nn_arch_str}_{nn_encoder_str} {createion_time_str}'\n",
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f4c339",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9e6b7cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:658: Checkpoint directory C:\\Users\\mokhail\\develop\\MultispectralSegmentation\\saving_dir\\att_unet_efficientnet-b0 2025-09-21T23-47-28 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | UnetAtt          | 18.4 M | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "18.4 M    Trainable params\n",
      "0         Non-trainable params\n",
      "18.4 M    Total params\n",
      "73.480    Total estimated model params size (MB)\n",
      "530       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############################\n",
      "att_unet_efficientnet-b0 2025-09-21T23-47-28\n",
      "#############################\n",
      "\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 424/424 [25:34<00:00,  0.28it/s, v_num=0, val_loss=1.550, val_iou_UNLABELED=0.541, val_iou_buildings_territory=0.200, val_iou_natural_ground=0.000, val_iou_natural_grow=0.000144, val_iou_natural_wetland=4.14e-6, val_iou_natural_wood=0.651, val_iou_quasi_natural_ground=0.00827, val_iou_quasi_natural_grow=0.295, val_iou_quasi_natural_wetland=0.000, val_iou_transport=0.000, val_iou_water=0.236, val_iou_mean=0.176, val_precision_UNLABELED=0.671, val_precision_buildings_territory=0.345, val_precision_natural_ground=0.000, val_precision_natural_grow=0.0795, val_precision_natural_wetland=0.0117, val_precision_natural_wood=0.669, val_precision_quasi_natural_ground=0.101, val_precision_quasi_natural_grow=0.401, val_precision_quasi_natural_wetland=0.000, val_precision_transport=0.000, val_precision_water=0.989, val_precision_mean=0.297, val_recall_UNLABELED=0.671, val_recall_buildings_territory=0.345, val_recall_natural_ground=0.000, val_recall_natural_grow=0.0795, val_recall_natural_wetland=0.0117, val_recall_natural_wood=0.669, val_recall_quasi_natural_ground=0.101, val_recall_quasi_natural_grow=0.401, val_recall_quasi_natural_wetland=0.000, val_recall_transport=0.000, val_recall_water=0.989, val_recall_mean=0.297, train_loss=1.570, train_iou_UNLABELED=0.693, train_iou_buildings_territory=0.158, train_iou_natural_ground=2.69e-5, train_iou_natural_grow=0.0274, train_iou_natural_wetland=0.000207, train_iou_natural_wood=0.563, train_iou_quasi_natural_ground=8.49e-5, train_iou_quasi_natural_grow=0.167, train_iou_quasi_natural_wetland=0.000, train_iou_transport=9.29e-6, train_iou_water=0.128, train_iou_mean=0.158, train_precision_UNLABELED=0.802, train_precision_buildings_territory=0.277, train_precision_natural_ground=0.00134, train_precision_natural_grow=0.201, train_precision_natural_wetland=0.0414, train_precision_natural_wood=0.611, train_precision_quasi_natural_ground=0.00748, train_precision_quasi_natural_grow=0.290, train_precision_quasi_natural_wetland=0.000, train_precision_transport=0.00503, train_precision_water=0.359, train_precision_mean=0.236, train_recall_UNLABELED=0.802, train_recall_buildings_territory=0.277, train_recall_natural_ground=0.00134, train_recall_natural_grow=0.201, train_recall_natural_wetland=0.0414, train_recall_natural_wood=0.611, train_recall_quasi_natural_ground=0.00748, train_recall_quasi_natural_grow=0.290, train_recall_quasi_natural_wetland=0.000, train_recall_transport=0.00503, train_recall_water=0.359, train_recall_mean=0.236]              "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=4` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3: 100%|██████████| 424/424 [25:34<00:00,  0.28it/s, v_num=0, val_loss=1.550, val_iou_UNLABELED=0.541, val_iou_buildings_territory=0.200, val_iou_natural_ground=0.000, val_iou_natural_grow=0.000144, val_iou_natural_wetland=4.14e-6, val_iou_natural_wood=0.651, val_iou_quasi_natural_ground=0.00827, val_iou_quasi_natural_grow=0.295, val_iou_quasi_natural_wetland=0.000, val_iou_transport=0.000, val_iou_water=0.236, val_iou_mean=0.176, val_precision_UNLABELED=0.671, val_precision_buildings_territory=0.345, val_precision_natural_ground=0.000, val_precision_natural_grow=0.0795, val_precision_natural_wetland=0.0117, val_precision_natural_wood=0.669, val_precision_quasi_natural_ground=0.101, val_precision_quasi_natural_grow=0.401, val_precision_quasi_natural_wetland=0.000, val_precision_transport=0.000, val_precision_water=0.989, val_precision_mean=0.297, val_recall_UNLABELED=0.671, val_recall_buildings_territory=0.345, val_recall_natural_ground=0.000, val_recall_natural_grow=0.0795, val_recall_natural_wetland=0.0117, val_recall_natural_wood=0.669, val_recall_quasi_natural_ground=0.101, val_recall_quasi_natural_grow=0.401, val_recall_quasi_natural_wetland=0.000, val_recall_transport=0.000, val_recall_water=0.989, val_recall_mean=0.297, train_loss=1.570, train_iou_UNLABELED=0.693, train_iou_buildings_territory=0.158, train_iou_natural_ground=2.69e-5, train_iou_natural_grow=0.0274, train_iou_natural_wetland=0.000207, train_iou_natural_wood=0.563, train_iou_quasi_natural_ground=8.49e-5, train_iou_quasi_natural_grow=0.167, train_iou_quasi_natural_wetland=0.000, train_iou_transport=9.29e-6, train_iou_water=0.128, train_iou_mean=0.158, train_precision_UNLABELED=0.802, train_precision_buildings_territory=0.277, train_precision_natural_ground=0.00134, train_precision_natural_grow=0.201, train_precision_natural_wetland=0.0414, train_precision_natural_wood=0.611, train_precision_quasi_natural_ground=0.00748, train_precision_quasi_natural_grow=0.290, train_precision_quasi_natural_wetland=0.000, train_precision_transport=0.00503, train_precision_water=0.359, train_precision_mean=0.236, train_recall_UNLABELED=0.802, train_recall_buildings_territory=0.277, train_recall_natural_ground=0.00134, train_recall_natural_grow=0.201, train_recall_natural_wetland=0.0414, train_recall_natural_wood=0.611, train_recall_quasi_natural_ground=0.00748, train_recall_quasi_natural_grow=0.290, train_recall_quasi_natural_wetland=0.000, train_recall_transport=0.00503, train_recall_water=0.359, train_recall_mean=0.236]\n"
     ]
    }
   ],
   "source": [
    "epoch_num = config_dict['epoch_num']\n",
    "\n",
    "print('#############################')\n",
    "print(model_name)\n",
    "print('#############################')\n",
    "print()\n",
    "\n",
    "# создаем список словарей с информацией о вычисляемых метриках с помощью multiclass confusion matrix\n",
    "# см. подробнее ддокументацию к функции compute_metric_from_confusion\n",
    "metrics_dict = {\n",
    "    'train': {\n",
    "        'iou': classification.JaccardIndex(task='multiclass', average='none', num_classes=len(class_name2idx_dict)).to(device),\n",
    "        'precision': classification.Precision(task='multiclass', average='none', num_classes=len(class_name2idx_dict)).to(device),\n",
    "        'recall': classification.Precision(task='multiclass', average='none', num_classes=len(class_name2idx_dict)).to(device),\n",
    "        #'confusion': classification.ConfusionMatrix(task='multiclass', num_classes=len(class_name2idx_dict)).to(device),\n",
    "    },\n",
    "    'val': {\n",
    "        'iou': classification.JaccardIndex(task='multiclass', average='none', num_classes=len(class_name2idx_dict)).to(device),\n",
    "        'precision': classification.Precision(task='multiclass', average='none', num_classes=len(class_name2idx_dict)).to(device),\n",
    "        'recall': classification.Precision(task='multiclass', average='none', num_classes=len(class_name2idx_dict)).to(device),\n",
    "        #'confusion': classification.ConfusionMatrix(task='multiclass', num_classes=len(class_name2idx_dict)).to(device),\n",
    "    }\n",
    "}\n",
    "\n",
    "optimizer_cfg = {\n",
    "    'optmizer': optimizers_factory_dict[config_dict['optimizer']['type']],\n",
    "    'optimizer_args':config_dict['optimizer']['args'],\n",
    "    'lr_scheduler': lr_schedulers_factory_dict[config_dict['lr_scheduler']['type']],\n",
    "    'lr_scheduler_args': config_dict['lr_scheduler']['args'],\n",
    "    'lr_scheduler_params': config_dict['lr_scheduler']['params']\n",
    "\n",
    "}\n",
    "\n",
    "# Создаем модуль Lightning\n",
    "segmentation_module = LightningSegmentationModule(model, criterion, optimizer_cfg, metrics_dict, class_name2idx_dict)\n",
    "\n",
    "# задаем путь до папки с логгерами и создаем логгер, записывающий результаты в csv\n",
    "path_to_saving_dir = 'saving_dir'\n",
    "csv_logger = CSVLogger(\n",
    "    save_dir = path_to_saving_dir,\n",
    "    name=model_name, \n",
    "    flush_logs_every_n_steps=1,\n",
    "    )\n",
    "\n",
    "# создаем объект, записывающий в чекпоинт лучшую модель\n",
    "path_to_save_model_dir = os.path.join(path_to_saving_dir, model_name)\n",
    "os.makedirs(path_to_save_model_dir, exist_ok=True)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    mode=\"max\",\n",
    "    filename=model_name+\"-{epoch:02d}-{val_iou_mean:.3}\",\n",
    "    dirpath=path_to_save_model_dir, \n",
    "    save_top_k=1, monitor=\"val_iou_mean\"\n",
    "    )\n",
    "\n",
    "trainer = L.Trainer(logger=[csv_logger],\n",
    "        max_epochs=epoch_num, \n",
    "        callbacks=[checkpoint_callback],\n",
    "        accelerator = 'gpu'\n",
    "        )\n",
    "\n",
    "# сохраняем конфигурацию\n",
    "path_to_config = os.path.join(path_to_save_model_dir, 'training_config.yaml')\n",
    "with open(path_to_config, 'w', encoding='utf-8') as fd:\n",
    "    #json.dump(config_dict, fd, indent=4)\n",
    "    yaml.dump(config_dict, fd, indent=4)\n",
    "\n",
    "trainer.fit(segmentation_module , train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ea282e",
   "metadata": {},
   "source": [
    "# Черновики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5674eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.MAnet()\n",
    "model.encoder.conv1.stride=(1,1)\n",
    "\n",
    "def custom_forward(self, x):\n",
    "    \"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\n",
    "    features = self.encoder(x)\n",
    "    '''\n",
    "    for f in features:\n",
    "        print(f.shape)\n",
    "    print()\n",
    "    '''\n",
    "\n",
    "    decoder_output = self.decoder(features)\n",
    "    #print(decoder_output.shape)\n",
    "\n",
    "    masks = self.segmentation_head(decoder_output)\n",
    "\n",
    "    if self.classification_head is not None:\n",
    "        labels = self.classification_head(features[-1])\n",
    "        return masks, labels\n",
    "\n",
    "    return masks\n",
    "\n",
    "\n",
    "def decoder_custom_forward(self, features: List[torch.Tensor]) -> torch.Tensor:\n",
    "    features = features[1:]  # remove first skip with same spatial resolution\n",
    "    features = features[::-1]  # reverse channels to start from head of encoder\n",
    "\n",
    "    head = features[0]\n",
    "    skips = features[1:]\n",
    "    print('Features shapes')\n",
    "    print(f'head:{head.shape}')\n",
    "    print('Skips:')\n",
    "    for skip in skips:\n",
    "        print(skip.shape)\n",
    "\n",
    "    print('----------------------')\n",
    "\n",
    "    x = self.center(head)\n",
    "    print(f'Center:{x.shape}')\n",
    "\n",
    "    for i, decoder_block in enumerate(self.blocks):\n",
    "        skip = skips[i] if i < len(skips) else None\n",
    "        x = decoder_block(x, skip)\n",
    "        if skip is not None:\n",
    "            print(f'x:{x.shape}; skip:{skip.shape}')\n",
    "        else:\n",
    "            print(f'x:{x.shape}; skip:{skip}')\n",
    "\n",
    "    return x\n",
    "\n",
    "model.forward = types.MethodType(custom_forward, model)\n",
    "model.decoder.forward = types.MethodType(decoder_custom_forward, model.decoder)\n",
    "\n",
    "ret = model(torch.randn(1, 3, 96, 96))\n",
    "ret.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c98c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_in_batch = 1\n",
    "#transform = v2.RandomPerspective(distortion_scale=0.3, p=1.0, fill={tv_tensors.Image:0.0, tv_tensors.Mask:0})\n",
    "transform = v2.RandomAffine(degrees=[0, 30], translate=[0.0, 0.3], scale=[0.3, 0.5], shear=[0.0, 0.4], fill=[0])\n",
    "#transform = v2.RandomResize(min_size=96, max_size=256)\n",
    "#transform = v2.RandomRotation(degrees=(0, 45))\n",
    "\n",
    "transforms_factory_dict = {\n",
    "    'affine': v2.RandomAffine,\n",
    "    'perspective': v2.RandomPerspective,\n",
    "    'horizontal_flip': v2.RandomHorizontalFlip,\n",
    "    'vertical_flip': v2.RandomVerticalFlip,\n",
    "    'crop': v2.RandomCrop,\n",
    "    'gauss_noise': v2.GaussianNoise,\n",
    "    'gauss_blur': v2.GaussianBlur,\n",
    "    'elastic': v2.ElasticTransform,\n",
    "}\n",
    "\n",
    "def create_transforms(transforms_dict:Dict[str, Dict]):\n",
    "    transforms_list = []\n",
    "    for name, transform_params in transforms_dict.items():\n",
    "        transform_creation_fn = transforms_factory_dict[name]\n",
    "        transforms_list.append(transform_creation_fn(**transform_params))\n",
    "    #return v2.Compose([v2.RandomOrder(transforms_list)])\n",
    "    return v2.RandomOrder(transforms_list)\n",
    "        \n",
    "transforms_dict = {\n",
    "        'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "}\n",
    "\n",
    "transforms = create_transforms(transforms_dict)\n",
    "transforms\n",
    "\n",
    "mask = tv_tensors.Mask(labels[idx_in_batch])\n",
    "\n",
    "to_transform = {'image':data[idx_in_batch], 'mask':mask}\n",
    "\n",
    "out = transforms(to_transform)\n",
    "img_tr, mask_tr = out['image'].detach().cpu(), out['mask'].detach().cpu()\n",
    "\n",
    "img = data[idx_in_batch].detach().cpu()\n",
    "mask = labels[idx_in_batch].detach().cpu()\n",
    "\n",
    "fig, axs = plt.subplots(2,2)\n",
    "axs[0,0].imshow(img[idx_in_batch])\n",
    "axs[0,1].imshow(mask)\n",
    "axs[1,0].imshow(img_tr[idx_in_batch])\n",
    "axs[1,1].imshow(mask_tr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7f0022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "model = smp.Unet(encoder_name='resnet34')\n",
    "#input_weight = model.encoder._conv_stem.weight.detach()#.numpy()\n",
    "input_weight = model.encoder.conv1.weight.detach()#.numpy()\n",
    "interpolated_weight = F.interpolate(input_weight, size=(11, 11), mode='bicubic', antialias=True, align_corners=False)\n",
    "\n",
    "print(input_weight.shape)\n",
    "print(interpolated_weight.shape)\n",
    "\n",
    "filter_index = 9\n",
    "conv_filter = input_weight[filter_index]\n",
    "interpolated_conv_filter = interpolated_weight[filter_index]\n",
    "fig1, axs1 = plt.subplots(1,3)\n",
    "for idx, img in enumerate(conv_filter):\n",
    "    axs1[idx].imshow(img)\n",
    "\n",
    "fig2, axs2 = plt.subplots(1,3)\n",
    "for idx, img in enumerate(interpolated_conv_filter):\n",
    "    axs2[idx].imshow(img)\n",
    "\n",
    "\n",
    "filter_index += 1\n",
    "conv_filter = input_weight[filter_index]\n",
    "interpolated_conv_filter = interpolated_weight[filter_index]\n",
    "fig3, axs3 = plt.subplots(1,3)\n",
    "for idx, img in enumerate(conv_filter):\n",
    "    axs3[idx].imshow(img)\n",
    "\n",
    "fig4, axs4 = plt.subplots(1,3)\n",
    "for idx, img in enumerate(interpolated_conv_filter):\n",
    "    axs4[idx].imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "35c0200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [1, 2, 3, 4, 5, 6, 7, 8, 11, 12, 'ndvi', 'ndbi', 'ndwi', 'ndre']\n",
    "basic_indices = [1, 2, 3, 7]\n",
    "rest_indices = set(indices) - set(basic_indices)\n",
    "rest_indices = list(rest_indices)\n",
    "\n",
    "rest_indices = [x for x in rest_indices if isinstance(x, int)] + [x for x in rest_indices if isinstance(x, str)]\n",
    "#for combination in combinations()\n",
    "for k in range(len(rest_indices)):\n",
    "    k+=1\n",
    "    for combination_of_indices in combinations(rest_indices, k):\n",
    "        indices_to_test = basic_indices + list(combination_of_indices)\n",
    "        config_dict['multispecter_bands_indices'] = indices_to_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c05f400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6, 8, 11, 12, 'ndre', 'ndvi', 'ndwi', 'ndbi']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PABBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, pab_channels: int = 64):\n",
    "        super().__init__()\n",
    "\n",
    "        # Series of 1x1 conv to generate attention feature maps\n",
    "        self.pab_channels = pab_channels\n",
    "        self.in_channels = in_channels\n",
    "        self.top_conv = nn.Conv2d(in_channels, pab_channels, kernel_size=1)\n",
    "        self.center_conv = nn.Conv2d(in_channels, pab_channels, kernel_size=1)\n",
    "        self.bottom_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "        self.map_softmax = nn.Softmax(dim=1)\n",
    "        self.out_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, _, height, width = x.shape\n",
    "\n",
    "        x_top = self.top_conv(x)\n",
    "        x_center = self.center_conv(x)\n",
    "        x_bottom = self.bottom_conv(x)\n",
    "\n",
    "        x_top = x_top.flatten(2)\n",
    "        x_center = x_center.flatten(2).transpose(1, 2)\n",
    "        x_bottom = x_bottom.flatten(2).transpose(1, 2)\n",
    "\n",
    "        sp_map = torch.matmul(x_center, x_top)\n",
    "        sp_map = self.map_softmax(sp_map.view(batch_size, -1))\n",
    "        sp_map = sp_map.view(batch_size, height * width, height * width)\n",
    "\n",
    "        sp_map = torch.matmul(sp_map, x_bottom)\n",
    "        sp_map = sp_map.reshape(batch_size, self.in_channels, height, width)\n",
    "\n",
    "        x = x + sp_map\n",
    "        x = self.out_conv(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "block = PABBlock(in_channels=64, pab_channels=64)\n",
    "\n",
    "\n",
    "pab_channels = 256\n",
    "in_channels = 128\n",
    "top_conv = nn.Conv2d(in_channels, pab_channels, kernel_size=1)\n",
    "center_conv = nn.Conv2d(in_channels, pab_channels, kernel_size=1)\n",
    "bottom_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "map_softmax = nn.Softmax(dim=1)\n",
    "out_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "\n",
    "x = torch.randn(1, in_channels, 12, 12)\n",
    "batch_size, _, height, width = x.shape\n",
    "\n",
    "x_top = top_conv(x)\n",
    "x_center = center_conv(x)\n",
    "x_bottom = bottom_conv(x)\n",
    "print('After conv:')\n",
    "print(f'x_top={x_top.shape}, x_center={x_center.shape}, x_bottom={x_bottom.shape},')\n",
    "\n",
    "x_top = x_top.flatten(2)\n",
    "x_center = x_center.flatten(2).transpose(1, 2)\n",
    "x_bottom = x_bottom.flatten(2).transpose(1, 2)\n",
    "print('After reshape and transpose:')\n",
    "print(f'x_top_r={x_top.shape}, x_center_rt={x_center.shape}, x_bottom_rt={x_bottom.shape},')\n",
    "\n",
    "sp_map = torch.matmul(x_center, x_top)\n",
    "print(f'sp_map={sp_map.shape} (x_center_rt × x_top_r)')\n",
    "sp_map = map_softmax(sp_map.view(batch_size, -1))\n",
    "print(f'sp_map after softmax={sp_map.shape}')\n",
    "sp_map = sp_map.view(batch_size, height * width, height * width)\n",
    "print(f'sp_map after reshape={sp_map.shape}')\n",
    "sp_map = torch.matmul(sp_map, x_bottom)\n",
    "print(f'sp_map × x_bottom = {sp_map.shape}')\n",
    "sp_map = sp_map.reshape(batch_size, in_channels, height, width)\n",
    "print(f'sp_map after reshape = {sp_map.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330f7a52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
