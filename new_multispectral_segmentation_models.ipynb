{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0cede1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import json\n",
    "import yaml\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from itertools import combinations\n",
    "import warnings\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import tv_tensors\n",
    "from torchvision import models\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.base import (\n",
    "    ClassificationHead,\n",
    "    SegmentationHead,\n",
    "    SegmentationModel,\n",
    ")\n",
    "\n",
    "from segmentation_models_pytorch.base import modules as md\n",
    "\n",
    "from segmentation_models_pytorch.encoders import get_encoder\n",
    "from segmentation_models_pytorch.base.hub_mixin import supports_config_loading\n",
    "from typing import Any, Dict, Optional, Union, Callable, Sequence, List, Literal\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger, Logger\n",
    "\n",
    "from torchmetrics import classification\n",
    "from torchmetrics import segmentation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e32608",
   "metadata": {},
   "source": [
    "# Работа с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "678fd044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "class SegmentationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path_to_dataset_root:str, samples_df:pd.DataFrame, channel_indices:list, transforms:v2._transform.Transform, dtype:torch.dtype, device:torch.device):\n",
    "        '''\n",
    "        In:\n",
    "            path_to_dataset_root - путь до корневой папки с датасетом\n",
    "            samples_df - pandas.DataFrame с информацией о файлах\n",
    "            channel_indices - список с номерами каналов мультиспектрального изображения\n",
    "            transforms - аугментация изображений\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.path_to_dataset_root = path_to_dataset_root\n",
    "        self.samples_df = samples_df\n",
    "        self.specter_bands_list = [i for i in channel_indices if isinstance(i, int)]\n",
    "        self.specter_indices_names = [s for s in channel_indices if isinstance(s, str)]\n",
    "        self.dtype_trasform = v2.ToDtype(dtype=dtype, scale=True)\n",
    "        self.other_transforms = transforms\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples_df)\n",
    "    @staticmethod\n",
    "    def compute_spectral_index(index_name, image):\n",
    "        if index_name.lower() == 'ndvi':\n",
    "            b0 = image[7] # NIR, B8\n",
    "            b1 = image[3] # RED, B4\n",
    "            \n",
    "        elif index_name.lower() == 'ndbi':\n",
    "            b0 = image[10] #SWIR, B11\n",
    "            b1 = image[7] #NIR, B8\n",
    "\n",
    "        elif index_name.lower() == 'ndwi':\n",
    "            b0 = image[2] #green, B3\n",
    "            b1 = image[7] #NIR, B8\n",
    "\n",
    "        elif index_name.lower() == 'ndre':\n",
    "            b0 = image[7] #NIR, B8\n",
    "            b1 = image[5] #Red Edge, B6\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            index = (b0 - b1)/(b0 + b1)\n",
    "            \n",
    "        index = np.nan_to_num(index, nan=-5)\n",
    "\n",
    "        return index\n",
    "            \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples_df.iloc[idx]\n",
    "\n",
    "        file_name = sample['file_name']\n",
    "\n",
    "        path_to_image = os.path.join(self.path_to_dataset_root, 'images', f'{file_name}.npy')\n",
    "        path_to_labels = os.path.join(self.path_to_dataset_root, 'labels', f'{file_name}.npy')\n",
    "\n",
    "        image = np.load(path_to_image)\n",
    "        spectral_indices = []\n",
    "        # вычисляем спектральные индексы\n",
    "        if len(self.specter_indices_names) > 0:\n",
    "            for sp_index_name in self.specter_indices_names:\n",
    "                spectral_index = self.compute_spectral_index(sp_index_name, image)\n",
    "                spectral_index = torch.as_tensor(spectral_index)\n",
    "                spectral_indices.append(spectral_index.unsqueeze(0))\n",
    "\n",
    "            spectral_indices = torch.cat(spectral_indices)\n",
    "            spectral_indices = self.dtype_trasform(spectral_indices)\n",
    "\n",
    "\n",
    "        image = torch.as_tensor(image[self.specter_bands_list], dtype=torch.int16)\n",
    "        image = self.dtype_trasform(image)\n",
    "        # добавляем спектральные индексы\n",
    "        if len(self.specter_indices_names) > 0:\n",
    "            image = torch.cat([image, spectral_indices], dim=0) \n",
    "        #image = np.load(path_to_image)\n",
    "        # метки читаем как одноканальное изображение\n",
    "        label = np.load(path_to_labels)\n",
    "        label = np.where(label >= 0, label, 0)\n",
    "        #label = torch.as_tensor(np.load(path_to_labels), dtype=torch.uint8).long()\n",
    "        label = torch.as_tensor(label, dtype=torch.uint8).long()\n",
    "        \n",
    "        image = tv_tensors.Image(image, device=self.device)\n",
    "        label = tv_tensors.Mask(label, device=self.device)\n",
    "\n",
    "        transforms_dict = {'image':image, 'mask':label}\n",
    "        transformed = self.other_transforms(transforms_dict)\n",
    "        return transformed['image'], transformed['mask']#, image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fac0331",
   "metadata": {},
   "source": [
    "# Описание модуля Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7082272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pred_mask(pred):\n",
    "    '''\n",
    "    Определение маски классов на основе сгенерированной softmax маски\n",
    "    '''\n",
    "    #pred = pred.detach()\n",
    "    _, pred_mask = pred.max(dim=1)\n",
    "    return pred_mask#.cpu().numpy()\n",
    "\n",
    "class SegmentationModule(L.LightningModule):\n",
    "    def __init__(self, model:nn.Module, criterion:nn.Module, optimizer_cfg:dict, metrics_dict:dict, name2class_idx_dict:dict) -> None:\n",
    "        '''\n",
    "        Модуль Lightning для обучения сегментационной сети\n",
    "        In:\n",
    "            model - нейронная сеть\n",
    "            criterion - функция потерь\n",
    "            \n",
    "            name2class_idx_dict - словарь с отображением {class_name(str): class_idx(int)}\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer_cfg = optimizer_cfg\n",
    "        self.metrics_dict = metrics_dict\n",
    "        \n",
    "        self.name2class_idx_dict = name2class_idx_dict\n",
    "        # словарь, выполняющий обратное отображение class_idx в class_name\n",
    "        self.class_idx2name_dict = {v:k for k, v in name2class_idx_dict.items()}\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self.optimizer_cfg['optmizer'](self.parameters(), **self.optimizer_cfg['optimizer_args'])\n",
    "        ret_dict = {'optimizer': optimizer}\n",
    "        if self.optimizer_cfg['lr_scheduler'] is not None:\n",
    "            scheduler = self.optimizer_cfg['lr_scheduler'](optimizer, **self.optimizer_cfg['lr_scheduler_args'])\n",
    "            ret_dict['lr_scheduler'] = {'scheduler': scheduler}\n",
    "            ret_dict['lr_scheduler'].update(self.optimizer_cfg['lr_scheduler_params'])\n",
    "        \n",
    "        return ret_dict\n",
    "\n",
    "    def compute_metrics(self, pred_labels, true_labels, mode):\n",
    "        metrics_names_list = self.metrics_dict[mode].keys()\n",
    "        for metric_name in metrics_names_list:\n",
    "            if 'dice' in metric_name.lower():\n",
    "                self.metrics_dict[mode][metric_name].update(pred_labels, true_labels)\n",
    "            else:\n",
    "                self.metrics_dict[mode][metric_name].update(pred_labels.reshape(-1), true_labels.reshape(-1))\n",
    "        \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, true_labels = batch\n",
    "        pred = self.model(data)\n",
    "        loss = self.criterion(pred, true_labels)\n",
    "        # вычисление сгенерированной маски\n",
    "        pred_labels = compute_pred_mask(pred)\n",
    "        #true_labels = true_labels.detach().cpu().numpy()\n",
    "        \n",
    "        self.compute_metrics(pred_labels=pred_labels, true_labels=true_labels, mode='train')\n",
    "\n",
    "        # т.к. мы вычисляем общую ошибку на всей эпохе, то записываем в лог только значение функции потерь\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data, true_labels = batch\n",
    "        pred = self.model(data)\n",
    "        loss = self.criterion(pred, true_labels)\n",
    "        pred_labels = compute_pred_mask(pred)\n",
    "        self.compute_metrics(pred_labels=pred_labels, true_labels=true_labels, mode='val')\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def log_metrics(self, mode):\n",
    "        for metric_name, metric in self.metrics_dict[mode].items():\n",
    "            metric_val = metric.compute()\n",
    "            if 'confusion' in metric_name.lower():\n",
    "                disp_name = f'{mode}_{metric_name}'\n",
    "                self.log(disp_name, metric_val.cpu().tolist(), on_step=False, on_epoch=True, prog_bar=False)\n",
    "            else:\n",
    "                for i, value in enumerate(metric_val):\n",
    "                    class_name = self.class_idx2name_dict[i]\n",
    "                    disp_name = f'{mode}_{metric_name}_{class_name}'\n",
    "                    self.log(disp_name, value, on_step=False, on_epoch=True, prog_bar=True)\n",
    "                disp_name = f'{mode}_{metric_name}_mean'\n",
    "                self.log(disp_name, metric_val.mean(), on_step=False, on_epoch=True, prog_bar=True)\n",
    "            self.metrics_dict[mode][metric_name].reset()\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        '''\n",
    "        Декодирование результатов тренировочной эпохи и запись их в лог\n",
    "        '''\n",
    "        self.log_metrics(mode='train')\n",
    " \n",
    "    def on_validation_epoch_end(self):\n",
    "        '''\n",
    "        Декодирование результатов тестовой эпохи и запись их в лог\n",
    "        (работает точно также, как и )\n",
    "        '''\n",
    "        self.log_metrics(mode='val')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b4de78",
   "metadata": {},
   "source": [
    "# Новые функции потерь (Dice-Crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5889a22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceCELoss(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            ce_weight,\n",
    "            ce_ignore_index,\n",
    "            ce_reducion,\n",
    "            ce_label_smoothing,\n",
    "            dice_mode,\n",
    "            dice_classes,\n",
    "            dice_log_loss,\n",
    "            dice_from_logits,\n",
    "            dice_smooth,\n",
    "            dice_ignore_index,\n",
    "            dice_eps,\n",
    "            losses_weight: List = [0.5, 0.5],\n",
    "            is_trainable_weights: bool = False,\n",
    "            weights_processing_type: str = None,\n",
    "            ):\n",
    "        super().__init__()\n",
    "        self.dice = smp.losses.DiceLoss(\n",
    "            mode=dice_mode,\n",
    "            classes=dice_classes,\n",
    "            log_loss=dice_log_loss,\n",
    "            from_logits=dice_from_logits,\n",
    "            smooth=dice_smooth,\n",
    "            ignore_index=dice_ignore_index,\n",
    "            eps=dice_eps\n",
    "            )\n",
    "        self.ce = nn.CrossEntropyLoss(\n",
    "            weight=ce_weight,\n",
    "            ignore_index=ce_ignore_index,\n",
    "            reduction=ce_reducion,\n",
    "            label_smoothing=ce_label_smoothing,\n",
    "        )\n",
    "        self.loss_weights = torch.tensor(losses_weight)\n",
    "        if is_trainable_weights:\n",
    "            self.loss_weights = nn.Parameter(self.loss_weights)\n",
    "        self.weights_processing_type = weights_processing_type\n",
    "\n",
    "    def forward(self, pred, true):\n",
    "        weights = self.loss_weights\n",
    "        if self.weights_processing_type == 'softmax':\n",
    "            weights = weights.softmax(dim=0)\n",
    "        elif self.weights_processing_type == 'sigmoid':\n",
    "            weights = weights.softmax(dim=0)\n",
    "\n",
    "        ce_loss = self.ce(pred, true) * weights[0]\n",
    "        dice_loss = self.dice(pred, true) * weights[1]\n",
    "        return ce_loss + dice_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200468db",
   "metadata": {},
   "source": [
    "# Адаптация FPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "617f9a98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 96, 96])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FPNMod(SegmentationModel):\n",
    "    \"\"\"FPN_ is a fully convolution neural network for image semantic segmentation.\n",
    "\n",
    "    Args:\n",
    "        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n",
    "            to extract features of different spatial resolution\n",
    "        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features\n",
    "            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n",
    "            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n",
    "            Default is 5\n",
    "        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and\n",
    "            other pretrained weights (see table with available weights for each encoder_name)\n",
    "        decoder_pyramid_channels: A number of convolution filters in Feature Pyramid of FPN_\n",
    "        decoder_segmentation_channels: A number of convolution filters in segmentation blocks of FPN_\n",
    "        decoder_merge_policy: Determines how to merge pyramid features inside FPN. Available options are **add**\n",
    "            and **cat**\n",
    "        decoder_dropout: Spatial dropout rate in range (0, 1) for feature pyramid in FPN_\n",
    "        decoder_interpolation: Interpolation mode used in decoder of the model. Available options are\n",
    "            **\"nearest\"**, **\"bilinear\"**, **\"bicubic\"**, **\"area\"**, **\"nearest-exact\"**. Default is **\"nearest\"**.\n",
    "        in_channels: A number of input channels for the model, default is 3 (RGB images)\n",
    "        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n",
    "        activation: An activation function to apply after the final convolution layer.\n",
    "            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**,\n",
    "            **callable** and **None**. Default is **None**.\n",
    "        upsampling: Final upsampling factor. Default is 4 to preserve input-output spatial shape identity\n",
    "        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build\n",
    "            on top of encoder if **aux_params** is not **None** (default). Supported params:\n",
    "                - classes (int): A number of classes\n",
    "                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n",
    "                - dropout (float): Dropout factor in [0, 1)\n",
    "                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\"\n",
    "                    (could be **None** to return logits)\n",
    "        kwargs: Arguments passed to the encoder class ``__init__()`` function. Applies only to ``timm`` models. Keys with ``None`` values are pruned before passing.\n",
    "\n",
    "    Returns:\n",
    "        ``torch.nn.Module``: **FPN**\n",
    "\n",
    "    .. _FPN:\n",
    "        http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    @supports_config_loading\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_name: str = \"resnet34\",\n",
    "        encoder_depth: int = 5,\n",
    "        encoder_weights: Optional[str] = \"imagenet\",\n",
    "        encoder_type:str = 'conv',\n",
    "        decoder_pyramid_channels: int = 256,\n",
    "        decoder_segmentation_channels: int = 128,\n",
    "        decoder_merge_policy: str = \"add\",\n",
    "        decoder_dropout: float = 0.2,\n",
    "        decoder_interpolation: str = \"nearest\",\n",
    "        in_channels: int = 3,\n",
    "        classes: int = 1,\n",
    "        activation: Optional[str] = None,\n",
    "        upsampling: int = 4,\n",
    "        aux_params: Optional[dict] = None,\n",
    "        **kwargs: dict[str, Any],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # validate input params\n",
    "        if encoder_name.startswith(\"mit_b\") and encoder_depth != 5:\n",
    "            raise ValueError(\n",
    "                \"Encoder {} support only encoder_depth=5\".format(encoder_name)\n",
    "            )\n",
    "\n",
    "        self.encoder = get_encoder(\n",
    "            encoder_name,\n",
    "            in_channels=in_channels,\n",
    "            depth=encoder_depth,\n",
    "            weights=encoder_weights,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.decoder = FPNDecoderMod(\n",
    "            encoder_channels=self.encoder.out_channels,\n",
    "            encoder_depth=encoder_depth,\n",
    "            pyramid_channels=decoder_pyramid_channels,\n",
    "            segmentation_channels=decoder_segmentation_channels,\n",
    "            dropout=decoder_dropout,\n",
    "            merge_policy=decoder_merge_policy,\n",
    "            interpolation_mode=decoder_interpolation,\n",
    "            encoder_type=encoder_type,\n",
    "        )\n",
    "\n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=self.decoder.out_channels,\n",
    "            out_channels=classes,\n",
    "            activation=activation,\n",
    "            kernel_size=1,\n",
    "            upsampling=upsampling,\n",
    "        )\n",
    "\n",
    "        if aux_params is not None:\n",
    "            self.classification_head = ClassificationHead(\n",
    "                in_channels=self.encoder.out_channels[-1], **aux_params\n",
    "            )\n",
    "        else:\n",
    "            self.classification_head = None\n",
    "\n",
    "        self.name = \"fpn-{}\".format(encoder_name)\n",
    "        self.initialize()\n",
    "\n",
    "class FPNModBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pyramid_channels: int,\n",
    "        skip_channels: int,\n",
    "        interpolation_mode: str = \"nearest\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.skip_conv = nn.Conv2d(skip_channels, pyramid_channels, kernel_size=1)\n",
    "        self.interpolation_mode = interpolation_mode\n",
    "\n",
    "    def forward(self, x: torch.Tensor, skip: torch.Tensor, scale_factor: float) -> torch.Tensor:\n",
    "        x = F.interpolate(x, scale_factor=scale_factor, mode=self.interpolation_mode)\n",
    "        if skip.size(1) != 0:\n",
    "            #print(x.shape, skip.shape)\n",
    "            skip = self.skip_conv(skip)\n",
    "            x = x + skip\n",
    "        return x\n",
    "\n",
    "class FPNDecoderMod(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_channels: List[int],\n",
    "        encoder_depth: int = 5,\n",
    "        pyramid_channels: int = 256,\n",
    "        segmentation_channels: int = 128,\n",
    "        dropout: float = 0.2,\n",
    "        merge_policy: Literal[\"add\", \"cat\"] = \"add\",\n",
    "        interpolation_mode: str = \"nearest\",\n",
    "        encoder_type:str = 'conv',\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.out_channels = (\n",
    "            segmentation_channels\n",
    "            if merge_policy == \"add\"\n",
    "            else segmentation_channels * 4\n",
    "        )\n",
    "        #print(self.out_channels)\n",
    "        if encoder_depth < 3:\n",
    "            raise ValueError(\n",
    "                \"Encoder depth for FPN decoder cannot be less than 3, got {}.\".format(\n",
    "                    encoder_depth\n",
    "                )\n",
    "            )\n",
    "\n",
    "        encoder_channels = encoder_channels[::-1]\n",
    "        encoder_channels = encoder_channels[: encoder_depth + 1]\n",
    "        \n",
    "        self.p6 = nn.Conv2d(encoder_channels[0], pyramid_channels, kernel_size=1)\n",
    "        '''\n",
    "        self.p5 = smp.decoders.fpn.decoder.FPNBlock(pyramid_channels, encoder_channels[1], interpolation_mode)\n",
    "        self.p4 = smp.decoders.fpn.decoder.FPNBlock(pyramid_channels, encoder_channels[2], interpolation_mode)\n",
    "        self.p3 = smp.decoders.fpn.decoder.FPNBlock(pyramid_channels, encoder_channels[3], interpolation_mode)\n",
    "        self.p2 = smp.decoders.fpn.decoder.FPNBlock(pyramid_channels, encoder_channels[4], interpolation_mode)\n",
    "        '''\n",
    "        self.p5 = FPNModBlock(pyramid_channels, encoder_channels[1], interpolation_mode)\n",
    "        self.p4 = FPNModBlock(pyramid_channels, encoder_channels[2], interpolation_mode)\n",
    "        self.p3 = FPNModBlock(pyramid_channels, encoder_channels[3], interpolation_mode)\n",
    "        self.p2 = FPNModBlock(pyramid_channels, encoder_channels[4], interpolation_mode)\n",
    "        \n",
    "        if encoder_type == 'conv':\n",
    "            upsamples_list = [4, 3, 2, 1, 0]\n",
    "        elif encoder_type == 'vit':\n",
    "            upsamples_list = [3, 2, 1, 0, 0]\n",
    "\n",
    "\n",
    "        self.seg_blocks = nn.ModuleList(\n",
    "            [\n",
    "                smp.decoders.fpn.decoder.SegmentationBlock(\n",
    "                    pyramid_channels, segmentation_channels, n_upsamples=n_upsamples\n",
    "                )\n",
    "                for n_upsamples in upsamples_list\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.merge = smp.decoders.fpn.decoder.MergeBlock(merge_policy)\n",
    "        self.dropout = nn.Dropout2d(p=dropout, inplace=True)\n",
    "\n",
    "    def forward(self, features: List[torch.Tensor]) -> torch.Tensor:\n",
    "        c2, c3, c4, c5, c6 = features[-5:]\n",
    "\n",
    "        #print([c2.shape, c3.shape, c4.shape, c5.shape, c6.shape])\n",
    "        #print([f.shape for f in features])\n",
    "        #print()\n",
    "        #print(f'c6:{c6.shape}')\n",
    "        p6 = self.p6(c6)\n",
    "        #print(f'p6:{p6.shape};c5:{c5.shape}')\n",
    "        p5 = self.p5(p6, c5, scale_factor=2.0)\n",
    "        #print(f'p5:{p5.shape};c4:{c4.shape}')\n",
    "        p4 = self.p4(p5, c4, scale_factor=2.0)\n",
    "        #print(f'p4:{p4.shape};c3:{c3.shape}')\n",
    "        p3 = self.p3(p4, c3, scale_factor=2.0)\n",
    "        #print(f'p3:{p3.shape};c2:{c2.shape}')\n",
    "        p2 = self.p2(p3, c2, scale_factor=2.0)\n",
    "        #print(f'p2:{p4.shape}')\n",
    "\n",
    "        s6 = self.seg_blocks[0](p6)\n",
    "        s5 = self.seg_blocks[1](p5)\n",
    "        s4 = self.seg_blocks[2](p4)\n",
    "        s3 = self.seg_blocks[3](p3)\n",
    "        s2 = self.seg_blocks[4](p2)\n",
    "\n",
    "        feature_pyramid = [s6, s5, s4, s3, s2]\n",
    "\n",
    "        #print([f.shape for f in feature_pyramid])\n",
    "\n",
    "        x = self.merge(feature_pyramid)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "#model = FPNMod(encoder_name='mit_b0', upsampling=0, encoder_type='vit', image_size=(96, 96))\n",
    "#model.encoder.patch_embed1.proj.stride=1\n",
    "\n",
    "model = FPNMod(encoder_name='resnet34', upsampling=0)\n",
    "model.encoder.conv1.stride=(1,1)\n",
    "\n",
    "ret = model(torch.randn(1, 3, 96, 96))\n",
    "ret.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9248728c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2dStaticSamePadding(\n",
       "  3, 32, kernel_size=(3, 3), stride=(2, 2), bias=False\n",
       "  (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       ")"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.encoder._conv_stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "0a41405e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 96, 96])"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = smp.FPN(encoder_name='mit_b0', upsampling=0, image_size=(96, 96))\n",
    "model.encoder.patch_embed1.proj.stride=1\n",
    "ret = model(torch.randn(1, 3, 96, 96))\n",
    "ret.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "1eddf948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(torch.zeros(size=(32, 0, 14, 88)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfd9964",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "079b9d59",
   "metadata": {},
   "source": [
    "# Адаптация FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "21b14254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 224, 224])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FCNDecoderBlock(nn.Module):\n",
    "    \"\"\"A decoder block in the FCN architecture that performs upsampling and feature fusion.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        use_norm: Union[bool, str, Dict[str, Any]] = \"batchnorm\",\n",
    "        attention_type: Optional[str] = None,\n",
    "        interpolation_mode: str = \"nearest\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.interpolation_mode = interpolation_mode\n",
    "        self.conv1 = md.Conv2dReLU(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_norm=use_norm,\n",
    "        )\n",
    "        self.attention1 = md.Attention(\n",
    "            attention_type, in_channels=in_channels\n",
    "        )\n",
    "        self.conv2 = md.Conv2dReLU(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_norm=use_norm,\n",
    "        )\n",
    "        self.attention2 = md.Attention(attention_type, in_channels=out_channels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        feature_map: torch.Tensor,\n",
    "        target_height: int,\n",
    "        target_width: int,\n",
    "        skip_connection: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        # сначала интерполяция и свертка\n",
    "        feature_map = F.interpolate(\n",
    "            feature_map,\n",
    "            size=(target_height, target_width),\n",
    "            mode=self.interpolation_mode,\n",
    "        )\n",
    "        feature_map = self.conv1(feature_map)\n",
    "        feature_map = self.attention1(feature_map)\n",
    "        \n",
    "        # потом сложение и выходная свертка\n",
    "        if skip_connection is not None:\n",
    "            feature_map = feature_map + skip_connection\n",
    "        feature_map = self.conv2(feature_map)\n",
    "        feature_map = self.attention2(feature_map)\n",
    "        \n",
    "        return feature_map\n",
    "    \n",
    "class FCNDecoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            encoder_channels: Sequence[int],\n",
    "            decoder_last_channel: Sequence[int],\n",
    "            n_blocks: int = 5,\n",
    "            use_norm: Union[bool, str, Dict[str, Any]] = \"batchnorm\",\n",
    "            attention_type: Optional[str] = None,\n",
    "            add_center_block: bool = False,\n",
    "            interpolation_mode: str = \"nearest\",\n",
    "        ):\n",
    "            super().__init__()\n",
    "            # remove first skip with same spatial resolution\n",
    "            encoder_channels = encoder_channels[1:]\n",
    "            # reverse channels to start from head of encoder\n",
    "            encoder_channels = encoder_channels[::-1]\n",
    "\n",
    "            # computing blocks input and output channels\n",
    "            head_channels = encoder_channels[0]\n",
    "            in_channels = encoder_channels\n",
    "            out_channels = encoder_channels[1:] + [decoder_last_channel]\n",
    "            \n",
    "            if add_center_block:\n",
    "                self.center = smp.decoders.unet.decoder.UnetCenterBlock(\n",
    "                    head_channels,\n",
    "                    head_channels//2,\n",
    "                    use_norm=use_norm,\n",
    "                )\n",
    "            else:\n",
    "                self.center = nn.Identity()\n",
    "\n",
    "            # combine decoder keyword arguments\n",
    "            self.blocks = nn.ModuleList()\n",
    "            for block_in_channels, block_out_channels in zip(\n",
    "                in_channels, out_channels\n",
    "            ):\n",
    "                block = FCNDecoderBlock(\n",
    "                    block_in_channels,\n",
    "                    block_out_channels,\n",
    "                    use_norm=use_norm,\n",
    "                    attention_type=attention_type,\n",
    "                    interpolation_mode=interpolation_mode,\n",
    "                )\n",
    "                self.blocks.append(block)\n",
    "\n",
    "    def forward(self, features: List[torch.Tensor]) -> torch.Tensor:\n",
    "        # spatial shapes of features: [hw, hw/2, hw/4, hw/8, ...]\n",
    "        spatial_shapes = [feature.shape[2:] for feature in features]\n",
    "        spatial_shapes = spatial_shapes[::-1]\n",
    "\n",
    "        features = features[1:]  # remove first skip with same spatial resolution\n",
    "        features = features[::-1]  # reverse channels to start from head of encoder\n",
    "        \n",
    "\n",
    "        head = features[0]\n",
    "        skip_connections = features[1:]\n",
    "        \n",
    "\n",
    "        x = self.center(head)\n",
    "\n",
    "        for i, decoder_block in enumerate(self.blocks):\n",
    "            # upsample to the next spatial shape\n",
    "            height, width = spatial_shapes[i + 1]\n",
    "            \n",
    "            skip_connection = skip_connections[i] if i < len(skip_connections) else None\n",
    "            \n",
    "            x = decoder_block(x, height, width, skip_connection=skip_connection)\n",
    "\n",
    "        return x\n",
    "\n",
    "class FCN(SegmentationModel):\n",
    "    \"\"\"\n",
    "    FCN is a fully convolutional neural network architecture designed for semantic image segmentation.\n",
    "\n",
    "    It consists of two main parts:\n",
    "\n",
    "    1. An encoder (downsampling path) that extracts increasingly abstract features\n",
    "    2. A decoder (upsampling path) that gradually recovers spatial details\n",
    "\n",
    "    The key is the use of skip connections between corresponding encoder and decoder layers.\n",
    "    These connections allow the decoder to access fine-grained details from earlier encoder layers,\n",
    "    which helps produce more precise segmentation masks.\n",
    "\n",
    "    The skip connections work by concatenating feature maps from the encoder directly into the decoder\n",
    "    at corresponding resolutions. This helps preserve important spatial information that would\n",
    "    otherwise be lost during the encoding process.\n",
    "\n",
    "    Args:\n",
    "        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n",
    "            to extract features of different spatial resolution\n",
    "        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features\n",
    "            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n",
    "            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n",
    "            Default is 5\n",
    "        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and\n",
    "            other pretrained weights (see table with available weights for each encoder_name)\n",
    "        decoder_channels: List of integers which specify **in_channels** parameter for convolutions used in decoder.\n",
    "            Length of the list should be the same as **encoder_depth**\n",
    "        decoder_use_norm:     Specifies normalization between Conv2D and activation.\n",
    "            Accepts the following types:\n",
    "            - **True**: Defaults to `\"batchnorm\"`.\n",
    "            - **False**: No normalization (`nn.Identity`).\n",
    "            - **str**: Specifies normalization type using default parameters. Available values:\n",
    "              `\"batchnorm\"`, `\"identity\"`, `\"layernorm\"`, `\"instancenorm\"`, `\"inplace\"`.\n",
    "            - **dict**: Fully customizable normalization settings. Structure:\n",
    "              ```python\n",
    "              {\"type\": <norm_type>, **kwargs}\n",
    "              ```\n",
    "              where `norm_name` corresponds to normalization type (see above), and `kwargs` are passed directly to the normalization layer as defined in PyTorch documentation.\n",
    "\n",
    "            **Example**:\n",
    "            ```python\n",
    "            decoder_use_norm={\"type\": \"layernorm\", \"eps\": 1e-2}\n",
    "            ```\n",
    "        decoder_attention_type: Attention module used in decoder of the model. Available options are\n",
    "            **None** and **scse** (https://arxiv.org/abs/1808.08127).\n",
    "        decoder_interpolation: Interpolation mode used in decoder of the model. Available options are\n",
    "            **\"nearest\"**, **\"bilinear\"**, **\"bicubic\"**, **\"area\"**, **\"nearest-exact\"**. Default is **\"nearest\"**.\n",
    "        in_channels: A number of input channels for the model, default is 3 (RGB images)\n",
    "        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n",
    "        activation: An activation function to apply after the final convolution layer.\n",
    "            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**,\n",
    "            **callable** and **None**. Default is **None**.\n",
    "        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build\n",
    "            on top of encoder if **aux_params** is not **None** (default). Supported params:\n",
    "                - classes (int): A number of classes\n",
    "                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n",
    "                - dropout (float): Dropout factor in [0, 1)\n",
    "                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\"\n",
    "                    (could be **None** to return logits)\n",
    "        kwargs: Arguments passed to the encoder class ``__init__()`` function. Applies only to ``timm`` models. Keys with ``None`` values are pruned before passing.\n",
    "\n",
    "    Returns:\n",
    "        ``torch.nn.Module``: Unet\n",
    "\n",
    "    Example:\n",
    "        .. code-block:: python\n",
    "\n",
    "            import torch\n",
    "            import segmentation_models_pytorch as smp\n",
    "\n",
    "            model = smp.Unet(\"resnet18\", encoder_weights=\"imagenet\", classes=5)\n",
    "            model.eval()\n",
    "\n",
    "            # generate random images\n",
    "            images = torch.rand(2, 3, 256, 256)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                mask = model(images)\n",
    "\n",
    "            print(mask.shape)\n",
    "            # torch.Size([2, 5, 256, 256])\n",
    "\n",
    "    .. _Unet:\n",
    "        https://arxiv.org/abs/1505.04597\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    requires_divisible_input_shape = False\n",
    "\n",
    "    @supports_config_loading\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_name: str = \"resnet34\",\n",
    "        encoder_depth: int = 5,\n",
    "        encoder_weights: Optional[str] = \"imagenet\",\n",
    "        decoder_use_norm: Union[bool, str, Dict[str, Any]] = \"batchnorm\",\n",
    "        decoder_last_channel: int = 16,\n",
    "        decoder_attention_type: Optional[str] = None,\n",
    "        decoder_interpolation: str = \"nearest\",\n",
    "        in_channels: int = 3,\n",
    "        classes: int = 1,\n",
    "        activation: Optional[Union[str, Callable]] = None,\n",
    "        aux_params: Optional[dict] = None,\n",
    "        **kwargs: dict[str, Any],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        decoder_use_batchnorm = kwargs.pop(\"decoder_use_batchnorm\", None)\n",
    "        if decoder_use_batchnorm is not None:\n",
    "            warnings.warn(\n",
    "                \"The usage of decoder_use_batchnorm is deprecated. Please modify your code for decoder_use_norm\",\n",
    "                DeprecationWarning,\n",
    "                stacklevel=2,\n",
    "            )\n",
    "            decoder_use_norm = decoder_use_batchnorm\n",
    "\n",
    "        self.encoder = get_encoder(\n",
    "            encoder_name,\n",
    "            in_channels=in_channels,\n",
    "            depth=encoder_depth,\n",
    "            weights=encoder_weights,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        add_center_block = encoder_name.startswith(\"vgg\")\n",
    "\n",
    "        self.decoder = FCNDecoder(\n",
    "            encoder_channels=self.encoder.out_channels,\n",
    "            decoder_last_channel=decoder_last_channel,\n",
    "            n_blocks=encoder_depth,\n",
    "            use_norm=decoder_use_norm,\n",
    "            add_center_block=add_center_block,\n",
    "            attention_type=decoder_attention_type,\n",
    "            interpolation_mode=decoder_interpolation,\n",
    "        )\n",
    "\n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=decoder_last_channel,\n",
    "            out_channels=classes,\n",
    "            activation=activation,\n",
    "            kernel_size=3,\n",
    "        )\n",
    "\n",
    "        if aux_params is not None:\n",
    "            self.classification_head = ClassificationHead(\n",
    "                in_channels=decoder_last_channel, **aux_params\n",
    "            )\n",
    "        else:\n",
    "            self.classification_head = None\n",
    "\n",
    "        self.name = \"fcn-{}\".format(encoder_name)\n",
    "        self.initialize()\n",
    "\n",
    "model = FCN(decoder_last_channel=32)\n",
    "ret = model(torch.randn(1, 3, 224, 224))\n",
    "ret.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4035171",
   "metadata": {},
   "source": [
    "# Фабрики для создания моделей по конфигурациям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1960f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relace_input_layer(model:nn.Module, config:dict):\n",
    "    pass\n",
    "\n",
    "def create_weights_from_avg_ch(weight, new_in_channels):\n",
    "    return torch.cat([weight.mean(dim=1).unsqueeze(1)]*new_in_channels, dim=1)\n",
    "\n",
    "def cerate_weights_from_repeated_ch(weight, in_channels, new_in_channels):\n",
    "    ch_multiple = new_in_channels//in_channels\n",
    "    reminded_channels = new_in_channels%in_channels\n",
    "    # сначала набираем новые каналы путем подставления друг за другом (stack) каналов изначального изображения,\n",
    "    # а затем, если количество новых каналов не делится без остатка на количество изначальных, \n",
    "    # то набираем оставшиеся новые каналы из оставшихся изначальных    \n",
    "    new_weight = torch.cat(\n",
    "        [weight]*ch_multiple + [weight[:,:reminded_channels]], dim=1)\n",
    "    return new_weight\n",
    "\n",
    "\n",
    "def create_model(config_dict, segmentation_nns_factory_dict):\n",
    "    model_name = config_dict['segmentation_nn']['nn_architecture']\n",
    "    # создаем нейронную сеть из фабрики\n",
    "    model = segmentation_nns_factory_dict[model_name](**config_dict['segmentation_nn']['params'])\n",
    "    multispecter_bands_indices = config_dict['multispecter_bands_indices']\n",
    "    in_channels = len(multispecter_bands_indices)\n",
    "    # замена входного слоя, если кол-во каналов изображения не равно трем\n",
    "    input_conv = model.get_submodule(\n",
    "        config_dict['segmentation_nn']['input_layer_config']['layer_path']\n",
    "        )\n",
    "    if 'channels' in config_dict['segmentation_nn']['input_layer_config']['replace_type']:\n",
    "        if in_channels != 3:\n",
    "            # получаем входной слой, специфический для конкретной нейронной сети\n",
    "            \n",
    "            new_input_conv = nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=input_conv.out_channels,\n",
    "                kernel_size=input_conv.kernel_size,\n",
    "                #stride=conv1.stride,\n",
    "                stride=config_dict['segmentation_nn']['input_layer_config']['params']['stride'],\n",
    "                #padding=conv1.padding,\n",
    "                padding=config_dict['segmentation_nn']['input_layer_config']['params']['padding'],\n",
    "                dilation=input_conv.dilation,\n",
    "                groups=input_conv.groups,\n",
    "                bias=input_conv.bias is not None\n",
    "            )\n",
    "            if config_dict['segmentation_nn']['params']['encoder_weights'] is not None:\n",
    "                # выбор типа обнолвления весов\n",
    "                if config_dict['segmentation_nn']['input_layer_config']['weight_update_type'] == 'average_all':\n",
    "                    \n",
    "                    #new_weight = torch.cat([input_conv.weight.mean(dim=1).unsqueeze(1)]*in_channels, dim=1)\n",
    "                    new_weight = create_weights_from_avg_ch(input_conv.weght, in_channels)\n",
    "                    input_conv.weight = nn.Parameter(new_weight)\n",
    "\n",
    "                elif config_dict['segmentation_nn']['input_layer_config']['weight_update_type'] == 'repeate':\n",
    "                    '''\n",
    "                    ch_multiple = in_channels//input_conv.in_channels\n",
    "                    reminded_channels = in_channels%input_conv.in_channels\n",
    "                    new_weight = torch.cat(\n",
    "                        [input_conv.weight]*ch_multiple + [input_conv.weight[:,:reminded_channels]], dim=1)\n",
    "                    '''\n",
    "                    new_weight = cerate_weights_from_repeated_ch(input_conv.weight, input_conv.in_channels, in_channels)\n",
    "                    \n",
    "                if input_conv.bias is not None:\n",
    "                    new_input_conv.bias = input_conv.bias\n",
    "\n",
    "            # перезаписываем входной слой исходя из специфики оригинальной сети\n",
    "            model.set_submodule(\n",
    "                config_dict['segmentation_nn']['input_layer_config']['layer_path'],\n",
    "                new_input_conv\n",
    "                )\n",
    "    elif 'multisize_conv' in config_dict['segmentation_nn']['input_layer_config']['replace_type']:        \n",
    "        multisize_params = config_dict['segmentation_nn']['input_layer_config']['params']\n",
    "        new_input_conv = MultisizeConv(**multisize_params)\n",
    "\n",
    "        # Если мы модифицируем входной слой.\n",
    "        if config_dict['segmentation_nn']['params']['encoder_weights'] is not None:\n",
    "            # вычленяем словрь с параметрами размеров ядер сверток.\n",
    "            kernel_sizes_dict = config_dict['segmentation_nn']['input_layer_config']['params']['kernel_size']\n",
    "            interpolated_kernels_dict = {}\n",
    "            # выполняем \n",
    "            for name, kernel_size in kernel_sizes_dict.items():\n",
    "                if isinstance(kernel_size, int):\n",
    "                    kernel_size = (kernel_size, kernel_size)\n",
    "                # получаем интерполированную версию ядер свертки\n",
    "                interpolated_kernels_dict[name] = F.interpolate(input_conv.weight, size=kernel_size, mode='bicubic', antialias=True)\n",
    "            if config_dict['segmentation_nn']['input_layer_config']['weight_update_type'] == 'average_all':\n",
    "                out_channels_dict = config_dict['segmentation_nn']['input_layer_config']['params']['out_channels']\n",
    "                for name, out_channels in out_channels_dict.items():\n",
    "                    weights = interpolated_kernels_dict[name]\n",
    "                    out_channels_dict[name] = create_weights_from_avg_ch(weights, in_channels)\n",
    "\n",
    "            elif config_dict['segmentation_nn']['input_layer_config']['weight_update_type'] == 'repeate':\n",
    "                raise ValueError('Repate is not supported yet')\n",
    "        if config_dict['segmentation_nn']['input_layer_config']['params']['aggregation_type'] == 'cat':\n",
    "            # Если тип агрегации выхода MultisizeConv - это конкатенация, то изменяем также второй сверточный слой,\n",
    "            # чтобы число его входных каналов соответствовало числу выходных первого слоя \n",
    "            raise NotImplementedError\n",
    "        # заменяем сходной слой по заранее определенному пути, который может варьировать в зависимости от архитектуры энкодера\n",
    "        model.set_submodule(\n",
    "                config_dict['segmentation_nn']['input_layer_config']['layer_path'],\n",
    "                new_input_conv\n",
    "                )\n",
    "    return model\n",
    "\n",
    "class MultisizeConv(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels:int,\n",
    "            out_channels:dict,\n",
    "            kernel_size:dict,\n",
    "            stride:dict,\n",
    "            padding:dict,\n",
    "            dilation:dict,\n",
    "            groups:dict,\n",
    "            bias:dict,\n",
    "            aggregation_type:str,\n",
    "            ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.aggregation_type = aggregation_type\n",
    "        self.multisize_convs = nn.ModuleDict()\n",
    "        for conv_name in kernel_size.keys():\n",
    "            self.multisize_convs[conv_name] = nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels[conv_name],\n",
    "                kernel_size=kernel_size[conv_name],\n",
    "                stride=stride[conv_name],\n",
    "                padding=padding[conv_name],\n",
    "                dilation=dilation[conv_name],\n",
    "                groups=groups[conv_name],\n",
    "                bias=bias[conv_name]\n",
    "                )\n",
    "        \n",
    "    def update_weights(self, new_weights_dict):\n",
    "        '''\n",
    "        На вход принимается словрь со структурой {'имя_свертки': (weight, bias)}\n",
    "        '''\n",
    "        for conv_name, (weight, bias) in new_weights_dict.items():\n",
    "            self.multisize_convs[conv_name].weight = nn.Parameter(weight)\n",
    "            if self.multisize_convs[conv_name].bias is not None:\n",
    "                self.multisize_convs[conv_name].bias = nn.Parameter(bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for conv_name in self.multisize_convs.keys():\n",
    "            out = self.multisize_convs[conv_name](x)\n",
    "            #print(out.shape)\n",
    "            outputs.append(out)\n",
    "        \n",
    "        if self.aggregation_type == 'add':\n",
    "            outputs = torch.stack(outputs, dim=0)\n",
    "            outputs = outputs.sum(dim=0)\n",
    "        elif self.aggregation_type == 'cat':\n",
    "            outputs = torch.cat(outputs, dim=1)\n",
    "        else:\n",
    "            raise ValueError(f'self.aggregation_type should be either \"add\" or \"cat\". Got {self.aggregation_type}')\n",
    "        return outputs\n",
    "\n",
    "segmentation_nns_factory_dict = {\n",
    "    'unet': smp.Unet,\n",
    "    'fpn': smp.FPN,\n",
    "    'custom_fpn': FPNMod,\n",
    "    'unet++': smp.UnetPlusPlus,\n",
    "    'fcn': FCN,\n",
    "}\n",
    "\n",
    "criterion_factory_dict = {\n",
    "    'crossentropy': nn.CrossEntropyLoss,\n",
    "    'dice_crossentropy': DiceCELoss,\n",
    "    'dice': smp.losses.DiceLoss\n",
    "}\n",
    "\n",
    "optimizers_factory_dict = {\n",
    "    'adam': torch.optim.Adam,\n",
    "    'adamw': torch.optim.AdamW\n",
    "}\n",
    "\n",
    "lr_schedulers_factory_dict = {\n",
    "    'cosine_warm_restarts': torch.optim.lr_scheduler.CosineAnnealingWarmRestarts,\n",
    "    'plateau': torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    'cosine': torch.optim.lr_scheduler.CosineAnnealingLR,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "becc52d3",
   "metadata": {},
   "source": [
    "# Черновик"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be909333",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 7, 7])\n",
      "torch.Size([64, 3, 11, 11])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 3 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "model = smp.Unet(encoder_name='resnet34')\n",
    "#input_weight = model.encoder._conv_stem.weight.detach()#.numpy()\n",
    "input_weight = model.encoder.conv1.weight.detach()#.numpy()\n",
    "interpolated_weight = F.interpolate(input_weight, size=(11, 11), mode='bicubic', antialias=True, align_corners=False)\n",
    "\n",
    "print(input_weight.shape)\n",
    "print(interpolated_weight.shape)\n",
    "\n",
    "filter_index = 9\n",
    "conv_filter = input_weight[filter_index]\n",
    "interpolated_conv_filter = interpolated_weight[filter_index]\n",
    "fig1, axs1 = plt.subplots(1,3)\n",
    "for idx, img in enumerate(conv_filter):\n",
    "    axs1[idx].imshow(img)\n",
    "\n",
    "fig2, axs2 = plt.subplots(1,3)\n",
    "for idx, img in enumerate(interpolated_conv_filter):\n",
    "    axs2[idx].imshow(img)\n",
    "\n",
    "\n",
    "filter_index += 1\n",
    "conv_filter = input_weight[filter_index]\n",
    "interpolated_conv_filter = interpolated_weight[filter_index]\n",
    "fig3, axs3 = plt.subplots(1,3)\n",
    "for idx, img in enumerate(conv_filter):\n",
    "    axs3[idx].imshow(img)\n",
    "\n",
    "fig4, axs4 = plt.subplots(1,3)\n",
    "for idx, img in enumerate(interpolated_conv_filter):\n",
    "    axs4[idx].imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c8850dbd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 3, 3, 3])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_weight.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4680fb11",
   "metadata": {},
   "source": [
    "# Конфигурации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2291919a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'segmentation_nn': {'nn_architecture': 'custom_fpn',\n",
       "  'params': {'encoder_name': 'efficientnet-b0',\n",
       "   'encoder_depth': 5,\n",
       "   'encoder_weights': 'imagenet',\n",
       "   'decoder_pyramid_channels': 128,\n",
       "   'decoder_segmentation_channels': 128,\n",
       "   'decoder_merge_policy': 'add',\n",
       "   'decoder_dropout': 0.2,\n",
       "   'decoder_interpolation': 'nearest',\n",
       "   'in_channels': 3,\n",
       "   'classes': 11,\n",
       "   'activation': None,\n",
       "   'upsampling': 0,\n",
       "   'aux_params': None},\n",
       "  'input_layer_config': {'layer_path': 'encoder._conv_stem',\n",
       "   'replace_type': 'channels+stride',\n",
       "   'weight_update_type': 'repeate',\n",
       "   'params': {'in_channels': 3,\n",
       "    'out_channels': {'1x1': 32, '3x3': 32, '5x5': 32},\n",
       "    'kernel_size': {'1x1': 1, '3x3': 3, '5x5': 5},\n",
       "    'stride': {'1x1': 1, '3x3': 1, '5x5': 1},\n",
       "    'padding': {'1x1': 0, '3x3': 1, '5x5': 2},\n",
       "    'dilation': {'1x1': 1, '3x3': 1, '5x5': 1},\n",
       "    'groups': {'1x1': 1, '3x3': 1, '5x5': 1},\n",
       "    'bias': {'1x1': False, '3x3': False, '5x5': False},\n",
       "    'aggregation_type': 'add'}}},\n",
       " 'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
       " 'input_image_size': 96,\n",
       " 'loss': {'type': 'crossentropy',\n",
       "  'params': {'weight': None,\n",
       "   'ignore_index': -100,\n",
       "   'reduction': 'mean',\n",
       "   'label_smoothing': 0.15}},\n",
       " 'optimizer': {'type': 'adam', 'args': {}},\n",
       " 'lr_scheduler': {'type': 'cosine_warm_restarts',\n",
       "  'args': {'T_0': 25, 'T_mult': 1, 'eta_min': 0, 'last_epoch': -1},\n",
       "  'params': {'interval': 'epoch',\n",
       "   'frequency': 1,\n",
       "   'monitor': 'val_loss',\n",
       "   'strict': True,\n",
       "   'name': None}},\n",
       " 'device': 'cuda:0',\n",
       " 'batch_size': 16,\n",
       " 'path_to_dataset_root': 'C:\\\\Users\\\\admin\\\\python_programming\\\\DATA\\\\DATA_FOR_TRAINIG_96'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet_config_dict = {\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'unet',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': (256, 128, 128, 128, 128),\n",
    "            'decoder_attention_type': None,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96'\n",
    "}\n",
    "\n",
    "fpn_config_dict = {\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'custom_fpn',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_pyramid_channels': 128,\n",
    "            'decoder_segmentation_channels': 128,\n",
    "            'decoder_merge_policy': \"add\",\n",
    "            'decoder_dropout': 0.2,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'upsampling': 0,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                },\n",
    "        },\n",
    "    },\n",
    "    'multispecter_bands_indices': [1, 2, 3, 7, 'ndvi'],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'},\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.0,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "}\n",
    "\n",
    "fcn_config_dict = {\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'fcn',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': 'batchnorm',\n",
    "            'decoder_last_channel': 16,\n",
    "            'decoder_attention_type': None,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation':  None,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (2, 2),\n",
    "                'padding': (1, 1),\n",
    "                },\n",
    "        },\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.0,\n",
    "            },\n",
    "        #'params': {},\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "}\n",
    "\n",
    "fpn_config_dict_dice_ce = {\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'custom_fpn',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_pyramid_channels': 128,\n",
    "            'decoder_segmentation_channels': 128,\n",
    "            'decoder_merge_policy': \"add\",\n",
    "            'decoder_dropout': 0.2,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'upsampling': 0,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                },\n",
    "        },\n",
    "    },\n",
    "    'multispecter_bands_indices': [1, 2, 3, 7, 'ndvi'],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'dice_crossentropy',\n",
    "        #'params': {'weight': 'classes'},\n",
    "        'params': {\n",
    "            'ce_weight':None,\n",
    "            'ce_ignore_index':-100,\n",
    "            'ce_reducion':'mean',\n",
    "            'ce_label_smoothing':0.0,\n",
    "            'dice_mode':'multiclass',\n",
    "            'dice_classes': None,\n",
    "            'dice_log_loss':False,\n",
    "            'dice_from_logits':True,\n",
    "            'dice_smooth':0.0,\n",
    "            'dice_ignore_index':-100,\n",
    "            'dice_eps': 1e-7,\n",
    "            'losses_weight': [0.5, 0.5],\n",
    "            'is_trainable_weights': True,\n",
    "            'weights_processing_type': 'softmax',\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "}\n",
    "\n",
    "fpn_config_dict_dice = {\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'custom_fpn',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_pyramid_channels': 128,\n",
    "            'decoder_segmentation_channels': 128,\n",
    "            'decoder_merge_policy': \"add\",\n",
    "            'decoder_dropout': 0.2,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'upsampling': 0,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                },\n",
    "        },\n",
    "    },\n",
    "    'multispecter_bands_indices': [1, 2, 3, 7, 'ndvi'],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'dice',\n",
    "        #'params': {'weight': 'classes'},\n",
    "        'params': {\n",
    "            'mode':'multiclass',\n",
    "            'classes': None,\n",
    "            'log_loss':False,\n",
    "            'from_logits':True,\n",
    "            'smooth':0.0,\n",
    "            'ignore_index':-100,\n",
    "            'eps': 1e-7,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "}\n",
    "\n",
    "fpn_config_dict_ce = {\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'custom_fpn',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_pyramid_channels': 128,\n",
    "            'decoder_segmentation_channels': 128,\n",
    "            'decoder_merge_policy': \"add\",\n",
    "            'decoder_dropout': 0.2,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'upsampling': 0,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                },\n",
    "        },\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'},\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "}\n",
    "\n",
    "fpn_multisize_input_config_dict_ce = {\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'custom_fpn',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_pyramid_channels': 128,\n",
    "            'decoder_segmentation_channels': 128,\n",
    "            'decoder_merge_policy': \"add\",\n",
    "            'decoder_dropout': 0.2,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'upsampling': 0,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'in_channels': 3,\n",
    "                'out_channels': {\n",
    "                    '1x1': 32,\n",
    "                    '3x3': 32,\n",
    "                    '5x5': 32, \n",
    "                },\n",
    "                'kernel_size': {\n",
    "                    '1x1': 1,\n",
    "                    '3x3': 3,\n",
    "                    '5x5': 5, \n",
    "                },\n",
    "                'stride': {\n",
    "                    '1x1': 1,\n",
    "                    '3x3': 1,\n",
    "                    '5x5': 1, \n",
    "                },\n",
    "                'padding': {\n",
    "                    '1x1': 0,\n",
    "                    '3x3': 1,\n",
    "                    '5x5': 2, \n",
    "                },\n",
    "                'dilation': {\n",
    "                    '1x1': 1,\n",
    "                    '3x3': 1,\n",
    "                    '5x5': 1, \n",
    "                },\n",
    "                'groups': {\n",
    "                    '1x1': 1,\n",
    "                    '3x3': 1,\n",
    "                    '5x5': 1, \n",
    "                },\n",
    "                'bias': {\n",
    "                    '1x1': False,\n",
    "                    '3x3': False,\n",
    "                    '5x5': False, \n",
    "                },\n",
    "                'aggregation_type': 'add',\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'},\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "}\n",
    "\n",
    "fpn_mit_config_dict_ce = {\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'custom_fpn',\n",
    "        'params': {\n",
    "            'encoder_name': \"mit_b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_pyramid_channels': 128,\n",
    "            'decoder_segmentation_channels': 128,\n",
    "            'decoder_merge_policy': \"add\",\n",
    "            'decoder_dropout': 0.2,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'upsampling': 0,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                },\n",
    "        },\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'},\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "}\n",
    "\n",
    "config_dict = fpn_multisize_input_config_dict_ce\n",
    "config_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f7ddf978",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\torch\\nn\\init.py:511: UserWarning: Initializing zero-element tensors is a no-op\n",
      "  warnings.warn(\"Initializing zero-element tensors is a no-op\")\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (256) must match the size of tensor b (0) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[32]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m model = FPNMod(encoder_name=\u001b[33m'\u001b[39m\u001b[33mmit_b0\u001b[39m\u001b[33m'\u001b[39m, image_size=(\u001b[32m96\u001b[39m, \u001b[32m96\u001b[39m))\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m96\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m96\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.shape\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\segmentation_models_pytorch\\base\\model.py:67\u001b[39m, in \u001b[36mSegmentationModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     64\u001b[39m     \u001b[38;5;28mself\u001b[39m.check_input_shape(x)\n\u001b[32m     66\u001b[39m features = \u001b[38;5;28mself\u001b[39m.encoder(x)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m decoder_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m masks = \u001b[38;5;28mself\u001b[39m.segmentation_head(decoder_output)\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.classification_head \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 159\u001b[39m, in \u001b[36mFPNDecoderMod.forward\u001b[39m\u001b[34m(self, features)\u001b[39m\n\u001b[32m    157\u001b[39m p4 = \u001b[38;5;28mself\u001b[39m.p4(p5, c4)\n\u001b[32m    158\u001b[39m p3 = \u001b[38;5;28mself\u001b[39m.p3(p4, c3)\n\u001b[32m--> \u001b[39m\u001b[32m159\u001b[39m p2 = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mp2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mc2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    163\u001b[39m s6 = \u001b[38;5;28mself\u001b[39m.seg_blocks[\u001b[32m0\u001b[39m](p6)\n\u001b[32m    164\u001b[39m s5 = \u001b[38;5;28mself\u001b[39m.seg_blocks[\u001b[32m1\u001b[39m](p5)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1737\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1745\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1746\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1753\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\admin\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\segmentation_models_pytorch\\decoders\\fpn\\decoder.py:41\u001b[39m, in \u001b[36mFPNBlock.forward\u001b[39m\u001b[34m(self, x, skip)\u001b[39m\n\u001b[32m     39\u001b[39m x = F.interpolate(x, scale_factor=\u001b[32m2.0\u001b[39m, mode=\u001b[38;5;28mself\u001b[39m.interpolation_mode)\n\u001b[32m     40\u001b[39m skip = \u001b[38;5;28mself\u001b[39m.skip_conv(skip)\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m x = \u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip\u001b[49m\n\u001b[32m     42\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[31mRuntimeError\u001b[39m: The size of tensor a (256) must match the size of tensor b (0) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "model = FPNMod(encoder_name='mit_b0', image_size=(96, 96))\n",
    "model(torch.randn(1, 3, 96, 96)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1daab2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32, 112, 112])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv = MultisizeConv(**config_dict['segmentation_nn']['input_layer_config']['params'])\n",
    "conv(torch.randn(1, 3, 112, 112)).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "75fcee65",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = smp.Unet()\n",
    "model.get_submodule('encoder.layer1.0.conv1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d8ee3a",
   "metadata": {},
   "source": [
    "# Создание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5037b82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 13, 96, 96]) torch.Size([16, 11, 96, 96])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'unet_efficientnet-b0 2025-09-08-19-44-49'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_dataset_root = config_dict['path_to_dataset_root']\n",
    "\n",
    "path_to_dataset_info_csv = os.path.join(path_to_dataset_root, 'data_info_table.csv')\n",
    "path_to_surface_classes_json = os.path.join(path_to_dataset_root, 'surface_classes.json')\n",
    "\n",
    "input_image_size = config_dict['input_image_size']\n",
    "multispecter_bands_indices = config_dict['multispecter_bands_indices']\n",
    "device = config_dict['device']\n",
    "\n",
    "# чтение списка имен классов поверхностей\n",
    "with open(path_to_surface_classes_json) as fd:\n",
    "    surface_classes_list = json.load(fd)\n",
    "# чтение таблицы с информацией о каждом изображении в выборке\n",
    "images_df = pd.read_csv(path_to_dataset_info_csv)\n",
    "\n",
    "path_to_partition_json = os.path.join(path_to_dataset_root, 'dataset_partition.json')\n",
    "# чтение словаря со списками квадратов, находящихся в обучающей и тестовой выборке\n",
    "with open(path_to_partition_json) as fd:\n",
    "    partition_dict = json.load(fd)\n",
    "\n",
    "# формирование pandas DataFrame-ов с информацией об изображениях обучающей и тестовой выборках\n",
    "train_images_df = []\n",
    "for train_square in partition_dict['train_squares']:\n",
    "    train_images_df.append(images_df[images_df['square_id']==train_square])\n",
    "train_images_df = pd.concat(train_images_df, ignore_index=True)\n",
    "\n",
    "test_images_df = []\n",
    "for test_square in partition_dict['test_squares']:\n",
    "    test_images_df.append(images_df[images_df['square_id']==test_square])\n",
    "test_images_df = pd.concat(test_images_df, ignore_index=True)\n",
    "\n",
    "#train_images_df, test_images_df = train_test_split(images_df, test_size=0.3, random_state=0)\n",
    "\n",
    "class_num = images_df['class_num'].iloc[0]\n",
    "\n",
    "# формирование словаря, отображающейго имя класса поверхности в индекс класса\n",
    "class_name2idx_dict = {n:i for i, n in enumerate(surface_classes_list)}\n",
    "\n",
    "# вычисление распределений пикселей в классах поверхностей \n",
    "classes_pixels_distribution_df = images_df[surface_classes_list]\n",
    "classes_pixels_num = classes_pixels_distribution_df.sum()\n",
    "classes_weights = classes_pixels_num / classes_pixels_num.sum()\n",
    "classes_weights = classes_weights[surface_classes_list].to_numpy().astype(np.float32)\n",
    "\n",
    "input_image_size = config_dict['input_image_size']\n",
    "'''\n",
    "train_transforms = v2.Compose(\n",
    "    [v2.Resize((input_image_size,input_image_size), antialias=True),v2.ToDtype(torch.float32, scale=True)])\n",
    "test_transforms = v2.Compose(\n",
    "    [v2.Resize((input_image_size,input_image_size), antialias=True),v2.ToDtype(torch.float32, scale=True)])\n",
    "'''\n",
    "train_transforms = nn.Identity()\n",
    "test_transforms = nn.Identity()\n",
    "# если ф-ция потерь перекрестная энтропия, то проверяем, есть ли там веса классов\n",
    "if config_dict['loss']['type'] == 'crossentropy':\n",
    "    # если в параметрах функции потерь стоит строка 'classes', надо передать в функцию вектор весов классов\n",
    "    if 'weight' in config_dict['loss']['params']:\n",
    "        if isinstance(config_dict['loss']['params']['weight'], (list, tuple)):\n",
    "            config_dict['loss']['params']['weight'] = torch.tensor(config_dict['loss']['params']['weight'])\n",
    "        \n",
    "        elif config_dict['loss']['params']['weight'] is not None:\n",
    "            config_dict['loss']['params']['weight'] = torch.tensor(classes_weights)\n",
    "\n",
    "# создание функции потерь\n",
    "criterion = criterion_factory_dict[config_dict['loss']['type']](**config_dict['loss']['params'])\n",
    "\n",
    "# если ф-ция потерь перекрестная энтропия, то проверяем, есть ли там веса классов\n",
    "if config_dict['loss']['type'] == 'crossentropy':\n",
    "    # если в параметрах функции потерь стоит строка 'classes', надо передать в функцию вектор весов классов\n",
    "    if 'weight' in config_dict['loss']['params']:\n",
    "        if isinstance(config_dict['loss']['params']['weight'], torch.Tensor):\n",
    "            config_dict['loss']['params']['weight'] = config_dict['loss']['params']['weight'].cpu().tolist()\n",
    "\n",
    "model = create_model(config_dict, segmentation_nns_factory_dict)\n",
    "model = model.to(device)\n",
    "\n",
    "# создаем датасеты и даталоадеры\n",
    "train_dataset = SegmentationDataset(path_to_dataset_root=path_to_dataset_root, samples_df=train_images_df, channel_indices=multispecter_bands_indices, transforms=train_transforms, dtype=torch.float32, device=device)\n",
    "test_dataset = SegmentationDataset(path_to_dataset_root=path_to_dataset_root, samples_df=test_images_df, channel_indices=multispecter_bands_indices, transforms=test_transforms, dtype=torch.float32, device=device)\n",
    "#train_dataset = SegmentationDatasetApplSurf(path_to_dataset_root=path_to_dataset_root, samples_df=test_images_df, channel_indices=channel_indices, name2class_idx_dict=class_name2idx_dict, applicable_surfaces_dict=applicable_surfaces_dict, transforms=test_transforms, device=device)\n",
    "#test_dataset = SegmentationDatasetApplSurf(path_to_dataset_root=path_to_dataset_root, samples_df=test_images_df, channel_indices=channel_indices, name2class_idx_dict=class_name2idx_dict, applicable_surfaces_dict=applicable_surfaces_dict, transforms=test_transforms, device=device)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config_dict['batch_size'], shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=config_dict['batch_size'])\n",
    "\n",
    "# тестовое чтение данных\n",
    "for data, labels in test_loader:\n",
    "    break\n",
    "    pred = model(data)\n",
    "    loss = criterion(pred, labels)\n",
    "    loss.backward()\n",
    "    \n",
    "\n",
    "# тестовая обработка данных нейронной сетью\n",
    "ret = model(data)\n",
    "print(data.shape, ret.shape)\n",
    "\n",
    "createion_time_str = datetime.now().strftime('%Y-%m-%d-%H-%M-%S')\n",
    "nn_arch_str = config_dict[\"segmentation_nn\"][\"nn_architecture\"]\n",
    "nn_encoder_str = config_dict[\"segmentation_nn\"][\"params\"][\"encoder_name\"]\n",
    "model_name = f'{nn_arch_str}_{nn_encoder_str} {createion_time_str}'\n",
    "model_name"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f4c339",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "9e6b7cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############################\n",
      "unet_efficientnet-b0 2025-09-08-19-44-49\n",
      "#############################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\admin\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:654: Checkpoint directory C:\\Users\\admin\\python_programming\\MultispectralSegmentation\\saving_dir\\unet_efficientnet-b0 2025-09-08-19-44-49 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | Unet             | 7.0 M  | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "7.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.0 M     Total params\n",
      "28.196    Total estimated model params size (MB)\n",
      "324       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149: 100%|██████████| 106/106 [00:36<00:00,  2.89it/s, v_num=0, val_loss=1.300, val_iou_UNLABELED=0.859, val_iou_buildings_territory=0.721, val_iou_natural_ground=0.203, val_iou_natural_grow=0.184, val_iou_natural_wetland=0.416, val_iou_natural_wood=0.794, val_iou_quasi_natural_ground=0.0409, val_iou_quasi_natural_grow=0.475, val_iou_quasi_natural_wetland=0.454, val_iou_transport=0.338, val_iou_water=0.863, val_iou_mean=0.486, val_precision_UNLABELED=0.953, val_precision_buildings_territory=0.851, val_precision_natural_ground=0.531, val_precision_natural_grow=0.381, val_precision_natural_wetland=0.643, val_precision_natural_wood=0.834, val_precision_quasi_natural_ground=0.143, val_precision_quasi_natural_grow=0.643, val_precision_quasi_natural_wetland=0.822, val_precision_transport=0.587, val_precision_water=0.935, val_precision_mean=0.666, val_recall_UNLABELED=0.953, val_recall_buildings_territory=0.851, val_recall_natural_ground=0.531, val_recall_natural_grow=0.381, val_recall_natural_wetland=0.643, val_recall_natural_wood=0.834, val_recall_quasi_natural_ground=0.143, val_recall_quasi_natural_grow=0.643, val_recall_quasi_natural_wetland=0.822, val_recall_transport=0.587, val_recall_water=0.935, val_recall_mean=0.666, train_loss=0.777, train_iou_UNLABELED=0.932, train_iou_buildings_territory=0.939, train_iou_natural_ground=0.824, train_iou_natural_grow=0.893, train_iou_natural_wetland=0.936, train_iou_natural_wood=0.957, train_iou_quasi_natural_ground=0.904, train_iou_quasi_natural_grow=0.960, train_iou_quasi_natural_wetland=0.954, train_iou_transport=0.728, train_iou_water=0.900, train_iou_mean=0.902, train_precision_UNLABELED=0.976, train_precision_buildings_territory=0.966, train_precision_natural_ground=0.935, train_precision_natural_grow=0.945, train_precision_natural_wetland=0.968, train_precision_natural_wood=0.975, train_precision_quasi_natural_ground=0.950, train_precision_quasi_natural_grow=0.978, train_precision_quasi_natural_wetland=0.975, train_precision_transport=0.860, train_precision_water=0.954, train_precision_mean=0.953, train_recall_UNLABELED=0.976, train_recall_buildings_territory=0.966, train_recall_natural_ground=0.935, train_recall_natural_grow=0.945, train_recall_natural_wetland=0.968, train_recall_natural_wood=0.975, train_recall_quasi_natural_ground=0.950, train_recall_quasi_natural_grow=0.978, train_recall_quasi_natural_wetland=0.975, train_recall_transport=0.860, train_recall_water=0.954, train_recall_mean=0.953]             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=150` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 149: 100%|██████████| 106/106 [00:36<00:00,  2.88it/s, v_num=0, val_loss=1.300, val_iou_UNLABELED=0.859, val_iou_buildings_territory=0.721, val_iou_natural_ground=0.203, val_iou_natural_grow=0.184, val_iou_natural_wetland=0.416, val_iou_natural_wood=0.794, val_iou_quasi_natural_ground=0.0409, val_iou_quasi_natural_grow=0.475, val_iou_quasi_natural_wetland=0.454, val_iou_transport=0.338, val_iou_water=0.863, val_iou_mean=0.486, val_precision_UNLABELED=0.953, val_precision_buildings_territory=0.851, val_precision_natural_ground=0.531, val_precision_natural_grow=0.381, val_precision_natural_wetland=0.643, val_precision_natural_wood=0.834, val_precision_quasi_natural_ground=0.143, val_precision_quasi_natural_grow=0.643, val_precision_quasi_natural_wetland=0.822, val_precision_transport=0.587, val_precision_water=0.935, val_precision_mean=0.666, val_recall_UNLABELED=0.953, val_recall_buildings_territory=0.851, val_recall_natural_ground=0.531, val_recall_natural_grow=0.381, val_recall_natural_wetland=0.643, val_recall_natural_wood=0.834, val_recall_quasi_natural_ground=0.143, val_recall_quasi_natural_grow=0.643, val_recall_quasi_natural_wetland=0.822, val_recall_transport=0.587, val_recall_water=0.935, val_recall_mean=0.666, train_loss=0.777, train_iou_UNLABELED=0.932, train_iou_buildings_territory=0.939, train_iou_natural_ground=0.824, train_iou_natural_grow=0.893, train_iou_natural_wetland=0.936, train_iou_natural_wood=0.957, train_iou_quasi_natural_ground=0.904, train_iou_quasi_natural_grow=0.960, train_iou_quasi_natural_wetland=0.954, train_iou_transport=0.728, train_iou_water=0.900, train_iou_mean=0.902, train_precision_UNLABELED=0.976, train_precision_buildings_territory=0.966, train_precision_natural_ground=0.935, train_precision_natural_grow=0.945, train_precision_natural_wetland=0.968, train_precision_natural_wood=0.975, train_precision_quasi_natural_ground=0.950, train_precision_quasi_natural_grow=0.978, train_precision_quasi_natural_wetland=0.975, train_precision_transport=0.860, train_precision_water=0.954, train_precision_mean=0.953, train_recall_UNLABELED=0.976, train_recall_buildings_territory=0.966, train_recall_natural_ground=0.935, train_recall_natural_grow=0.945, train_recall_natural_wetland=0.968, train_recall_natural_wood=0.975, train_recall_quasi_natural_ground=0.950, train_recall_quasi_natural_grow=0.978, train_recall_quasi_natural_wetland=0.975, train_recall_transport=0.860, train_recall_water=0.954, train_recall_mean=0.953]\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 150\n",
    "\n",
    "print('#############################')\n",
    "print(model_name)\n",
    "print('#############################')\n",
    "print()\n",
    "\n",
    "# создаем список словарей с информацией о вычисляемых метриках с помощью multiclass confusion matrix\n",
    "# см. подробнее ддокументацию к функции compute_metric_from_confusion\n",
    "metrics_dict = {\n",
    "    'train': {\n",
    "        'iou': classification.JaccardIndex(task='multiclass', average='none', num_classes=len(class_name2idx_dict)).to(device),\n",
    "        'precision': classification.Precision(task='multiclass', average='none', num_classes=len(class_name2idx_dict)).to(device),\n",
    "        'recall': classification.Precision(task='multiclass', average='none', num_classes=len(class_name2idx_dict)).to(device),\n",
    "        #'confusion': classification.ConfusionMatrix(task='multiclass', num_classes=len(class_name2idx_dict)).to(device),\n",
    "    },\n",
    "    'val': {\n",
    "        'iou': classification.JaccardIndex(task='multiclass', average='none', num_classes=len(class_name2idx_dict)).to(device),\n",
    "        'precision': classification.Precision(task='multiclass', average='none', num_classes=len(class_name2idx_dict)).to(device),\n",
    "        'recall': classification.Precision(task='multiclass', average='none', num_classes=len(class_name2idx_dict)).to(device),\n",
    "        #'confusion': classification.ConfusionMatrix(task='multiclass', num_classes=len(class_name2idx_dict)).to(device),\n",
    "    }\n",
    "}\n",
    "\n",
    "optimizer_cfg = {\n",
    "    'optmizer': optimizers_factory_dict[config_dict['optimizer']['type']],\n",
    "    'optimizer_args':config_dict['optimizer']['args'],\n",
    "    'lr_scheduler': lr_schedulers_factory_dict[config_dict['lr_scheduler']['type']],\n",
    "    'lr_scheduler_args': config_dict['lr_scheduler']['args'],\n",
    "    'lr_scheduler_params': config_dict['lr_scheduler']['params']\n",
    "\n",
    "}\n",
    "\n",
    "# Создаем модуль Lightning\n",
    "segmentation_module = SegmentationModule(model, criterion, optimizer_cfg, metrics_dict, class_name2idx_dict)\n",
    "\n",
    "# задаем путь до папки с логгерами и создаем логгер, записывающий результаты в csv\n",
    "path_to_saving_dir = 'saving_dir'\n",
    "csv_logger = CSVLogger(\n",
    "    save_dir = path_to_saving_dir,\n",
    "    name=model_name, \n",
    "    flush_logs_every_n_steps=1,\n",
    "    )\n",
    "\n",
    "# создаем объект, записывающий в чекпоинт лучшую модель\n",
    "path_to_save_model_dir = os.path.join(path_to_saving_dir, model_name)\n",
    "os.makedirs(path_to_save_model_dir, exist_ok=True)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    mode=\"max\",\n",
    "    filename=model_name+\"-{epoch:02d}-{val_iou_mean:.3}\",\n",
    "    dirpath=path_to_save_model_dir, \n",
    "    save_top_k=1, monitor=\"val_iou_mean\"\n",
    "    )\n",
    "\n",
    "trainer = L.Trainer(logger=[csv_logger],\n",
    "        max_epochs=epoch_num, \n",
    "        callbacks=[checkpoint_callback],\n",
    "        accelerator = 'gpu'\n",
    "        )\n",
    "\n",
    "# сохраняем конфигурацию\n",
    "path_to_config = os.path.join(path_to_save_model_dir, 'training_config.yaml')\n",
    "with open(path_to_config, 'w', encoding='utf-8') as fd:\n",
    "    #json.dump(config_dict, fd, indent=4)\n",
    "    yaml.dump(config_dict, fd, indent=4)\n",
    "\n",
    "trainer.fit(segmentation_module , train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "12570f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "path_to_config = 'training_config.yaml'\n",
    "with open(path_to_config, 'w', encoding='utf-8') as fd:\n",
    "    #json.dump(config_dict, fd, indent=4)\n",
    "    yaml.dump(config_dict, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2cd5d12d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'saving_dir\\\\unet++_efficientnet-b0 2025-09-07-02-29-35\\\\version_0'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logger.log_dir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54847ac5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
