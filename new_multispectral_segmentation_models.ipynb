{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f0cede1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import json\n",
    "import yaml\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "\n",
    "from itertools import combinations, product\n",
    "import warnings\n",
    "\n",
    "from functools import partial\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import tv_tensors\n",
    "from torchvision import models\n",
    "\n",
    "from torchvision.models.vision_transformer import EncoderBlock as VitEncoderBlock, MLPBlock\n",
    "\n",
    "import types\n",
    "\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "from segmentation_models_pytorch.base import (\n",
    "    ClassificationHead,\n",
    "    SegmentationHead,\n",
    "    SegmentationModel,\n",
    ")\n",
    "\n",
    "from segmentation_models_pytorch.base import modules as md\n",
    "\n",
    "from segmentation_models_pytorch.encoders import get_encoder\n",
    "from segmentation_models_pytorch.base.hub_mixin import supports_config_loading\n",
    "from typing import Any, Dict, Optional, Union, Callable, Sequence, List, Literal\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger, Logger\n",
    "from lightning.pytorch.utilities.rank_zero import rank_zero_only\n",
    "\n",
    "from torchmetrics import classification\n",
    "from torchmetrics import segmentation\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from copy import deepcopy\n",
    "\n",
    "import einops as eo\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "2fe36263",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 200, 96, 96])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "class ComputeWeights(nn.Sequential):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding):\n",
    "        mod_lst = []\n",
    "        mod_lst.append(nn.Conv2d(in_channels=in_channels, out_channels=out_channels, kernel_size=kernel_size, padding=padding))\n",
    "        mod_lst.append(nn.Sigmoid())\n",
    "        super().__init__(*mod_lst)\n",
    "        \n",
    "\n",
    "class ChannelAtt(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding):\n",
    "        super().__init__()\n",
    "        self.in2_proj = ComputeWeights(in_channels, out_channels, kernel_size, padding)\n",
    "\n",
    "    def forward(self, in1, in2):\n",
    "        weights = self.in2_proj(in2)\n",
    "        bs, ch1, rows1, cols1 = in1.shape\n",
    "        bs, ch2, rows2, cols2 = weights.shape\n",
    "        winr = rows1//rows2\n",
    "        winc = cols1//cols2\n",
    "\n",
    "        weights = weights.view(bs, ch2, rows2, cols2, 1, 1)\n",
    "\n",
    "        in1 = eo.rearrange(in1, 'b ch (rn wr) (cn wc) -> b ch rn cn wr wc', rn=rows2, cn=cols2, wr=winr, wc=winc)\n",
    "\n",
    "        in1 = in1*weights\n",
    "\n",
    "        in1 = eo.rearrange(in1, 'b ch rn cn wr wc -> b ch (rn wr) (cn wc)', rn=rows2, cn=cols2, wr=winr, wc=winc)\n",
    "        return in1\n",
    "in1 = torch.randn(1, 200, 96, 96)\n",
    "in2 = torch.randn(1, 200, 24, 24)\n",
    "\n",
    "chat = ChannelAtt(in_channels=200, out_channels=200, kernel_size=3, padding=1)\n",
    "ret = chat(in1, in2)\n",
    "ret.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960aa9a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "in1 = torch.arange(0, 72).reshape(1, 2, 6, 6,)\n",
    "in2 = torch.arange(0, 18).reshape(1, 2, 3, 3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0a84349",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 0,  1],\n",
       "           [ 6,  7]],\n",
       " \n",
       "          [[ 2,  3],\n",
       "           [ 8,  9]],\n",
       " \n",
       "          [[ 4,  5],\n",
       "           [10, 11]]],\n",
       " \n",
       " \n",
       "         [[[12, 13],\n",
       "           [18, 19]],\n",
       " \n",
       "          [[14, 15],\n",
       "           [20, 21]],\n",
       " \n",
       "          [[16, 17],\n",
       "           [22, 23]]],\n",
       " \n",
       " \n",
       "         [[[24, 25],\n",
       "           [30, 31]],\n",
       " \n",
       "          [[26, 27],\n",
       "           [32, 33]],\n",
       " \n",
       "          [[28, 29],\n",
       "           [34, 35]]]]),\n",
       " tensor([[[[0]],\n",
       " \n",
       "          [[1]],\n",
       " \n",
       "          [[2]]],\n",
       " \n",
       " \n",
       "         [[[3]],\n",
       " \n",
       "          [[4]],\n",
       " \n",
       "          [[5]]],\n",
       " \n",
       " \n",
       "         [[[6]],\n",
       " \n",
       "          [[7]],\n",
       " \n",
       "          [[8]]]]),\n",
       " tensor([[[[  0,   0],\n",
       "           [  0,   0]],\n",
       " \n",
       "          [[  2,   3],\n",
       "           [  8,   9]],\n",
       " \n",
       "          [[  8,  10],\n",
       "           [ 20,  22]]],\n",
       " \n",
       " \n",
       "         [[[ 36,  39],\n",
       "           [ 54,  57]],\n",
       " \n",
       "          [[ 56,  60],\n",
       "           [ 80,  84]],\n",
       " \n",
       "          [[ 80,  85],\n",
       "           [110, 115]]],\n",
       " \n",
       " \n",
       "         [[[144, 150],\n",
       "           [180, 186]],\n",
       " \n",
       "          [[182, 189],\n",
       "           [224, 231]],\n",
       " \n",
       "          [[224, 232],\n",
       "           [272, 280]]]]))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in1 = torch.arange(0, 72).reshape(1, 2, 6, 6,)\n",
    "in2 = torch.arange(0, 18).reshape(1, 2, 3, 3)\n",
    "\n",
    "bs, ch1, rows1, cols1 = in1.shape\n",
    "bs, ch2, rows2, cols2 = in2.shape\n",
    "\n",
    "winr = rows1//rows2\n",
    "winc = cols1//cols2\n",
    "\n",
    "in2 = in2.view(bs, ch2, rows2, cols2, 1, 1)\n",
    "\n",
    "in1 = eo.rearrange(in1, 'b ch (rn wr) (cn wc) -> b ch rn cn wr wc', rn=rows2, cn=cols2, wr=winr, wc=winc)\n",
    "ret = in1 * in2\n",
    "in1[0, 0], in2[0, 0], ret[0,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "26804254",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[[[ 0,  2,  4],\n",
       "            [12, 14, 16],\n",
       "            [24, 26, 28]],\n",
       "\n",
       "           [[ 1,  3,  5],\n",
       "            [13, 15, 17],\n",
       "            [25, 27, 29]]],\n",
       "\n",
       "\n",
       "          [[[ 6,  8, 10],\n",
       "            [18, 20, 22],\n",
       "            [30, 32, 34]],\n",
       "\n",
       "           [[ 7,  9, 11],\n",
       "            [19, 21, 23],\n",
       "            [31, 33, 35]]]],\n",
       "\n",
       "\n",
       "\n",
       "         [[[[36, 38, 40],\n",
       "            [48, 50, 52],\n",
       "            [60, 62, 64]],\n",
       "\n",
       "           [[37, 39, 41],\n",
       "            [49, 51, 53],\n",
       "            [61, 63, 65]]],\n",
       "\n",
       "\n",
       "          [[[42, 44, 46],\n",
       "            [54, 56, 58],\n",
       "            [66, 68, 70]],\n",
       "\n",
       "           [[43, 45, 47],\n",
       "            [55, 57, 59],\n",
       "            [67, 69, 71]]]]]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf4f6468",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6b23ebb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "any([n in 'pidoras' for n in ['huy', 'pidor']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0fa0903b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.5000), tensor(0.4375), tensor(0.5000))"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "acc = classification.Accuracy(task='multiclass', num_classes=4)\n",
    "micro_avg_acc = classification.Accuracy(task='multiclass', average='micro', num_classes=4)\n",
    "macro_avg_acc = classification.Accuracy(task='multiclass', average='macro', num_classes=4)\n",
    "target = torch.tensor([0, 2, 2, 3])\n",
    "preds = torch.tensor([0, 2, 1, 2])\n",
    "acc.update(preds, target)\n",
    "micro_avg_acc.update(preds, target)\n",
    "macro_avg_acc.update(preds, target)\n",
    "\n",
    "target = torch.tensor([0, 2, 2, 3])\n",
    "preds = torch.tensor([0, 2, 2, 1])\n",
    "acc.update(preds, target)\n",
    "micro_avg_acc.update(preds, target)\n",
    "macro_avg_acc.update(preds, target)\n",
    "\n",
    "target = torch.tensor([0, 1, 3, 3])\n",
    "preds = torch.tensor([0, 2, 2, 2])\n",
    "acc.update(preds, target)\n",
    "micro_avg_acc.update(preds, target)\n",
    "macro_avg_acc.update(preds, target)\n",
    "\n",
    "micro_avg_acc.compute(), macro_avg_acc.compute(), acc.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e32608",
   "metadata": {},
   "source": [
    "# Работа с данными"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "678fd044",
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "class SegmentationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path_to_dataset_root:str, samples_df:pd.DataFrame, channel_indices:list, transforms:v2._transform.Transform, dtype:torch.dtype, device:torch.device):\n",
    "        '''\n",
    "        In:\n",
    "            path_to_dataset_root - путь до корневой папки с датасетом\n",
    "            samples_df - pandas.DataFrame с информацией о файлах\n",
    "            channel_indices - список с номерами каналов мультиспектрального изображения\n",
    "            transforms - аугментация изображений\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.path_to_dataset_root = path_to_dataset_root\n",
    "        self.samples_df = samples_df\n",
    "        self.specter_bands_list = [i for i in channel_indices if isinstance(i, int)]\n",
    "        self.specter_indices_names = [s for s in channel_indices if isinstance(s, str)]\n",
    "        self.dtype_trasform = v2.ToDtype(dtype=dtype, scale=True)\n",
    "        self.other_transforms = transforms\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples_df)\n",
    "    @staticmethod\n",
    "    def compute_spectral_index(index_name, image):\n",
    "        if index_name.lower() == 'ndvi':\n",
    "            b0 = image[7] # NIR, B8\n",
    "            b1 = image[3] # RED, B4\n",
    "            \n",
    "        elif index_name.lower() == 'ndbi':\n",
    "            b0 = image[10] #SWIR, B11\n",
    "            b1 = image[7] #NIR, B8\n",
    "\n",
    "        elif index_name.lower() == 'ndwi':\n",
    "            b0 = image[2] #green, B3\n",
    "            b1 = image[7] #NIR, B8\n",
    "\n",
    "        elif index_name.lower() == 'ndre':\n",
    "            b0 = image[7] #NIR, B8\n",
    "            b1 = image[5] #Red Edge, B6\n",
    "        \n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter('ignore')\n",
    "            index = (b0 - b1)/(b0 + b1)\n",
    "            \n",
    "        index = np.nan_to_num(index, nan=-5)\n",
    "\n",
    "        return index    \n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples_df.iloc[idx]\n",
    "\n",
    "        file_name = sample['file_name']\n",
    "\n",
    "        path_to_image = os.path.join(self.path_to_dataset_root, 'images', f'{file_name}.npy')\n",
    "        path_to_labels = os.path.join(self.path_to_dataset_root, 'labels', f'{file_name}.npy')\n",
    "\n",
    "        image = np.load(path_to_image)\n",
    "        spectral_indices = []\n",
    "        # вычисляем спектральные индексы\n",
    "        if len(self.specter_indices_names) > 0:\n",
    "            for sp_index_name in self.specter_indices_names:\n",
    "                spectral_index = self.compute_spectral_index(sp_index_name, image)\n",
    "                spectral_index = torch.as_tensor(spectral_index)\n",
    "                spectral_indices.append(spectral_index.unsqueeze(0))\n",
    "\n",
    "            spectral_indices = torch.cat(spectral_indices)\n",
    "            spectral_indices = self.dtype_trasform(spectral_indices)\n",
    "\n",
    "        image = torch.as_tensor(image[self.specter_bands_list], dtype=torch.int16)\n",
    "        image = self.dtype_trasform(image)\n",
    "        # добавляем спектральные индексы\n",
    "        if len(self.specter_indices_names) > 0:\n",
    "            image = torch.cat([image, spectral_indices], dim=0) \n",
    "        #image = np.load(path_to_image)\n",
    "        # метки читаем как одноканальное изображение\n",
    "        label = np.load(path_to_labels)\n",
    "        label = np.where(label >= 0, label, 0)\n",
    "        #label = torch.as_tensor(np.load(path_to_labels), dtype=torch.uint8).long()\n",
    "        label = torch.as_tensor(label, dtype=torch.uint8).long()\n",
    "        \n",
    "        image = tv_tensors.Image(image, device=self.device)\n",
    "        label = tv_tensors.Mask(label, device=self.device)\n",
    "\n",
    "        transforms_dict = {'image':image, 'mask':label}\n",
    "        transformed = self.other_transforms(transforms_dict)\n",
    "        return transformed['image'], transformed['mask']#, image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fac0331",
   "metadata": {},
   "source": [
    "# Описание модуля Lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7082272f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_pred_mask(pred):\n",
    "    '''\n",
    "    Определение маски классов на основе сгенерированной softmax маски\n",
    "    '''\n",
    "    #pred = pred.detach()\n",
    "    _, pred_mask = pred.max(dim=1)\n",
    "    return pred_mask#.cpu().numpy()\n",
    "\n",
    "class CSVLoggerMetricsAndConfusion(CSVLogger):\n",
    "    @rank_zero_only\n",
    "    def save_confusion(self, epoch_idx, confusion_matrix, class_names, mode):\n",
    "        os.makedirs(self.log_dir, exist_ok=True)\n",
    "        path_to_saving_file = os.path.join(self.log_dir, f'{mode}_confusion_matrices.csv')\n",
    "        if os.path.isfile(path_to_saving_file):\n",
    "            # читаем матрицы ошибок\n",
    "            confusion_df = pd.read_csv(path_to_saving_file)\n",
    "            multiindex = pd.MultiIndex.from_arrays([confusion_df['epoch'], confusion_df['classes']])\n",
    "            confusion_df = confusion_df.set_index(multiindex)\n",
    "            confusion_df = confusion_df.drop(columns=['epoch', 'classes'])\n",
    "        else:\n",
    "            confusion_df = pd.DataFrame()\n",
    "\n",
    "        multiindex = pd.MultiIndex.from_product([[epoch_idx], class_names], names=['epoch', 'classes'])\n",
    "        epoch_confusion_df = pd.DataFrame(data=confusion_matrix, columns=class_names, index=multiindex)\n",
    "        confusion_df = pd.concat([confusion_df, epoch_confusion_df])\n",
    "        confusion_df.to_csv(path_to_saving_file)\n",
    "\n",
    "class LightningSegmentationModule(L.LightningModule):\n",
    "    def __init__(self, model:nn.Module, criterion:nn.Module, optimizer_cfg:dict, metrics_dict:dict, name2class_idx_dict:dict) -> None:\n",
    "        '''\n",
    "        Модуль Lightning для обучения сегментационной сети\n",
    "        In:\n",
    "            model - нейронная сеть\n",
    "            criterion - функция потерь\n",
    "            \n",
    "            name2class_idx_dict - словарь с отображением {class_name(str): class_idx(int)}\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.optimizer_cfg = optimizer_cfg\n",
    "        self.metrics_dict = metrics_dict\n",
    "        \n",
    "        self.name2class_idx_dict = name2class_idx_dict\n",
    "        # словарь, выполняющий обратное отображение class_idx в class_name\n",
    "        self.class_idx2name_dict = {v:k for k, v in name2class_idx_dict.items()}\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self.optimizer_cfg['optmizer'](self.parameters(), **self.optimizer_cfg['optimizer_args'])\n",
    "        ret_dict = {'optimizer': optimizer}\n",
    "        if self.optimizer_cfg['lr_scheduler'] is not None:\n",
    "            scheduler = self.optimizer_cfg['lr_scheduler'](optimizer, **self.optimizer_cfg['lr_scheduler_args'])\n",
    "            ret_dict['lr_scheduler'] = {'scheduler': scheduler}\n",
    "            ret_dict['lr_scheduler'].update(self.optimizer_cfg['lr_scheduler_params'])\n",
    "        \n",
    "        return ret_dict\n",
    "\n",
    "    def compute_metrics(self, pred_labels, true_labels, mode):\n",
    "        metrics_names_list = self.metrics_dict[mode].keys()\n",
    "        for metric_name in metrics_names_list:\n",
    "            if 'dice' in metric_name.lower():\n",
    "                self.metrics_dict[mode][metric_name].update(pred_labels, true_labels)\n",
    "            else:\n",
    "                self.metrics_dict[mode][metric_name].update(pred_labels.reshape(-1), true_labels.reshape(-1))    \n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, true_labels = batch\n",
    "        pred = self.model(data)\n",
    "        loss = self.criterion(pred, true_labels)\n",
    "        # вычисление сгенерированной маски\n",
    "        pred_labels = compute_pred_mask(pred)\n",
    "        #true_labels = true_labels.detach().cpu().numpy()\n",
    "        \n",
    "        self.compute_metrics(pred_labels=pred_labels, true_labels=true_labels, mode='train')\n",
    "\n",
    "        # т.к. мы вычисляем общую ошибку на всей эпохе, то записываем в лог только значение функции потерь\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data, true_labels = batch\n",
    "        pred = self.model(data)\n",
    "        loss = self.criterion(pred, true_labels)\n",
    "        pred_labels = compute_pred_mask(pred)\n",
    "        self.compute_metrics(pred_labels=pred_labels, true_labels=true_labels, mode='val')\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def log_metrics(self, mode):\n",
    "        for metric_name, metric in self.metrics_dict[mode].items():\n",
    "            metric_val = metric.compute()\n",
    "            if 'confusion' in metric_name.lower():\n",
    "                disp_name = f'{mode}_{metric_name}'\n",
    "                class_names = [self.class_idx2name_dict[i] for i in range(len(self.class_idx2name_dict))]\n",
    "                if isinstance(self.logger, CSVLoggerMetricsAndConfusion):\n",
    "                    self.logger.save_confusion(\n",
    "                        epoch_idx=self.current_epoch,\n",
    "                        confusion_matrix=metric_val.cpu().tolist(),\n",
    "                        class_names=class_names,\n",
    "                        mode=mode)\n",
    "            else:\n",
    "                for i, value in enumerate(metric_val):\n",
    "                    class_name = self.class_idx2name_dict[i]\n",
    "                    disp_name = f'{mode}_{metric_name}_{class_name}'\n",
    "                    self.log(disp_name, value, on_step=False, on_epoch=True, prog_bar=True)\n",
    "                disp_name = f'{mode}_{metric_name}_mean'\n",
    "                self.log(disp_name, metric_val.mean(), on_step=False, on_epoch=True, prog_bar=True)\n",
    "            self.metrics_dict[mode][metric_name].reset()\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        '''\n",
    "        Декодирование результатов тренировочной эпохи и запись их в лог\n",
    "        '''\n",
    "        self.log_metrics(mode='train')\n",
    " \n",
    "    def on_validation_epoch_end(self):\n",
    "        '''\n",
    "        Декодирование результатов тестовой эпохи и запись их в лог\n",
    "        (работает точно также, как и )\n",
    "        '''\n",
    "        self.log_metrics(mode='val')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00b4de78",
   "metadata": {},
   "source": [
    "# Новые функции потерь (Dice-Crossentropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5889a22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DiceCELoss(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            ce_weight,\n",
    "            ce_ignore_index,\n",
    "            ce_reducion,\n",
    "            ce_label_smoothing,\n",
    "            dice_mode,\n",
    "            dice_classes,\n",
    "            dice_log_loss,\n",
    "            dice_from_logits,\n",
    "            dice_smooth,\n",
    "            dice_ignore_index,\n",
    "            dice_eps,\n",
    "            losses_weight: List = [0.5, 0.5],\n",
    "            is_trainable_weights: bool = False,\n",
    "            weights_processing_type: str = None,\n",
    "            ):\n",
    "        super().__init__()\n",
    "        self.dice = smp.losses.DiceLoss(\n",
    "            mode=dice_mode,\n",
    "            classes=dice_classes,\n",
    "            log_loss=dice_log_loss,\n",
    "            from_logits=dice_from_logits,\n",
    "            smooth=dice_smooth,\n",
    "            ignore_index=dice_ignore_index,\n",
    "            eps=dice_eps\n",
    "            )\n",
    "        self.ce = nn.CrossEntropyLoss(\n",
    "            weight=ce_weight,\n",
    "            ignore_index=ce_ignore_index,\n",
    "            reduction=ce_reducion,\n",
    "            label_smoothing=ce_label_smoothing,\n",
    "        )\n",
    "        self.loss_weights = torch.tensor(losses_weight)\n",
    "        if is_trainable_weights:\n",
    "            self.loss_weights = nn.Parameter(self.loss_weights)\n",
    "        self.weights_processing_type = weights_processing_type\n",
    "\n",
    "    def forward(self, pred, true):\n",
    "        weights = self.loss_weights\n",
    "        if self.weights_processing_type == 'softmax':\n",
    "            weights = weights.softmax(dim=0)\n",
    "        elif self.weights_processing_type == 'sigmoid':\n",
    "            weights = weights.softmax(dim=0)\n",
    "\n",
    "        ce_loss = self.ce(pred, true) * weights[0]\n",
    "        dice_loss = self.dice(pred, true) * weights[1]\n",
    "        return ce_loss + dice_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aaa745e",
   "metadata": {},
   "source": [
    "# Адаптация UNet++"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "7124811f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetppDecoderBlockMod(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        skip_channels: int,\n",
    "        out_channels: int,\n",
    "        use_norm: Union[bool, str, Dict[str, Any]] = \"batchnorm\",\n",
    "        attention_type: Optional[str] = None,\n",
    "        interpolation_mode: str = \"nearest\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.conv1 = md.Conv2dReLU(\n",
    "            in_channels + skip_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_norm=use_norm,\n",
    "        )\n",
    "        self.attention1 = md.Attention(\n",
    "            attention_type, in_channels=in_channels + skip_channels\n",
    "        )\n",
    "        self.conv2 = md.Conv2dReLU(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_norm=use_norm,\n",
    "        )\n",
    "        self.attention2 = md.Attention(attention_type, in_channels=out_channels)\n",
    "        self.interpolation_mode = interpolation_mode\n",
    "\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, skip: Optional[torch.Tensor] = None, scale_factor: float = 2.0\n",
    "    ) -> torch.Tensor:\n",
    "        if scale_factor != 1:\n",
    "            x = F.interpolate(x, scale_factor=scale_factor, mode=self.interpolation_mode)\n",
    "        if skip is not None:\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "            x = self.attention1(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.attention2(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class UnetppCenterBlock(nn.Sequential):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        use_norm: Union[bool, str, Dict[str, Any]] = \"batchnorm\",\n",
    "    ):\n",
    "        conv1 = md.Conv2dReLU(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_norm=use_norm,\n",
    "        )\n",
    "        conv2 = md.Conv2dReLU(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_norm=use_norm,\n",
    "        )\n",
    "        super().__init__(conv1, conv2)\n",
    "\n",
    "\n",
    "class UnetPlusPlusDecoderMod(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_channels: Sequence[int],\n",
    "        decoder_channels: Sequence[int],\n",
    "        n_blocks: int = 5,\n",
    "        use_norm: Union[bool, str, Dict[str, Any]] = \"batchnorm\",\n",
    "        attention_type: Optional[str] = None,\n",
    "        interpolation_mode: str = \"nearest\",\n",
    "        center: bool = False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if n_blocks != len(decoder_channels):\n",
    "            raise ValueError(\n",
    "                f\"Model depth is {n_blocks}, but you provide `decoder_channels` for {len(decoder_channels)} blocks.\"\n",
    "            )\n",
    "\n",
    "        # remove first skip with same spatial resolution\n",
    "        encoder_channels = encoder_channels[1:]\n",
    "        # reverse channels to start from head of encoder\n",
    "        encoder_channels = encoder_channels[::-1]\n",
    "\n",
    "        # computing blocks input and output channels\n",
    "        head_channels = encoder_channels[0]\n",
    "        self.in_channels = [head_channels] + list(decoder_channels[:-1])\n",
    "        self.skip_channels = list(encoder_channels[1:]) + [0]\n",
    "        self.out_channels = decoder_channels\n",
    "        if center:\n",
    "            self.center = UnetppCenterBlock(\n",
    "                head_channels,\n",
    "                head_channels,\n",
    "                use_norm=use_norm,\n",
    "            )\n",
    "        else:\n",
    "            self.center = nn.Identity()\n",
    "\n",
    "        # combine decoder keyword arguments\n",
    "        kwargs = dict(\n",
    "            use_norm=use_norm,\n",
    "            attention_type=attention_type,\n",
    "            interpolation_mode=interpolation_mode,\n",
    "        )\n",
    "\n",
    "        blocks = {}\n",
    "        for layer_idx in range(len(self.in_channels) - 1):\n",
    "            for depth_idx in range(layer_idx + 1):\n",
    "                if depth_idx == 0:\n",
    "                    in_ch = self.in_channels[layer_idx]\n",
    "                    skip_ch = self.skip_channels[layer_idx] * (layer_idx + 1)\n",
    "                    out_ch = self.out_channels[layer_idx]\n",
    "                else:\n",
    "                    out_ch = self.skip_channels[layer_idx]\n",
    "                    skip_ch = self.skip_channels[layer_idx] * (\n",
    "                        layer_idx + 1 - depth_idx\n",
    "                    )\n",
    "                    in_ch = self.skip_channels[layer_idx - 1]\n",
    "                blocks[f\"x_{depth_idx}_{layer_idx}\"] = UnetppDecoderBlockMod(\n",
    "                    in_ch, skip_ch, out_ch, **kwargs\n",
    "                )\n",
    "        blocks[f\"x_{0}_{len(self.in_channels) - 1}\"] = UnetppDecoderBlockMod(\n",
    "            self.in_channels[-1], 0, self.out_channels[-1], **kwargs\n",
    "        )\n",
    "        self.blocks = nn.ModuleDict(blocks)\n",
    "        self.depth = len(self.in_channels) - 1\n",
    "\n",
    "    def forward(self, features: List[torch.Tensor]) -> torch.Tensor:\n",
    "        # если \n",
    "        bs, channels, img_rows, img_cols = features[0].shape\n",
    "        _, _, feat1_rows, feat1_cols = features[1].shape\n",
    "        #upsample_scale_factors_list = [2.0 for i in range(len(self.blocks))]\n",
    "        if (img_rows, img_cols) == (feat1_rows, feat1_cols):\n",
    "            output_upsample_scaling_factor = 1.0\n",
    "        else:\n",
    "            output_upsample_scaling_factor = 2.0\n",
    "\n",
    "        features = features[1:]  # remove first skip with same spatial resolution\n",
    "        features = features[::-1]  # reverse channels to start from head of encoder\n",
    "\n",
    "        # start building dense connections\n",
    "        dense_x = {}\n",
    "        for layer_idx in range(len(self.in_channels) - 1):\n",
    "            for depth_idx in range(self.depth - layer_idx):\n",
    "                if layer_idx == 0:\n",
    "                    output = self.blocks[f\"x_{depth_idx}_{depth_idx}\"](\n",
    "                        features[depth_idx], features[depth_idx + 1]\n",
    "                    )\n",
    "                    dense_x[f\"x_{depth_idx}_{depth_idx}\"] = output\n",
    "                else:\n",
    "                    dense_l_i = depth_idx + layer_idx\n",
    "                    cat_features = [\n",
    "                        dense_x[f\"x_{idx}_{dense_l_i}\"]\n",
    "                        for idx in range(depth_idx + 1, dense_l_i + 1)\n",
    "                    ]\n",
    "                    cat_features = torch.cat(\n",
    "                        cat_features + [features[dense_l_i + 1]], dim=1\n",
    "                    )\n",
    "                    dense_x[f\"x_{depth_idx}_{dense_l_i}\"] = self.blocks[\n",
    "                        f\"x_{depth_idx}_{dense_l_i}\"\n",
    "                    ](dense_x[f\"x_{depth_idx}_{dense_l_i - 1}\"], cat_features)\n",
    "        \n",
    "        dense_x[f\"x_{0}_{self.depth}\"] = self.blocks[f\"x_{0}_{self.depth}\"](\n",
    "            dense_x[f\"x_{0}_{self.depth - 1}\"], scale_factor=output_upsample_scaling_factor\n",
    "        )\n",
    "        return dense_x[f\"x_{0}_{self.depth}\"]\n",
    "\n",
    "class UnetPlusPlusMod(SegmentationModel):\n",
    "    \"\"\"Unet++ is a fully convolution neural network for image semantic segmentation. Consist of *encoder*\n",
    "    and *decoder* parts connected with *skip connections*. Encoder extract features of different spatial\n",
    "    resolution (skip connections) which are used by decoder to define accurate segmentation mask. Decoder of\n",
    "    Unet++ is more complex than in usual Unet.\n",
    "\n",
    "    Args:\n",
    "        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n",
    "            to extract features of different spatial resolution\n",
    "        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features\n",
    "            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n",
    "            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n",
    "            Default is 5\n",
    "        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and\n",
    "            other pretrained weights (see table with available weights for each encoder_name)\n",
    "        decoder_channels: List of integers which specify **in_channels** parameter for convolutions used in decoder.\n",
    "            Length of the list should be the same as **encoder_depth**\n",
    "        decoder_use_norm:     Specifies normalization between Conv2D and activation.\n",
    "            Accepts the following types:\n",
    "            - **True**: Defaults to `\"batchnorm\"`.\n",
    "            - **False**: No normalization (`nn.Identity`).\n",
    "            - **str**: Specifies normalization type using default parameters. Available values:\n",
    "              `\"batchnorm\"`, `\"identity\"`, `\"layernorm\"`, `\"instancenorm\"`, `\"inplace\"`.\n",
    "            - **dict**: Fully customizable normalization settings. Structure:\n",
    "              ```python\n",
    "              {\"type\": <norm_type>, **kwargs}\n",
    "              ```\n",
    "              where `norm_name` corresponds to normalization type (see above), and `kwargs` are passed directly to the normalization layer as defined in PyTorch documentation.\n",
    "\n",
    "            **Example**:\n",
    "            ```python\n",
    "            decoder_use_norm={\"type\": \"layernorm\", \"eps\": 1e-2}\n",
    "            ```\n",
    "        decoder_attention_type: Attention module used in decoder of the model.\n",
    "            Available options are **None** and **scse** (https://arxiv.org/abs/1808.08127).\n",
    "        decoder_interpolation: Interpolation mode used in decoder of the model. Available options are\n",
    "            **\"nearest\"**, **\"bilinear\"**, **\"bicubic\"**, **\"area\"**, **\"nearest-exact\"**. Default is **\"nearest\"**.\n",
    "        in_channels: A number of input channels for the model, default is 3 (RGB images)\n",
    "        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n",
    "        activation: An activation function to apply after the final convolution layer.\n",
    "            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**,\n",
    "            **callable** and **None**. Default is **None**.\n",
    "        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build\n",
    "            on top of encoder if **aux_params** is not **None** (default). Supported params:\n",
    "                - classes (int): A number of classes\n",
    "                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n",
    "                - dropout (float): Dropout factor in [0, 1)\n",
    "                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\"\n",
    "                    (could be **None** to return logits)\n",
    "        kwargs: Arguments passed to the encoder class ``__init__()`` function. Applies only to ``timm`` models. Keys with ``None`` values are pruned before passing.\n",
    "\n",
    "    Returns:\n",
    "        ``torch.nn.Module``: **Unet++**\n",
    "\n",
    "    Reference:\n",
    "        https://arxiv.org/abs/1807.10165\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    _is_torch_scriptable = False\n",
    "\n",
    "    @supports_config_loading\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_name: str = \"resnet34\",\n",
    "        encoder_depth: int = 5,\n",
    "        encoder_weights: Optional[str] = \"imagenet\",\n",
    "        decoder_use_norm: Union[bool, str, Dict[str, Any]] = \"batchnorm\",\n",
    "        decoder_channels: Sequence[int] = (256, 128, 64, 32, 16),\n",
    "        decoder_attention_type: Optional[str] = None,\n",
    "        decoder_interpolation: str = \"nearest\",\n",
    "        in_channels: int = 3,\n",
    "        classes: int = 1,\n",
    "        activation: Optional[Union[str, Callable]] = None,\n",
    "        aux_params: Optional[dict] = None,\n",
    "        **kwargs: dict[str, Any],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if encoder_name.startswith(\"mit_b\"):\n",
    "            raise ValueError(\n",
    "                \"UnetPlusPlus is not support encoder_name={}\".format(encoder_name)\n",
    "            )\n",
    "\n",
    "        decoder_use_batchnorm = kwargs.pop(\"decoder_use_batchnorm\", None)\n",
    "        if decoder_use_batchnorm is not None:\n",
    "            warnings.warn(\n",
    "                \"The usage of decoder_use_batchnorm is deprecated. Please modify your code for decoder_use_norm\",\n",
    "                DeprecationWarning,\n",
    "                stacklevel=2,\n",
    "            )\n",
    "            decoder_use_norm = decoder_use_batchnorm\n",
    "\n",
    "        self.encoder = get_encoder(\n",
    "            encoder_name,\n",
    "            in_channels=in_channels,\n",
    "            depth=encoder_depth,\n",
    "            weights=encoder_weights,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.decoder = UnetPlusPlusDecoderMod(\n",
    "            encoder_channels=self.encoder.out_channels,\n",
    "            decoder_channels=decoder_channels,\n",
    "            n_blocks=encoder_depth,\n",
    "            use_norm=decoder_use_norm,\n",
    "            center=True if encoder_name.startswith(\"vgg\") else False,\n",
    "            attention_type=decoder_attention_type,\n",
    "            interpolation_mode=decoder_interpolation,\n",
    "        )\n",
    "\n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=decoder_channels[-1],\n",
    "            out_channels=classes,\n",
    "            activation=activation,\n",
    "            kernel_size=3,\n",
    "        )\n",
    "\n",
    "        if aux_params is not None:\n",
    "            self.classification_head = ClassificationHead(\n",
    "                in_channels=self.encoder.out_channels[-1], **aux_params\n",
    "            )\n",
    "        else:\n",
    "            self.classification_head = None\n",
    "\n",
    "        self.name = \"unetplusplus-{}\".format(encoder_name)\n",
    "        self.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7444b3b3",
   "metadata": {},
   "source": [
    "# Адаптация MANet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c55c40b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MFABBlockMod(smp.decoders.manet.decoder.MFABBlock):\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, skip: Optional[torch.Tensor] = None, scale_factor=2.0,\n",
    "    ) -> torch.Tensor:\n",
    "        x = self.hl_conv(x)\n",
    "        x = F.interpolate(x, scale_factor=scale_factor, mode=self.interpolation_mode)\n",
    "        attention_hl = self.SE_hl(x)\n",
    "        if skip is not None:\n",
    "            attention_ll = self.SE_ll(skip)\n",
    "            attention_hl = attention_hl + attention_ll\n",
    "            x = x * attention_hl\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "class DecoderBlockMod(smp.decoders.manet.decoder.DecoderBlock):\n",
    "    def forward(\n",
    "        self, x: torch.Tensor, skip: Optional[torch.Tensor] = None, scale_factor=2.0,\n",
    "    ) -> torch.Tensor:\n",
    "        x = F.interpolate(x, scale_factor=scale_factor, mode=self.interpolation_mode)\n",
    "        if skip is not None:\n",
    "            x = torch.cat([x, skip], dim=1)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        return x\n",
    "\n",
    "class MAnetDecoderMod(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_channels: List[int],\n",
    "        decoder_channels: List[int],\n",
    "        n_blocks: int = 5,\n",
    "        reduction: int = 16,\n",
    "        use_norm: Union[bool, str, Dict[str, Any]] = \"batchnorm\",\n",
    "        pab_channels: int = 64,\n",
    "        interpolation_mode: str = \"nearest\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if n_blocks != len(decoder_channels):\n",
    "            raise ValueError(\n",
    "                \"Model depth is {}, but you provide `decoder_channels` for {} blocks.\".format(\n",
    "                    n_blocks, len(decoder_channels)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # remove first skip with same spatial resolution\n",
    "        encoder_channels = encoder_channels[1:]\n",
    "\n",
    "        # reverse channels to start from head of encoder\n",
    "        encoder_channels = encoder_channels[::-1]\n",
    "\n",
    "        # computing blocks input and output channels\n",
    "        head_channels = encoder_channels[0]\n",
    "        in_channels = [head_channels] + list(decoder_channels[:-1])\n",
    "        skip_channels = list(encoder_channels[1:]) + [0]\n",
    "        out_channels = decoder_channels\n",
    "\n",
    "        self.center = smp.decoders.manet.decoder.PABBlock(head_channels, pab_channels=pab_channels)\n",
    "\n",
    "        # combine decoder keyword arguments\n",
    "        kwargs = dict(\n",
    "            use_norm=use_norm, interpolation_mode=interpolation_mode\n",
    "        )  # no attention type here\n",
    "        blocks = [\n",
    "            MFABBlockMod(in_ch, skip_ch, out_ch, reduction=reduction, **kwargs)\n",
    "            if skip_ch > 0\n",
    "            else DecoderBlockMod(in_ch, skip_ch, out_ch, **kwargs)\n",
    "            for in_ch, skip_ch, out_ch in zip(in_channels, skip_channels, out_channels)\n",
    "        ]\n",
    "        # for the last we dont have skip connection -> use simple decoder block\n",
    "        self.blocks = nn.ModuleList(blocks)\n",
    "\n",
    "    def forward(self, features: List[torch.Tensor]) -> torch.Tensor:\n",
    "        bs, channels, img_rows, img_cols = features[0].shape\n",
    "        _, _, feat1_rows, feat1_cols = features[1].shape\n",
    "        upsample_scale_factors_list = [2.0 for i in range(len(self.blocks))]\n",
    "        if (img_rows, img_cols) == (feat1_rows, feat1_cols):\n",
    "            upsample_scale_factors_list[-1] = 1.0\n",
    "\n",
    "        features = features[1:]  # remove first skip with same spatial resolution\n",
    "        features = features[::-1]  # reverse channels to start from head of encoder\n",
    "\n",
    "        head = features[0]\n",
    "        skips = features[1:]\n",
    "\n",
    "\n",
    "        x = self.center(head)\n",
    "        for i, (decoder_block, scale_factor) in enumerate(zip(self.blocks, upsample_scale_factors_list)):\n",
    "            skip = skips[i] if i < len(skips) else None\n",
    "            x = decoder_block(x, skip, scale_factor)\n",
    "\n",
    "        return x\n",
    "    \n",
    "class MAnetMod(SegmentationModel):\n",
    "    \"\"\"MAnet_ :  Multi-scale Attention Net. The MA-Net can capture rich contextual dependencies based on\n",
    "    the attention mechanism, using two blocks:\n",
    "     - Position-wise Attention Block (PAB), which captures the spatial dependencies between pixels in a global view\n",
    "     - Multi-scale Fusion Attention Block (MFAB), which  captures the channel dependencies between any feature map by\n",
    "       multi-scale semantic feature fusion\n",
    "\n",
    "    Args:\n",
    "        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n",
    "            to extract features of different spatial resolution\n",
    "        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features\n",
    "            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n",
    "            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n",
    "            Default is 5\n",
    "        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and\n",
    "            other pretrained weights (see table with available weights for each encoder_name)\n",
    "        decoder_channels: List of integers which specify **in_channels** parameter for convolutions used in decoder.\n",
    "            Length of the list should be the same as **encoder_depth**\n",
    "        decoder_use_norm: Specifies normalization between Conv2D and activation.\n",
    "            Accepts the following types:\n",
    "            - **True**: Defaults to `\"batchnorm\"`.\n",
    "            - **False**: No normalization (`nn.Identity`).\n",
    "            - **str**: Specifies normalization type using default parameters. Available values:\n",
    "              `\"batchnorm\"`, `\"identity\"`, `\"layernorm\"`, `\"instancenorm\"`, `\"inplace\"`.\n",
    "            - **dict**: Fully customizable normalization settings. Structure:\n",
    "              ```python\n",
    "              {\"type\": <norm_type>, **kwargs}\n",
    "              ```\n",
    "              where `norm_name` corresponds to normalization type (see above), and `kwargs` are passed directly to the normalization layer as defined in PyTorch documentation.\n",
    "\n",
    "            **Example**:\n",
    "            ```python\n",
    "            decoder_use_norm={\"type\": \"layernorm\", \"eps\": 1e-2}\n",
    "            ```\n",
    "        decoder_pab_channels: A number of channels for PAB module in decoder.\n",
    "            Default is 64.\n",
    "        decoder_interpolation: Interpolation mode used in decoder of the model. Available options are\n",
    "            **\"nearest\"**, **\"bilinear\"**, **\"bicubic\"**, **\"area\"**, **\"nearest-exact\"**. Default is **\"nearest\"**.\n",
    "        in_channels: A number of input channels for the model, default is 3 (RGB images)\n",
    "        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n",
    "        activation: An activation function to apply after the final convolution layer.\n",
    "            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**,\n",
    "            **callable** and **None**. Default is **None**.\n",
    "        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build\n",
    "            on top of encoder if **aux_params** is not **None** (default). Supported params:\n",
    "                - classes (int): A number of classes\n",
    "                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n",
    "                - dropout (float): Dropout factor in [0, 1)\n",
    "                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\"\n",
    "                    (could be **None** to return logits)\n",
    "        kwargs: Arguments passed to the encoder class ``__init__()`` function. Applies only to ``timm`` models. Keys with ``None`` values are pruned before passing.\n",
    "\n",
    "    Returns:\n",
    "        ``torch.nn.Module``: **MAnet**\n",
    "\n",
    "    .. _MAnet:\n",
    "        https://ieeexplore.ieee.org/abstract/document/9201310\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    @supports_config_loading\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_name: str = \"resnet34\",\n",
    "        encoder_depth: int = 5,\n",
    "        encoder_weights: Optional[str] = \"imagenet\",\n",
    "        decoder_use_norm: Union[bool, str, Dict[str, Any]] = \"batchnorm\",\n",
    "        decoder_channels: Sequence[int] = (256, 128, 64, 32, 16),\n",
    "        decoder_pab_channels: int = 64,\n",
    "        decoder_interpolation: str = \"nearest\",\n",
    "        in_channels: int = 3,\n",
    "        classes: int = 1,\n",
    "        activation: Optional[Union[str, Callable]] = None,\n",
    "        aux_params: Optional[dict] = None,\n",
    "        **kwargs: dict[str, Any],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        decoder_use_batchnorm = kwargs.pop(\"decoder_use_batchnorm\", None)\n",
    "        if decoder_use_batchnorm is not None:\n",
    "            warnings.warn(\n",
    "                \"The usage of decoder_use_batchnorm is deprecated. Please modify your code for decoder_use_norm\",\n",
    "                DeprecationWarning,\n",
    "                stacklevel=2,\n",
    "            )\n",
    "            decoder_use_norm = decoder_use_batchnorm\n",
    "\n",
    "        self.encoder = get_encoder(\n",
    "            encoder_name,\n",
    "            in_channels=in_channels,\n",
    "            depth=encoder_depth,\n",
    "            weights=encoder_weights,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.decoder = MAnetDecoderMod(\n",
    "            encoder_channels=self.encoder.out_channels,\n",
    "            decoder_channels=decoder_channels,\n",
    "            n_blocks=encoder_depth,\n",
    "            use_norm=decoder_use_norm,\n",
    "            pab_channels=decoder_pab_channels,\n",
    "            interpolation_mode=decoder_interpolation,\n",
    "        )\n",
    "\n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=decoder_channels[-1],\n",
    "            out_channels=classes,\n",
    "            activation=activation,\n",
    "            kernel_size=3,\n",
    "        )\n",
    "\n",
    "        if aux_params is not None:\n",
    "            self.classification_head = ClassificationHead(\n",
    "                in_channels=self.encoder.out_channels[-1], **aux_params\n",
    "            )\n",
    "        else:\n",
    "            self.classification_head = None\n",
    "\n",
    "        self.name = \"manet-{}\".format(encoder_name)\n",
    "        self.initialize()\n",
    "\n",
    "\n",
    "#model = MAnetMod()\n",
    "#model.encoder.conv1.stride = 1\n",
    "#ret = model(torch.randn(1, 3, 96, 96))\n",
    "#ret.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "200468db",
   "metadata": {},
   "source": [
    "# Адаптация FPN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "617f9a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FPNMod(SegmentationModel):\n",
    "    \"\"\"FPN_ is a fully convolution neural network for image semantic segmentation.\n",
    "\n",
    "    Args:\n",
    "        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n",
    "            to extract features of different spatial resolution\n",
    "        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features\n",
    "            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n",
    "            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n",
    "            Default is 5\n",
    "        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and\n",
    "            other pretrained weights (see table with available weights for each encoder_name)\n",
    "        decoder_pyramid_channels: A number of convolution filters in Feature Pyramid of FPN_\n",
    "        decoder_segmentation_channels: A number of convolution filters in segmentation blocks of FPN_\n",
    "        decoder_merge_policy: Determines how to merge pyramid features inside FPN. Available options are **add**\n",
    "            and **cat**\n",
    "        decoder_dropout: Spatial dropout rate in range (0, 1) for feature pyramid in FPN_\n",
    "        decoder_interpolation: Interpolation mode used in decoder of the model. Available options are\n",
    "            **\"nearest\"**, **\"bilinear\"**, **\"bicubic\"**, **\"area\"**, **\"nearest-exact\"**. Default is **\"nearest\"**.\n",
    "        in_channels: A number of input channels for the model, default is 3 (RGB images)\n",
    "        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n",
    "        activation: An activation function to apply after the final convolution layer.\n",
    "            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**,\n",
    "            **callable** and **None**. Default is **None**.\n",
    "        upsampling: Final upsampling factor. Default is 4 to preserve input-output spatial shape identity\n",
    "        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build\n",
    "            on top of encoder if **aux_params** is not **None** (default). Supported params:\n",
    "                - classes (int): A number of classes\n",
    "                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n",
    "                - dropout (float): Dropout factor in [0, 1)\n",
    "                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\"\n",
    "                    (could be **None** to return logits)\n",
    "        kwargs: Arguments passed to the encoder class ``__init__()`` function. Applies only to ``timm`` models. Keys with ``None`` values are pruned before passing.\n",
    "\n",
    "    Returns:\n",
    "        ``torch.nn.Module``: **FPN**\n",
    "\n",
    "    .. _FPN:\n",
    "        http://presentations.cocodataset.org/COCO17-Stuff-FAIR.pdf\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    @supports_config_loading\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_name: str = \"resnet34\",\n",
    "        encoder_depth: int = 5,\n",
    "        encoder_weights: Optional[str] = \"imagenet\",\n",
    "        encoder_type:str = 'conv',\n",
    "        decoder_pyramid_channels: int = 256,\n",
    "        decoder_segmentation_channels: int = 128,\n",
    "        decoder_merge_policy: str = \"add\",\n",
    "        decoder_dropout: float = 0.2,\n",
    "        decoder_interpolation: str = \"nearest\",\n",
    "        in_channels: int = 3,\n",
    "        classes: int = 1,\n",
    "        activation: Optional[str] = None,\n",
    "        upsampling: int = 4,\n",
    "        aux_params: Optional[dict] = None,\n",
    "        **kwargs: dict[str, Any],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        # validate input params\n",
    "        if encoder_name.startswith(\"mit_b\") and encoder_depth != 5:\n",
    "            raise ValueError(\n",
    "                \"Encoder {} support only encoder_depth=5\".format(encoder_name)\n",
    "            )\n",
    "\n",
    "        self.encoder = get_encoder(\n",
    "            encoder_name,\n",
    "            in_channels=in_channels,\n",
    "            depth=encoder_depth,\n",
    "            weights=encoder_weights,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        self.decoder = FPNDecoderMod(\n",
    "            encoder_channels=self.encoder.out_channels,\n",
    "            encoder_depth=encoder_depth,\n",
    "            pyramid_channels=decoder_pyramid_channels,\n",
    "            segmentation_channels=decoder_segmentation_channels,\n",
    "            dropout=decoder_dropout,\n",
    "            merge_policy=decoder_merge_policy,\n",
    "            interpolation_mode=decoder_interpolation,\n",
    "            encoder_type=encoder_type,\n",
    "        )\n",
    "\n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=self.decoder.out_channels,\n",
    "            out_channels=classes,\n",
    "            activation=activation,\n",
    "            kernel_size=1,\n",
    "            upsampling=upsampling,\n",
    "        )\n",
    "\n",
    "        if aux_params is not None:\n",
    "            self.classification_head = ClassificationHead(\n",
    "                in_channels=self.encoder.out_channels[-1], **aux_params\n",
    "            )\n",
    "        else:\n",
    "            self.classification_head = None\n",
    "\n",
    "        self.name = \"fpn-{}\".format(encoder_name)\n",
    "        self.initialize()\n",
    "\n",
    "class FPNModBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        pyramid_channels: int,\n",
    "        skip_channels: int,\n",
    "        interpolation_mode: str = \"nearest\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.skip_conv = nn.Conv2d(skip_channels, pyramid_channels, kernel_size=1)\n",
    "        self.interpolation_mode = interpolation_mode\n",
    "\n",
    "    def forward(self, x: torch.Tensor, skip: torch.Tensor, scale_factor: float) -> torch.Tensor:\n",
    "        x = F.interpolate(x, scale_factor=scale_factor, mode=self.interpolation_mode)\n",
    "        if skip.size(1) != 0:\n",
    "            #print(x.shape, skip.shape)\n",
    "            skip = self.skip_conv(skip)\n",
    "            x = x + skip\n",
    "        return x\n",
    "\n",
    "class FPNDecoderMod(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_channels: List[int],\n",
    "        encoder_depth: int = 5,\n",
    "        pyramid_channels: int = 256,\n",
    "        segmentation_channels: int = 128,\n",
    "        dropout: float = 0.2,\n",
    "        merge_policy: Literal[\"add\", \"cat\"] = \"add\",\n",
    "        interpolation_mode: str = \"nearest\",\n",
    "        encoder_type:str = 'conv',\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.out_channels = (\n",
    "            segmentation_channels\n",
    "            if merge_policy == \"add\"\n",
    "            else segmentation_channels * 4\n",
    "        )\n",
    "        #print(self.out_channels)\n",
    "        if encoder_depth < 3:\n",
    "            raise ValueError(\n",
    "                \"Encoder depth for FPN decoder cannot be less than 3, got {}.\".format(\n",
    "                    encoder_depth\n",
    "                )\n",
    "            )\n",
    "\n",
    "        encoder_channels = encoder_channels[::-1]\n",
    "        encoder_channels = encoder_channels[: encoder_depth + 1]\n",
    "        \n",
    "        self.p6 = nn.Conv2d(encoder_channels[0], pyramid_channels, kernel_size=1)\n",
    "        '''\n",
    "        self.p5 = smp.decoders.fpn.decoder.FPNBlock(pyramid_channels, encoder_channels[1], interpolation_mode)\n",
    "        self.p4 = smp.decoders.fpn.decoder.FPNBlock(pyramid_channels, encoder_channels[2], interpolation_mode)\n",
    "        self.p3 = smp.decoders.fpn.decoder.FPNBlock(pyramid_channels, encoder_channels[3], interpolation_mode)\n",
    "        self.p2 = smp.decoders.fpn.decoder.FPNBlock(pyramid_channels, encoder_channels[4], interpolation_mode)\n",
    "        '''\n",
    "        self.p5 = FPNModBlock(pyramid_channels, encoder_channels[1], interpolation_mode)\n",
    "        self.p4 = FPNModBlock(pyramid_channels, encoder_channels[2], interpolation_mode)\n",
    "        self.p3 = FPNModBlock(pyramid_channels, encoder_channels[3], interpolation_mode)\n",
    "        self.p2 = FPNModBlock(pyramid_channels, encoder_channels[4], interpolation_mode)\n",
    "        \n",
    "        if encoder_type == 'conv':\n",
    "            upsamples_list = [4, 3, 2, 1, 0]\n",
    "        elif encoder_type == 'vit':\n",
    "            upsamples_list = [3, 2, 1, 0, 0]\n",
    "\n",
    "\n",
    "        self.seg_blocks = nn.ModuleList(\n",
    "            [\n",
    "                smp.decoders.fpn.decoder.SegmentationBlock(\n",
    "                    pyramid_channels, segmentation_channels, n_upsamples=n_upsamples\n",
    "                )\n",
    "                for n_upsamples in upsamples_list\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        self.merge = smp.decoders.fpn.decoder.MergeBlock(merge_policy)\n",
    "        self.dropout = nn.Dropout2d(p=dropout, inplace=True)\n",
    "\n",
    "    def forward(self, features: List[torch.Tensor]) -> torch.Tensor:\n",
    "        c2, c3, c4, c5, c6 = features[-5:]\n",
    "\n",
    "        #print([c2.shape, c3.shape, c4.shape, c5.shape, c6.shape])\n",
    "        #print([f.shape for f in features])\n",
    "        #print()\n",
    "        #print(f'c6:{c6.shape}')\n",
    "        p6 = self.p6(c6)\n",
    "        #print(f'p6:{p6.shape};c5:{c5.shape}')\n",
    "        p5 = self.p5(p6, c5, scale_factor=2.0)\n",
    "        #print(f'p5:{p5.shape};c4:{c4.shape}')\n",
    "        p4 = self.p4(p5, c4, scale_factor=2.0)\n",
    "        #print(f'p4:{p4.shape};c3:{c3.shape}')\n",
    "        p3 = self.p3(p4, c3, scale_factor=2.0)\n",
    "        #print(f'p3:{p3.shape};c2:{c2.shape}')\n",
    "        p2 = self.p2(p3, c2, scale_factor=2.0)\n",
    "        #print(f'p2:{p4.shape}')\n",
    "\n",
    "        s6 = self.seg_blocks[0](p6)\n",
    "        s5 = self.seg_blocks[1](p5)\n",
    "        s4 = self.seg_blocks[2](p4)\n",
    "        s3 = self.seg_blocks[3](p3)\n",
    "        s2 = self.seg_blocks[4](p2)\n",
    "\n",
    "        feature_pyramid = [s6, s5, s4, s3, s2]\n",
    "\n",
    "        #print([f.shape for f in feature_pyramid])\n",
    "\n",
    "        x = self.merge(feature_pyramid)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "#model = FPNMod(encoder_name='mit_b0', upsampling=0, encoder_type='vit', image_size=(96, 96))\n",
    "#model.encoder.patch_embed1.proj.stride=1\n",
    "\n",
    "#model = FPNMod(encoder_name='resnet34', upsampling=0)\n",
    "#model.encoder.conv1.stride=(1,1)\n",
    "\n",
    "#ret = model(torch.randn(1, 3, 96, 96))\n",
    "#ret.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eaaf945c",
   "metadata": {},
   "source": [
    "# UNet with attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b444c9f1",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7003e5d4",
   "metadata": {},
   "source": [
    "## Attention and supplementary modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30a46ca9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 1, 128, 64])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / (d_k ** 0.5)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)  # Masking\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    return output, attention_weights\n",
    "\n",
    "class ChannelMultiHeadAttentionExpandHeads(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, kdim, vdim):\n",
    "        '''\n",
    "        This layer expands query, key, value vectors to dim*num_heads using Q, K, V linear layers\n",
    "        input linear layers\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.qdim = embed_dim\n",
    "        self.kdim = kdim\n",
    "        self.vdim = vdim\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        self.qhead_dim = embed_dim\n",
    "        self.khead_dim = kdim\n",
    "        self.vhead_dim = vdim\n",
    "        #assert self.head_dim * num_heads == embed_dim, \"embed_dim must be divisible by num_heads\"\n",
    "\n",
    "        self.wq = nn.Linear(embed_dim, embed_dim * num_heads)\n",
    "        self.wk = nn.Linear(kdim, kdim * num_heads)\n",
    "        self.wv = nn.Linear(vdim, vdim * num_heads)\n",
    "        self.wo = nn.Linear(embed_dim * num_heads, embed_dim)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None, need_weights=False, **kwargs):\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 1. Linear projections for Q, K, V\n",
    "        q = self.wq(query)\n",
    "        k = self.wk(key)\n",
    "        v = self.wv(value)\n",
    "\n",
    "        #print(q.shape, k.shape, v.shape)\n",
    "\n",
    "        # 2. Split into multiple heads\n",
    "        q = q.view(batch_size, -1, self.num_heads, self.qhead_dim).transpose(1, 2)\n",
    "        k = k.view(batch_size, -1, self.num_heads, self.khead_dim).transpose(1, 2)\n",
    "        v = v.view(batch_size, -1, self.num_heads, self.vhead_dim).transpose(1, 2)\n",
    "\n",
    "        #print(q.shape, k.shape, v.shape)\n",
    "\n",
    "        # 3. Apply scaled dot-product attention\n",
    "        if need_weights:\n",
    "            x, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        else:\n",
    "            x = F.scaled_dot_product_attention(q, k, v, mask)\n",
    "            attention_weights = None\n",
    "\n",
    "        #print(x.shape)\n",
    "        # 4. Concatenate heads and apply final linear projection\n",
    "        x = x.transpose(1, 2).contiguous().view(batch_size, -1, self.qdim * self.num_heads)\n",
    "        #print(x.shape)\n",
    "        output = self.wo(x)\n",
    "        \n",
    "        return output, attention_weights\n",
    "   \n",
    "bs = 1\n",
    "ch = 200\n",
    "rows = 96\n",
    "cols=96\n",
    "\n",
    "rp = 8\n",
    "cp = 8\n",
    "\n",
    "rpn = rows//rp\n",
    "cpn = cols//cp\n",
    "\n",
    "hsi = torch.randn(bs, ch, rows, cols)\n",
    "another_hsi = torch.randn(bs, 128, 16, 16)\n",
    "\n",
    "ch_att = ChannelMultiHeadAttentionExpandHeads(embed_dim=rp*cp, kdim=rp*cp, vdim=rp*cp, num_heads=4)\n",
    "\n",
    "axes1 = {\n",
    "    'bs': bs,\n",
    "    'rpn': rpn,\n",
    "    'cpn': cpn,\n",
    "    'rp': rp,\n",
    "    'cp': cp\n",
    "\n",
    "}\n",
    "axes2 = {\n",
    "    'bs': bs,\n",
    "    'rpn': 16//rp,\n",
    "    'cpn': 16//cp,\n",
    "    'rp': rp,\n",
    "    'cp': cp\n",
    "}\n",
    "rearr_rule = 'bs ch (rp rpn) (cp cpn) -> (rpn cpn) bs ch (rp cp)'\n",
    "hsi = eo.rearrange(hsi, rearr_rule, **axes1)\n",
    "another_hsi = eo.rearrange(another_hsi, rearr_rule, **axes2)\n",
    "another_hsi.shape\n",
    "\n",
    "#ret = ch_att(query=hsi[0], key=hsi[0], value=hsi[0])\n",
    "#ret.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "51897d98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([1, 200, 96, 96]), torch.Size([1, 30, 96, 96]))"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class VisionTransformerBlock(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_heads: int,\n",
    "            hidden_dim: int, # is equal to query_dim\n",
    "            kdim: int,\n",
    "            vdim: int,\n",
    "            mlp_dim: int,\n",
    "            attention_layer: Callable[..., torch.nn.Module],\n",
    "            dropout:float,\n",
    "            norm_layer: Callable[..., torch.nn.Module] = partial(nn.LayerNorm, eps=1e-6),\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        assert kdim==vdim, f\"kdim should be equal to vdim! kdim={kdim}, vdim={vdim}\"\n",
    "        \n",
    "        self.num_heads = num_heads\n",
    "        self.kdim = kdim\n",
    "        self.vdim = vdim\n",
    "        self.hidden_dim = hidden_dim\n",
    "\n",
    "        # Attention block\n",
    "        self.ln_11 = norm_layer(hidden_dim)\n",
    "        self.ln_12 = norm_layer(kdim)\n",
    "\n",
    "        if attention_layer is nn.MultiheadAttention:\n",
    "            \n",
    "            attention_layer = partial(attention_layer, batch_first=True)\n",
    "        self.self_attention = attention_layer(hidden_dim, num_heads, kdim=kdim, vdim=vdim,)\n",
    "        #if kdim==vdim==hidden_dim:    \n",
    "        #else:self.self_attention = attention_layer(hidden_dim, num_heads,)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "        # MLP block\n",
    "        self.ln_2 = norm_layer(hidden_dim)\n",
    "        self.mlp = MLPBlock(hidden_dim, mlp_dim, dropout)\n",
    "\n",
    "    def forward(self, target, source=None, need_weghts=False):\n",
    "        x = self.ln_11(target)\n",
    "        \n",
    "        if source is None:\n",
    "            # Self Attention\n",
    "            x, weights = self.self_attention(query=x, key=x, value=x, average_attn_weights=False, need_weights=need_weghts)\n",
    "\n",
    "        else:\n",
    "            y = self.ln_12(source)\n",
    "            #print(x.shape, y.shape)\n",
    "            # Cross Attention\n",
    "            x, weights = self.self_attention(query=x, key=y, value=y, average_attn_weights=False, need_weights=need_weghts)\n",
    "        #print(x.shape)\n",
    "        x = self.dropout(x)\n",
    "        x = x + target\n",
    "\n",
    "        x2 = self.ln_2(x)\n",
    "        x2 = self.mlp(x2)\n",
    "        return x2 + x, weights\n",
    "\n",
    "class WindowVisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            cols_in_patch:int,\n",
    "            rows_in_patch:int,\n",
    "            channels:int,\n",
    "            # transformers block params\n",
    "            num_heads:int,\n",
    "            mlp_dim:int,\n",
    "            dropout:float,\n",
    "            layer_num:int,\n",
    "            transformer_type:'str', # channels and patches are possible\n",
    "            positional_encoding: Callable[..., nn.Module] = nn.Identity,\n",
    "            ):\n",
    "    \n",
    "        super().__init__()\n",
    "\n",
    "        self.cols_in_patch = cols_in_patch\n",
    "        self.rows_in_patch = rows_in_patch\n",
    "        self.transformer_type =  transformer_type\n",
    "        \n",
    "        if transformer_type == 'channels':\n",
    "            self.seq_len = channels\n",
    "            hidden_dim = cols_in_patch * rows_in_patch\n",
    "            \n",
    "        elif transformer_type == 'patches':\n",
    "            self.seq_len = cols_in_patch * rows_in_patch\n",
    "            hidden_dim = channels\n",
    "\n",
    "        #print(self.seq_len, hidden_dim)\n",
    "        \n",
    "        self.positional_encoding = positional_encoding(num_embeddings=self.seq_len, embedding_dim=hidden_dim)\n",
    "        # можно создать несколько трансформерных слоев\n",
    "        transformer_layers_list = [\n",
    "            VisionTransformerBlock(\n",
    "                num_heads=num_heads,\n",
    "                hidden_dim=hidden_dim,\n",
    "                kdim=hidden_dim,\n",
    "                vdim=hidden_dim,\n",
    "                mlp_dim=mlp_dim,\n",
    "                attention_layer=nn.MultiheadAttention,\n",
    "                dropout=dropout)\n",
    "            for i in range(layer_num)\n",
    "        ]\n",
    "        self.transformer_layers = nn.ModuleList(transformer_layers_list)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        bs, channels, rows, cols = x.shape\n",
    "        row_patch_num = rows//self.rows_in_patch\n",
    "        col_patch_num = cols//self.cols_in_patch\n",
    "        # размер (bs, channels, rows, cols) преобразовываем в размер (row_patches*col_patches, bs, channels, rows_in_patch*cols_in_patch)\n",
    "        # rows=row_patches*rows_in_patch, cols=col_patches*cols_in_patch\n",
    "        if self.transformer_type == 'channels':\n",
    "            rearrange_pattern = 'bs channels (row_patch_num rows_in_patch) (col_patch_num cols_in_patch) -> (row_patch_num col_patch_num) bs channels (rows_in_patch cols_in_patch)'\n",
    "            rearrange_args = {\n",
    "                'cols_in_patch':self.cols_in_patch,\n",
    "                'rows_in_patch':self.rows_in_patch,\n",
    "            }\n",
    "            \n",
    "        elif self.transformer_type == 'patches':\n",
    "            rearrange_pattern = 'bs channels (row_patch_num rows_in_patch) (col_patch_num cols_in_patch) -> (row_patch_num col_patch_num) bs (rows_in_patch cols_in_patch) channels'\n",
    "            rearrange_args = {\n",
    "                'cols_in_patch':self.cols_in_patch,\n",
    "                'rows_in_patch':self.rows_in_patch,\n",
    "            }\n",
    "        \n",
    "        h = eo.rearrange(\n",
    "            x,\n",
    "            rearrange_pattern,\n",
    "            **rearrange_args,\n",
    "        )\n",
    "        # позиционное кодирование (его может и не быть - nn.Identity)\n",
    "        layer_outs = self.positional_encoding(h)\n",
    "        processed_outs = []\n",
    "\n",
    "        # итерирование по окнам \n",
    "        for i, layer_out in enumerate(layer_outs):\n",
    "\n",
    "            for layer in self.transformer_layers:\n",
    "\n",
    "                layer_out, layer_att_weights = layer(layer_out)\n",
    "            processed_outs.append(layer_out.unsqueeze(0))\n",
    "        \n",
    "        processed_outs = torch.cat(processed_outs,dim=0)\n",
    "        if self.transformer_type == 'channels':\n",
    "            rearrange_pattern = '(row_patch_num col_patch_num) bs channels (rows_in_patch cols_in_patch) -> bs channels (row_patch_num rows_in_patch) (col_patch_num cols_in_patch)'\n",
    "            rearrange_args = {\n",
    "                'cols_in_patch':self.cols_in_patch,\n",
    "                'rows_in_patch':self.rows_in_patch,\n",
    "                'row_patch_num':row_patch_num,\n",
    "                'col_patch_num':col_patch_num,\n",
    "            }\n",
    "        elif self.transformer_type == 'patches':\n",
    "            rearrange_pattern = '(row_patch_num col_patch_num) bs (rows_in_patch cols_in_patch) channels -> bs channels (row_patch_num rows_in_patch) (col_patch_num cols_in_patch)'\n",
    "            rearrange_args = {\n",
    "                'cols_in_patch':self.cols_in_patch,\n",
    "                'rows_in_patch':self.rows_in_patch,\n",
    "                'row_patch_num':row_patch_num,\n",
    "                'col_patch_num':col_patch_num,\n",
    "            }\n",
    "        processed_outs = eo.rearrange(\n",
    "            processed_outs,\n",
    "            rearrange_pattern,\n",
    "            **rearrange_args,\n",
    "        )\n",
    "        return processed_outs\n",
    "\n",
    "\n",
    "class WindowCrossAttention(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            cols_in_patch_x:int,\n",
    "            rows_in_patch_x:int,\n",
    "            cols_in_patch_y:int,\n",
    "            rows_in_patch_y:int,\n",
    "            channels_x:int,\n",
    "            channels_y:int,\n",
    "            # transformers block params\n",
    "            num_heads:int,\n",
    "            mlp_dim:int,\n",
    "            dropout:float,\n",
    "            transformer_type:'str', # channels and patches are possible\n",
    "            positional_encoding_x: Callable[..., nn.Module] = nn.Identity,\n",
    "            positional_encoding_y: Callable[..., nn.Module] = nn.Identity,\n",
    "            ):\n",
    "    \n",
    "        super().__init__()\n",
    "        self.transformer_type =  transformer_type\n",
    "        self.cols_in_patch_x = cols_in_patch_x\n",
    "        self.rows_in_patch_x = rows_in_patch_x\n",
    "\n",
    "        self.cols_in_patch_y = cols_in_patch_y\n",
    "        self.rows_in_patch_y = rows_in_patch_y\n",
    "        \n",
    "        \n",
    "        if transformer_type == 'channels':\n",
    "            self.seq_len_x = channels_x\n",
    "            hidden_dim_x = cols_in_patch_x * rows_in_patch_x\n",
    "            self.seq_len_y = channels_y\n",
    "            hidden_dim_y = cols_in_patch_y * rows_in_patch_y\n",
    "            \n",
    "        elif transformer_type == 'patches':\n",
    "            self.seq_len_x = cols_in_patch_x * rows_in_patch_x\n",
    "            hidden_dim_x = channels_x\n",
    "            self.seq_len_y = cols_in_patch_y * rows_in_patch_y\n",
    "            hidden_dim_y = channels_y\n",
    "\n",
    "        #print(self.seq_len, hidden_dim)\n",
    "        \n",
    "        self.positional_encoding_x = positional_encoding_x(num_embeddings=self.seq_len_x, embedding_dim=hidden_dim_x)\n",
    "        self.positional_encoding_y = positional_encoding_x(num_embeddings=self.seq_len_y, embedding_dim=hidden_dim_y)\n",
    "        # можно создать несколько трансформерных слоев\n",
    "        self.cross_attention_block = VisionTransformerBlock(\n",
    "            num_heads=num_heads,\n",
    "            hidden_dim=hidden_dim_x,\n",
    "            kdim=hidden_dim_y,\n",
    "            vdim=hidden_dim_y,\n",
    "            mlp_dim=mlp_dim,\n",
    "            attention_layer=nn.MultiheadAttention,\n",
    "            dropout=dropout)\n",
    "        \n",
    "    def forward(self, x, y):\n",
    "        bs, channels, rows_x, cols_x = x.shape\n",
    "        \n",
    "        row_patch_num_x = rows_x//self.rows_in_patch_x\n",
    "        col_patch_num_x = cols_x//self.cols_in_patch_x\n",
    "\n",
    "        bs, channels, rows_y, cols_y = y.shape\n",
    "        row_patch_num_y = rows_y//self.rows_in_patch_y\n",
    "        col_patch_num_y = cols_y//self.cols_in_patch_y\n",
    "\n",
    "        \n",
    "\n",
    "        assert row_patch_num_x == row_patch_num_y, f'number of window rows in X={row_patch_num_x} and Y={row_patch_num_y} tensors ought to coinside'\n",
    "        assert col_patch_num_x == col_patch_num_y, f'number of window cols in X={col_patch_num_x} and Y={col_patch_num_y} tensors ought to coinside'\n",
    "\n",
    "        # размер (bs, channels, rows, cols) преобразовываем в размер (row_patches*col_patches, bs, channels, rows_in_patch*cols_in_patch)\n",
    "        # rows=row_patches*rows_in_patch, cols=col_patches*cols_in_patch\n",
    "        if self.transformer_type == 'channels':\n",
    "            rearrange_pattern = 'bs channels (row_patch_num rows_in_patch) (col_patch_num cols_in_patch) -> (row_patch_num col_patch_num) bs channels (rows_in_patch cols_in_patch)'\n",
    "            rearrange_args_x = {\n",
    "                'cols_in_patch':self.cols_in_patch_x,\n",
    "                'rows_in_patch':self.rows_in_patch_x,\n",
    "            }\n",
    "\n",
    "            rearrange_args_y = {\n",
    "                'cols_in_patch':self.cols_in_patch_y,\n",
    "                'rows_in_patch':self.rows_in_patch_y,\n",
    "            }\n",
    "             \n",
    "        elif self.transformer_type == 'patches':\n",
    "            rearrange_pattern = 'bs channels (row_patch_num rows_in_patch) (col_patch_num cols_in_patch) -> (row_patch_num col_patch_num) bs (rows_in_patch cols_in_patch) channels'\n",
    "            rearrange_args_x = {\n",
    "                'cols_in_patch':self.cols_in_patch_x,\n",
    "                'rows_in_patch':self.rows_in_patch_x,\n",
    "            }\n",
    "            rearrange_args_y = {\n",
    "                'cols_in_patch':self.cols_in_patch_y,\n",
    "                'rows_in_patch':self.rows_in_patch_y,\n",
    "            }\n",
    "        \n",
    "        hx = eo.rearrange(\n",
    "            x,\n",
    "            rearrange_pattern,\n",
    "            **rearrange_args_x,\n",
    "        )\n",
    "        hy = eo.rearrange(\n",
    "            y,\n",
    "            rearrange_pattern,\n",
    "            **rearrange_args_y,\n",
    "        )\n",
    "\n",
    "        # позиционное кодирование (его может и не быть - nn.Identity)\n",
    "        \n",
    "        layer_outs_x = self.positional_encoding_x(hx)\n",
    "        layer_outs_y = self.positional_encoding_y(hy)\n",
    "        processed_outs = []\n",
    "\n",
    "        # итерирование по окнам \n",
    "        for i, (layer_out_x, layer_out_y) in enumerate(zip(layer_outs_x, layer_outs_y)):\n",
    "            layer_out, layer_att_weights = self.cross_attention_block(layer_out_x, layer_out_y)\n",
    "            processed_outs.append(layer_out.unsqueeze(0))\n",
    "        \n",
    "        processed_outs = torch.cat(processed_outs,dim=0)\n",
    "        if self.transformer_type == 'channels':\n",
    "            rearrange_pattern = '(row_patch_num col_patch_num) bs channels (rows_in_patch cols_in_patch) -> bs channels (row_patch_num rows_in_patch) (col_patch_num cols_in_patch)'\n",
    "            rearrange_args = {\n",
    "                'cols_in_patch':self.cols_in_patch_x,\n",
    "                'rows_in_patch':self.rows_in_patch_x,\n",
    "                'row_patch_num':row_patch_num_x,\n",
    "                'col_patch_num':col_patch_num_x,\n",
    "            }\n",
    "        elif self.transformer_type == 'patches':\n",
    "            rearrange_pattern = '(row_patch_num col_patch_num) bs (rows_in_patch cols_in_patch) channels -> bs channels (row_patch_num rows_in_patch) (col_patch_num cols_in_patch)'\n",
    "            rearrange_args = {\n",
    "                'cols_in_patch':self.cols_in_patch_x,\n",
    "                'rows_in_patch':self.rows_in_patch_x,\n",
    "                'row_patch_num':row_patch_num_x,\n",
    "                'col_patch_num':col_patch_num_x,\n",
    "            }\n",
    "        processed_outs = eo.rearrange(\n",
    "            processed_outs,\n",
    "            rearrange_pattern,\n",
    "            **rearrange_args,\n",
    "        )\n",
    "        return processed_outs\n",
    "    \n",
    "class HyperspectralTransformer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            config\n",
    "            ):\n",
    "    \n",
    "        super().__init__()\n",
    "        self.config = config\n",
    "        self.patch_emd = config['patch_emd']['layer'](**config['patch_emd']['params'])\n",
    "        transformer_layers = {}\n",
    "        for i, transformer_layer_config in enumerate(config['transformer_layers']):\n",
    "            #print(transformer_layer_config['params'])\n",
    "            if transformer_layer_config['layer'] == 'crossatt':\n",
    "                transformer_layer_config['params']['positional_encoding_x'] = pos_enc_factory_dict[transformer_layer_config['params']['positional_encoding_x']]\n",
    "                transformer_layer_config['params']['positional_encoding_y'] = pos_enc_factory_dict[transformer_layer_config['params']['positional_encoding_y']]\n",
    "            else:\n",
    "                transformer_layer_config['params']['positional_encoding'] = pos_enc_factory_dict[transformer_layer_config['params']['positional_encoding']]\n",
    "            layer_name = transformer_layer_config['layer']\n",
    "            layer_creat = transformer_factory_dict[layer_name]\n",
    "            transformer_layers[f'{i}_{layer_name}'] = layer_creat(**transformer_layer_config['params'])\n",
    "\n",
    "        self.transformer_layers = nn.ModuleDict(transformer_layers)\n",
    "        #self.output_layer = config['output_layer']['layer'](**config['patch_emd']['params'])\n",
    "\n",
    "    def forward(self, x, y: Dict[str, torch.Tensor]):\n",
    "        x = self.patch_emd(x)\n",
    "        results = []\n",
    "        for i, (name, layer) in enumerate(self.transformer_layers.items()):\n",
    "            if name in y:       \n",
    "                x = layer(x, y[name])\n",
    "            else:\n",
    "                x = layer(x)\n",
    "        return x\n",
    "\n",
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_embeddings,\n",
    "            embedding_dim,\n",
    "            padding_idx=None,\n",
    "            max_norm=None,\n",
    "            norm_type=2.0,\n",
    "            scale_grad_by_freq=False,\n",
    "            sparse=False,\n",
    "            _weight=None,\n",
    "            _freeze=False,\n",
    "            device=None,\n",
    "            dtype=None\n",
    "            ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings,\n",
    "            embedding_dim,\n",
    "            padding_idx,\n",
    "            max_norm,\n",
    "            norm_type,\n",
    "            scale_grad_by_freq,\n",
    "            sparse,\n",
    "            _weight,\n",
    "            _freeze,\n",
    "            device,\n",
    "            dtype\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        #!!!!\n",
    "        bs, seq_len, emb_dim = x.shape\n",
    "        positions = torch.arange(0, seq_len, dtype=torch.long).to(x.device)\n",
    "        pos_embeddings = self.embedding(positions)\n",
    "        return x + pos_embeddings\n",
    "    \n",
    "class FixedSizeLearnableEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_embeddings,\n",
    "            embedding_dim,\n",
    "            ):\n",
    "        super().__init__()\n",
    "        self.positional_encoding = nn.Parameter(torch.empty(1, num_embeddings, embedding_dim).normal_(std=0.02))\n",
    "\n",
    "    def forward(self,x):\n",
    "        return x + self.positional_encoding\n",
    "\n",
    "class AddFeatures(nn.Module):\n",
    "    def forward(self, x, y):\n",
    "        return x+y\n",
    "\n",
    "class ConcatFeatures(nn.Module):\n",
    "    '''\n",
    "    Implementation of concatenation. It is nececcary for various aggreagation strategies in UNet decoder\n",
    "    '''\n",
    "    def forward(self, *tensors, dim=1):\n",
    "        return torch.cat(tensors, dim=dim)\n",
    "    \n",
    "class PassOneFeature(nn.Module):\n",
    "    def __init__(self, passing_idx):\n",
    "        super().__init__()\n",
    "        self.passing_idx = passing_idx\n",
    "    def forward(self, *tensors):\n",
    "        return tensors[self.passing_idx]\n",
    "\n",
    "    \n",
    "\n",
    "pos_enc_factory_dict = {\n",
    "    'fixed_embeddings': FixedSizeLearnableEmbeddings,\n",
    "    'embedding_layer': EmbeddingLayer,\n",
    "    'none': nn.Identity\n",
    "}\n",
    "\n",
    "transformer_factory_dict = {\n",
    "    'win_mha': WindowVisionTransformer,\n",
    "    'crossatt': WindowCrossAttention,\n",
    "    'none': nn.Identity,\n",
    "}\n",
    "\n",
    "feature_aggregation_factory_dict ={\n",
    "    'add': AddFeatures,\n",
    "    'pass_one': PassOneFeature,\n",
    "    'concat': ConcatFeatures,\n",
    "    'crossatt': WindowCrossAttention\n",
    "}\n",
    "\n",
    "\n",
    "aux_transformer_config = {\n",
    "    'patch_emd':{\n",
    "        'layer':nn.Conv2d,\n",
    "        'params':{\n",
    "            'in_channels': 200,\n",
    "            'out_channels': 200,\n",
    "            'kernel_size': 3,\n",
    "            'stride': 1,\n",
    "            'padding': 1,\n",
    "            'groups':200\n",
    "        }\n",
    "    },\n",
    "    'input_transformer': {\n",
    "            'layer': 'win_mha',\n",
    "            'params': {\n",
    "                'cols_in_patch':12,\n",
    "                'rows_in_patch':12,\n",
    "                'channels':200,\n",
    "                'num_heads':12,\n",
    "                'mlp_dim':12*12*4,\n",
    "                'dropout':0.2,\n",
    "                'layer_num':3,\n",
    "                'transformer_type':'channels',\n",
    "                'positional_encoding':'fixed_embeddings',\n",
    "            },\n",
    "        },\n",
    "    'hsi_augmentation':{\n",
    "        'layer': 'add',\n",
    "        'params': {}\n",
    "    },\n",
    "    'intermediate_layers':{\n",
    "            'layer': 'win_mha',\n",
    "            'params': {\n",
    "                'cols_in_patch':12,\n",
    "                'rows_in_patch':12,\n",
    "                'channels':200,\n",
    "                'num_heads':12,\n",
    "                'mlp_dim':12*12*4,\n",
    "                'dropout':0.2,\n",
    "                'layer_num':3,\n",
    "                'transformer_type':'channels',\n",
    "                'positional_encoding':'none',\n",
    "            },\n",
    "        },\n",
    "\n",
    "    'output_crossatt': {\n",
    "            'layer': 'crossatt',\n",
    "            'params': {\n",
    "                'rows_in_patch_x':12,\n",
    "                'cols_in_patch_x':12,\n",
    "                'rows_in_patch_y':12,\n",
    "                'cols_in_patch_y':12,\n",
    "                'channels_x':30,\n",
    "                'channels_y':200,\n",
    "                'num_heads':12,\n",
    "                'mlp_dim':12*12*4,\n",
    "                'dropout':0.2,\n",
    "                'transformer_type':'channels',\n",
    "                'positional_encoding_x':'fixed_embeddings',\n",
    "                'positional_encoding_y':'none',\n",
    "            },\n",
    "        },\n",
    "    'output_layer':{\n",
    "        'layer':nn.Conv2d,\n",
    "        'params':{\n",
    "            'in_channels': 30,\n",
    "            'out_channels': 30,\n",
    "            'kernel_size': 1,\n",
    "            'stride': 1,\n",
    "            'padding': 0,\n",
    "            \n",
    "        }\n",
    "    }\n",
    "}\n",
    "            \n",
    "batch_size = 3  \n",
    "seq_len1 = 14\n",
    "seq_len2 = 88\n",
    "hidden_size1 = 128\n",
    "hidden_size2 = 256\n",
    "\n",
    "transf_block = VisionTransformerBlock(\n",
    "    num_heads=8,\n",
    "    hidden_dim=hidden_size1,\n",
    "    kdim=hidden_size2,\n",
    "    vdim=hidden_size2,\n",
    "    mlp_dim=hidden_size1*4,\n",
    "    attention_layer=nn.MultiheadAttention,\n",
    "    dropout=0.2\n",
    ")\n",
    "\n",
    "w_transf = WindowVisionTransformer(\n",
    "    cols_in_patch=12,\n",
    "    rows_in_patch=12,\n",
    "    channels=200,\n",
    "    num_heads=8,\n",
    "    mlp_dim=16*16*4,\n",
    "    dropout=0.2,\n",
    "    layer_num=2,\n",
    "    transformer_type='patches',\n",
    "    positional_encoding=FixedSizeLearnableEmbeddings,\n",
    ")\n",
    "\n",
    "crossatt = WindowCrossAttention(\n",
    "    rows_in_patch_x=12,\n",
    "    cols_in_patch_x=12,\n",
    "    rows_in_patch_y=12,\n",
    "    cols_in_patch_y=12,\n",
    "    channels_x=100,\n",
    "    channels_y=200,\n",
    "    num_heads=6,\n",
    "    mlp_dim=6*6*4,\n",
    "    dropout=0.2,\n",
    "    transformer_type='channels',\n",
    "    positional_encoding_x=FixedSizeLearnableEmbeddings,\n",
    "    positional_encoding_y=FixedSizeLearnableEmbeddings,\n",
    "\n",
    ")\n",
    "\n",
    "#hs_former = HyperspectralTransformer(config)\n",
    "\n",
    "hsi = torch.randn(1, 200, 96, 96)\n",
    "hsi2 = torch.randn(1, 30, 96, 96)\n",
    "#ret = w_transf(hsi)\n",
    "#ret = crossatt(hsi2, hsi)\n",
    "#ret.shape\n",
    "\n",
    "aux_transformer_config['output_crossatt']['params']['positional_encoding_x'] = pos_enc_factory_dict[aux_transformer_config['output_crossatt']['params']['positional_encoding_x']]\n",
    "aux_transformer_config['output_crossatt']['params']['positional_encoding_y'] = pos_enc_factory_dict[aux_transformer_config['output_crossatt']['params']['positional_encoding_y']]\n",
    "aux_transformer_config['input_transformer']['params']['positional_encoding'] = pos_enc_factory_dict[aux_transformer_config['input_transformer']['params']['positional_encoding']]\n",
    "aux_transformer_config['intermediate_layers']['params']['positional_encoding'] = pos_enc_factory_dict[aux_transformer_config['intermediate_layers']['params']['positional_encoding']]\n",
    "#config['hsi_augmentation']['layer'] = feature_aggregation_factory_dict['hsi_augmentation']['layer']\n",
    "\n",
    "\n",
    "aux_transf = nn.ModuleDict()\n",
    "aux_transf['patch_emd'] = aux_transformer_config['patch_emd']['layer'](**aux_transformer_config['patch_emd']['params'])\n",
    "layer_name = aux_transformer_config['input_transformer']['layer']\n",
    "aux_transf['input_transformer'] = transformer_factory_dict[layer_name](**aux_transformer_config['input_transformer']['params'])\n",
    "aux_transf['hsi_augmentation'] = feature_aggregation_factory_dict[aux_transformer_config['hsi_augmentation']['layer']](**aux_transformer_config['hsi_augmentation']['params'])\n",
    "layer_name = aux_transformer_config['intermediate_layers']['layer']\n",
    "aux_transf['intermediate_layers'] = transformer_factory_dict[layer_name](**aux_transformer_config['intermediate_layers']['params'])\n",
    "layer_name = aux_transformer_config['output_crossatt']['layer']\n",
    "aux_transf['output_crossatt'] = transformer_factory_dict[layer_name](**aux_transformer_config['output_crossatt']['params'])\n",
    "aux_transf['output_layer'] = aux_transformer_config['output_layer']['layer'](**aux_transformer_config['output_layer']['params'])\n",
    "\n",
    "\n",
    "            \n",
    "\n",
    "x = aux_transf['patch_emd'](hsi)\n",
    "x = aux_transf['input_transformer'](x)\n",
    "augmented = aux_transf['hsi_augmentation'](hsi, x)\n",
    "x = aux_transf['intermediate_layers'](x)\n",
    "out = aux_transf['output_crossatt'](hsi2, x)\n",
    "out = aux_transf['output_layer'](out)\n",
    "x.shape, out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "26540a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    'patch_emd':{\n",
    "        'layer':nn.Conv2d,\n",
    "        'params':{\n",
    "            'in_channels': 200,\n",
    "            'out_channels': 200,\n",
    "            'kernel_size': 3,\n",
    "            'stride': 1,\n",
    "            'padding': 1,\n",
    "            'groups':200\n",
    "        }\n",
    "    },\n",
    "    'input_transformer': {\n",
    "            'layer': 'win_mha',\n",
    "            'params': {\n",
    "                'cols_in_patch':12,\n",
    "                'rows_in_patch':12,\n",
    "                'channels':200,\n",
    "                'num_heads':12,\n",
    "                'mlp_dim':12*12*4,\n",
    "                'dropout':0.2,\n",
    "                'layer_num':3,\n",
    "                'transformer_type':'channels',\n",
    "                'positional_encoding':'fixed_embeddings',\n",
    "            },\n",
    "        },\n",
    "    'hsi_augmentation':{\n",
    "        'layer': 'add',\n",
    "        'params': {}\n",
    "    },\n",
    "    'intermediate_layers':{\n",
    "            'layer': 'win_mha',\n",
    "            'params': {\n",
    "                'cols_in_patch':12,\n",
    "                'rows_in_patch':12,\n",
    "                'channels':200,\n",
    "                'num_heads':12,\n",
    "                'mlp_dim':12*12*4,\n",
    "                'dropout':0.2,\n",
    "                'layer_num':3,\n",
    "                'transformer_type':'channels',\n",
    "                'positional_encoding':'none',\n",
    "            },\n",
    "        },\n",
    "\n",
    "    'output_crossatt': {\n",
    "            'layer': 'crossatt',\n",
    "            'params': {\n",
    "                'rows_in_patch_x':12,\n",
    "                'cols_in_patch_x':12,\n",
    "                'rows_in_patch_y':12,\n",
    "                'cols_in_patch_y':12,\n",
    "                'channels_x':30,\n",
    "                'channels_y':200,\n",
    "                'num_heads':12,\n",
    "                'mlp_dim':12*12*4,\n",
    "                'dropout':0.2,\n",
    "                'transformer_type':'channels',\n",
    "                'positional_encoding_x':'fixed_embeddings',\n",
    "                'positional_encoding_y':'none',\n",
    "            },\n",
    "        },\n",
    "    'output_layer':{\n",
    "        'layer':nn.Conv2d,\n",
    "        'params':{\n",
    "            'in_channels': 30,\n",
    "            'out_channels': 30,\n",
    "            'kernel_size': 1,\n",
    "            'stride': 1,\n",
    "            'padding': 0,\n",
    "            \n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "with open(r'C:\\Users\\mokhail\\develop\\MultispectralSegmentation\\training_configs\\models\\unet_hsi.yaml') as fd:\n",
    "    new_config = yaml.load(fd, yaml.Loader)\n",
    "\n",
    "new_config['segmentation_nn']['params']['aux_transformer_config'] = config\n",
    "with open(r'training_configs\\models\\unet_aux_tr_hsi.yaml', 'w') as fd:\n",
    "    yaml.dump(new_config, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "37bcd7d9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 16,\n",
       " 'device': 'cuda:0',\n",
       " 'epoch_num': 300,\n",
       " 'input_image_size': 96,\n",
       " 'loss': {'params': {'ignore_index': -100,\n",
       "   'label_smoothing': 0.15,\n",
       "   'reduction': 'mean',\n",
       "   'weight': None},\n",
       "  'type': 'crossentropy'},\n",
       " 'lr_scheduler': {'args': {'T_0': 25,\n",
       "   'T_mult': 1,\n",
       "   'eta_min': 0,\n",
       "   'last_epoch': -1},\n",
       "  'params': {'frequency': 1,\n",
       "   'interval': 'epoch',\n",
       "   'monitor': 'val_loss',\n",
       "   'name': None,\n",
       "   'strict': True},\n",
       "  'type': 'cosine_warm_restarts'},\n",
       " 'multispecter_bands_indices': 200,\n",
       " 'name_postfix': 'HSI',\n",
       " 'optimizer': {'args': {}, 'type': 'adam'},\n",
       " 'path_to_dataset_root': 'C:\\\\Users\\\\mokhail\\\\develop\\\\DATA\\\\UAV-HSI-Crop-Dataset',\n",
       " 'segmentation_nn': {'input_layer_config': {},\n",
       "  'nn_architecture': 'unet',\n",
       "  'params': {'activation': None,\n",
       "   'aux_params': None,\n",
       "   'classes': 30,\n",
       "   'decoder_attention_type': None,\n",
       "   'decoder_channels': (256, 128, 128, 64, 64),\n",
       "   'decoder_interpolation': 'nearest',\n",
       "   'decoder_use_norm': 'batchnorm',\n",
       "   'encoder_depth': 5,\n",
       "   'in_channels': 3,\n",
       "   'aux_transformer_config': {'patch_emd': {'layer': torch.nn.modules.conv.Conv2d,\n",
       "     'params': {'in_channels': 200,\n",
       "      'out_channels': 200,\n",
       "      'kernel_size': 3,\n",
       "      'stride': 1,\n",
       "      'padding': 1,\n",
       "      'groups': 200}},\n",
       "    'input_transformer': {'layer': 'win_mha',\n",
       "     'params': {'cols_in_patch': 12,\n",
       "      'rows_in_patch': 12,\n",
       "      'channels': 200,\n",
       "      'num_heads': 12,\n",
       "      'mlp_dim': 576,\n",
       "      'dropout': 0.2,\n",
       "      'layer_num': 3,\n",
       "      'transformer_type': 'channels',\n",
       "      'positional_encoding': 'fixed_embeddings'}},\n",
       "    'hsi_augmentation': {'layer': 'add', 'params': {}},\n",
       "    'intermediate_layers': {'layer': 'win_mha',\n",
       "     'params': {'cols_in_patch': 12,\n",
       "      'rows_in_patch': 12,\n",
       "      'channels': 200,\n",
       "      'num_heads': 12,\n",
       "      'mlp_dim': 576,\n",
       "      'dropout': 0.2,\n",
       "      'layer_num': 3,\n",
       "      'transformer_type': 'channels',\n",
       "      'positional_encoding': 'none'}},\n",
       "    'output_crossatt': {'layer': 'crossatt',\n",
       "     'params': {'rows_in_patch_x': 12,\n",
       "      'cols_in_patch_x': 12,\n",
       "      'rows_in_patch_y': 12,\n",
       "      'cols_in_patch_y': 12,\n",
       "      'channels_x': 30,\n",
       "      'channels_y': 200,\n",
       "      'num_heads': 12,\n",
       "      'mlp_dim': 576,\n",
       "      'dropout': 0.2,\n",
       "      'transformer_type': 'channels',\n",
       "      'positional_encoding_x': 'fixed_embeddings',\n",
       "      'positional_encoding_y': 'none'}},\n",
       "    'output_layer': {'layer': torch.nn.modules.conv.Conv2d,\n",
       "     'params': {'in_channels': 30,\n",
       "      'out_channels': 30,\n",
       "      'kernel_size': 1,\n",
       "      'stride': 1,\n",
       "      'padding': 0}}}}},\n",
       " 'monitoring_metric': 'accuracy',\n",
       " 'train_augmentations': {'affine': {'degrees': [0, 45],\n",
       "   'fill': [0],\n",
       "   'scale': [0.7, 1.5],\n",
       "   'shear': [0, 0.2],\n",
       "   'translate': [0, 0.3]},\n",
       "  'horizontal_flip': {'p': 0.5},\n",
       "  'vertical_flip': {'p': 0.5}}}"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef370865",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 256, 24, 24])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class EmbeddingLayer(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_embeddings,\n",
    "            embedding_dim,\n",
    "            padding_idx=None,\n",
    "            max_norm=None,\n",
    "            norm_type=2.0,\n",
    "            scale_grad_by_freq=False,\n",
    "            sparse=False,\n",
    "            _weight=None,\n",
    "            _freeze=False,\n",
    "            device=None,\n",
    "            dtype=None\n",
    "            ):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(\n",
    "            num_embeddings,\n",
    "            embedding_dim,\n",
    "            padding_idx,\n",
    "            max_norm,\n",
    "            norm_type,\n",
    "            scale_grad_by_freq,\n",
    "            sparse,\n",
    "            _weight,\n",
    "            _freeze,\n",
    "            device,\n",
    "            dtype\n",
    "            )\n",
    "    def forward(self, x):\n",
    "        #!!!!\n",
    "        bs, seq_len, emb_dim = x.shape\n",
    "        positions = torch.arange(0, seq_len, dtype=torch.long).to(x.device)\n",
    "        pos_embeddings = self.embedding(positions)\n",
    "        return x + pos_embeddings\n",
    "    \n",
    "class FixedSizeLearnableEmbeddings(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            num_embeddings,\n",
    "            embedding_dim,\n",
    "            ):\n",
    "        super().__init__()\n",
    "        self.positional_encoding = nn.Parameter(torch.empty(1, num_embeddings, embedding_dim).normal_(std=0.02))\n",
    "\n",
    "    def forward(self,x):\n",
    "        return x + self.positional_encoding\n",
    "\n",
    "class LazyWindowVisionTransformerBlocks(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            rows_in_win,\n",
    "            cols_in_win,\n",
    "            channels:int,\n",
    "            layer_num:int,\n",
    "            num_heads: int,\n",
    "            transformer_mlp_dim:int,\n",
    "            attention_dropout:float,\n",
    "            dropout: float,\n",
    "            positional_encoding_block: str,\n",
    "            positional_encoding_block_params: Dict,\n",
    "            transformer_block: Callable[..., torch.nn.Module]=VitEncoderBlock,\n",
    "            ):\n",
    "        super().__init__()\n",
    "        assert channels % num_heads == 0, f'Channel num should be devisible by the number of MSA heads'\n",
    "        self.cols_in_win = cols_in_win\n",
    "        self.rows_in_win = rows_in_win\n",
    "        self.seq_len = self.rows_in_win * self.cols_in_win\n",
    "        self.head_dim = channels//num_heads\n",
    "\n",
    "        #print(pos_enc_factory_dict[positional_encoding_block] is EmbeddingLayer)\n",
    "        #print(pos_enc_factory_dict[positional_encoding_block] is FixedSizeLearnableEmbeddings)\n",
    "\n",
    "        #print(positional_encoding_block is FixedSizeLearnableEmbeddings)\n",
    "        \n",
    "        \n",
    "        if positional_encoding_block == 'fixed_embeddings':\n",
    "\n",
    "            positional_encoding_block_params.update(\n",
    "                {'seq_len': self.seq_len, 'embedding_dim': channels}\n",
    "            )\n",
    "        elif positional_encoding_block == 'embedding_layer':\n",
    "            positional_encoding_block_params.update(\n",
    "                {'num_embeddings': self.seq_len, 'embedding_dim': channels}\n",
    "            )\n",
    "\n",
    "        positional_encoding_block = pos_enc_factory_dict[positional_encoding_block]\n",
    "        #print(type(positional_encoding_block)==EmbeddingLayer)\n",
    "        #print(positional_encoding_block)\n",
    "        #print(positional_encoding_block_params)\n",
    "        self.positional_encoding = positional_encoding_block(**positional_encoding_block_params)\n",
    "\n",
    "        # можно создать несколько трансформерных слоев\n",
    "        transformer_layers_dict = {\n",
    "            f'transformer_enc_{i}': transformer_block(\n",
    "                num_heads=num_heads,\n",
    "                hidden_dim=channels,\n",
    "                mlp_dim=transformer_mlp_dim,\n",
    "                attention_dropout=attention_dropout,\n",
    "                dropout=dropout)\n",
    "            for i in range(layer_num)\n",
    "        }\n",
    "        self.transformer_layers = nn.ModuleDict(transformer_layers_dict)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        bs, channels, rows, cols = x.shape\n",
    "        rows_win_num = rows//self.rows_in_win\n",
    "        cols_win_num = cols//self.cols_in_win\n",
    "        #print(rows_win_num, cols_win_num)\n",
    "\n",
    "        # размер (bs, channels, rows, cols) преобразовываем в размер (bs, rows_win_num*cols_win_num, rows_in_win*cols_in_win, channels)\n",
    "        # rows=row_patches*rows_in_patch, cols=col_patches*cols_in_patch\n",
    "        \n",
    "        rearrange_pattern = 'bs channels (rows_win_num rows_in_win) (cols_win_num cols_in_win) -> (rows_win_num cols_win_num) bs (rows_in_win cols_in_win) channels'\n",
    "        rearrange_args = {\n",
    "            'rows_in_win':self.rows_in_win,\n",
    "            'rows_in_win':self.cols_in_win,\n",
    "            'rows_win_num':rows_win_num,\n",
    "            'cols_win_num':cols_win_num,\n",
    "        }\n",
    "            \n",
    "        h = eo.rearrange(\n",
    "            x,\n",
    "            rearrange_pattern,\n",
    "            **rearrange_args,\n",
    "        )\n",
    "        #print(f'h:{h.shape}')\n",
    "        windows_outs = []\n",
    "        # итерируем по окнам\n",
    "        for i, window in enumerate(h):\n",
    "            #print(f'encoded:{window.shape}')\n",
    "            # позиционное кодирование длля окна\n",
    "            window = self.positional_encoding(window)\n",
    "            #print(f'encoded window:{window.shape}')\n",
    "            #print()\n",
    "            # итерируем по слоям трансформера\n",
    "            for layer_name, layer in self.transformer_layers.items():\n",
    "\n",
    "                window = layer(window)\n",
    "            windows_outs.append(window.unsqueeze(0))\n",
    "        \n",
    "        windows_outs = torch.cat(windows_outs,dim=0)\n",
    "\n",
    "        # rearrange back\n",
    "        rearrange_pattern = '(rows_win_num cols_win_num) bs (rows_in_win cols_in_win) channels -> bs channels (rows_win_num rows_in_win) (cols_win_num cols_in_win)'\n",
    "        windows_outs = eo.rearrange(\n",
    "            windows_outs,\n",
    "            rearrange_pattern,\n",
    "            **rearrange_args,\n",
    "        )\n",
    "\n",
    "        return windows_outs\n",
    "\n",
    "pos_enc_factory_dict = {\n",
    "    'fixed_embeddings': FixedSizeLearnableEmbeddings,\n",
    "    'embedding_layer': EmbeddingLayer,\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "tr_block = LazyWindowVisionTransformerBlocks(\n",
    "    rows_in_win=6,\n",
    "    cols_in_win=6,\n",
    "    channels=256,\n",
    "    layer_num=1,\n",
    "    num_heads=8,\n",
    "    transformer_mlp_dim=512,\n",
    "    attention_dropout=0.2,\n",
    "    dropout=0.2,\n",
    "    positional_encoding_block='embedding_layer',#'fixed_embeddings' 'embedding_layer'\n",
    "    positional_encoding_block_params={},\n",
    "    transformer_block=VitEncoderBlock,\n",
    ")\n",
    "features = torch.randn(2, 256, 24, 24)\n",
    "out = tr_block(features)\n",
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0276c8d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scaled_dot_product_attention(query, key, value, mask=None):\n",
    "    # query, key, value are (batch_size, num_heads, seq_len, head_dim)\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / (d_k ** 0.5)\n",
    "\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9) # Apply mask for padding or causal attention\n",
    "\n",
    "    attention_weights = F.softmax(scores, dim=-1)\n",
    "    output = torch.matmul(attention_weights, value)\n",
    "    return output, attention_weights\n",
    "\n",
    "class ConvMultiheadAttention(nn.Module):\n",
    "    '''\n",
    "    Class for convolutional multihead self-attention Wq, Wk, Wv are replaced from fully connected to convolutional layers \n",
    "    '''\n",
    "    def __init__(\n",
    "            self,\n",
    "            q_in_channels,\n",
    "            k_in_channels,\n",
    "            v_in_channels,\n",
    "            q_out_channels,\n",
    "            k_out_channels,\n",
    "            v_out_channels,\n",
    "            in_kernel_size,\n",
    "            in_padding,\n",
    "            in_stride,\n",
    "            head_row_dim,\n",
    "            head_col_dim,\n",
    "            head_ch_dim,\n",
    "            norm=nn.LayerNorm\n",
    "            ):\n",
    "        super().__init__()\n",
    "\n",
    "        self.head_row_dim = head_row_dim\n",
    "        self.head_col_dim = head_col_dim\n",
    "        self.head_ch_dim = head_ch_dim\n",
    "\n",
    "        self.input_norm = norm(head_ch_dim*head_row_dim*head_col_dim, eps=1e-6)\n",
    "        \n",
    "        self.conv_q = nn.Conv2d(\n",
    "            in_channels=q_in_channels,\n",
    "            out_channels=q_out_channels,\n",
    "            kernel_size=in_kernel_size,\n",
    "            padding=in_padding,\n",
    "            stride=in_stride\n",
    "            )\n",
    "        \n",
    "        self.conv_k = nn.Conv2d(\n",
    "            in_channels=k_in_channels,\n",
    "            out_channels=k_out_channels,\n",
    "            kernel_size=in_kernel_size,\n",
    "            padding=in_padding,\n",
    "            stride=in_stride\n",
    "            )\n",
    "        \n",
    "        self.conv_v = nn.Conv2d(\n",
    "            in_channels=v_in_channels,\n",
    "            out_channels=v_out_channels,\n",
    "            kernel_size=in_kernel_size,\n",
    "            padding=in_padding,\n",
    "            stride=in_stride\n",
    "            )\n",
    "        \n",
    "        #self.out_conv = nn.Conv2d(in_channels=q_out_channels,out_channels=out_channels,kernel_size=out_kernel_size,padding=out_padding,stride=out_stride)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        bs, ch, rows, cols = value.shape\n",
    "        head_row_num = rows // self.head_row_dim\n",
    "        head_col_num = cols // self.head_col_dim\n",
    "        head_ch_num = ch // self.head_ch_dim\n",
    "\n",
    "        #print(head_row_num, head_col_num)\n",
    "\n",
    "        q = self.conv_q(query)\n",
    "        k = self.conv_k(key)\n",
    "        v = self.conv_v(value)\n",
    "        qkv_rearrangement_str = 'bs (head_ch_dim head_ch_num) (head_rdim head_rnum) (head_cdim head_cnum) -> bs (head_rnum head_cnum head_ch_num) (head_rdim head_cdim head_ch_dim) '\n",
    "        \n",
    "        q = eo.rearrange(\n",
    "            q,\n",
    "            qkv_rearrangement_str,\n",
    "            head_rdim=self.head_row_dim, head_cdim=self.head_col_dim, head_ch_dim=self.head_ch_dim)\n",
    "\n",
    "        k = eo.rearrange(\n",
    "            k,\n",
    "            qkv_rearrangement_str,\n",
    "            head_rdim=self.head_row_dim, head_cdim=self.head_col_dim, head_ch_dim=self.head_ch_dim\n",
    "            )\n",
    "        \n",
    "        v = eo.rearrange(\n",
    "            v,\n",
    "            qkv_rearrangement_str,\n",
    "            head_rdim=self.head_row_dim, head_cdim=self.head_col_dim, head_ch_dim=self.head_ch_dim)\n",
    "        #print(f'q:{q.shape};k:{k.shape};v:{v.shape}')\n",
    "        \n",
    "        weighted_v = F.scaled_dot_product_attention(query=q, key=k, value=v)\n",
    "        #print(f'v_w:{weighted_v.shape}')\n",
    "\n",
    "        weighted_v = eo.rearrange(\n",
    "            weighted_v,\n",
    "            'bs (head_rnum head_cnum head_ch_num) (head_rdim head_cdim head_ch_dim) -> bs (head_ch_dim head_ch_num) (head_rdim head_rnum) (head_cdim head_cnum)',\n",
    "            head_rdim=self.head_row_dim, head_cdim=self.head_col_dim, head_rnum=head_row_num, head_cnum=head_col_num,\n",
    "        )\n",
    "        #print(f'v_w_ra:{weighted_v.shape}')\n",
    "        return weighted_v\n",
    "\n",
    "        out = self.out_conv(weighted_v)\n",
    "        return out\n",
    "    \n",
    "class ConvMSABlock(nn.Module):\n",
    "    '''\n",
    "    Implementatuion of convolutional multihead self-attention block\n",
    "    '''\n",
    "    def __init__(\n",
    "            self,\n",
    "            msa_in_channels,\n",
    "            msa_intermediate_channels,\n",
    "            msa_in_kernel_size,\n",
    "            msa_in_padding,\n",
    "            msa_in_stride,\n",
    "            msa_out_channels,\n",
    "            \n",
    "            msa_head_row_dim,\n",
    "            msa_head_col_dim,\n",
    "            msa_head_ch_dim,\n",
    "\n",
    "            dropout,\n",
    "\n",
    "            out_conv_hidden_channels,\n",
    "            out_conv_kernel_size,\n",
    "            out_conv_padding,\n",
    "            out_conv_stride,\n",
    "\n",
    "            out_conv_out_channels,\n",
    "\n",
    "            out_conv_act,\n",
    "            \n",
    "            norm_layer: Callable[..., torch.nn.Module],\n",
    "            ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.norm1 = norm_layer(msa_in_channels)\n",
    "        self.self_att = ConvMultiheadAttention(\n",
    "            q_in_channels=msa_in_channels,\n",
    "            k_in_channels=msa_in_channels,\n",
    "            v_in_channels=msa_in_channels,\n",
    "            q_out_channels=msa_intermediate_channels,\n",
    "            k_out_channels=msa_intermediate_channels,\n",
    "            v_out_channels=msa_intermediate_channels,\n",
    "            in_kernel_size=msa_in_kernel_size,\n",
    "            in_padding=msa_in_padding,\n",
    "            in_stride=msa_in_stride,\n",
    "            head_row_dim=msa_head_row_dim,\n",
    "            head_col_dim=msa_head_col_dim,\n",
    "            head_ch_dim=msa_head_ch_dim,\n",
    "        )\n",
    "\n",
    "        self.dropout = nn.Dropout2d(dropout)\n",
    "\n",
    "        self.out_conv = nn.Sequential(\n",
    "            torchvision.ops.Conv2dNormActivation(\n",
    "                in_channels=msa_out_channels,\n",
    "                out_channels=out_conv_hidden_channels,\n",
    "                kernel_size=out_conv_kernel_size,\n",
    "                padding=out_conv_padding,\n",
    "                stride=out_conv_stride,\n",
    "                activation_layer=out_conv_act\n",
    "            ),\n",
    "            torchvision.ops.Conv2dNormActivation(\n",
    "                in_channels=out_conv_hidden_channels,\n",
    "                out_channels=out_conv_out_channels,\n",
    "                kernel_size=out_conv_kernel_size,\n",
    "                padding=out_conv_padding,\n",
    "                stride=out_conv_stride,\n",
    "                activation_layer=out_conv_act\n",
    "            )\n",
    "        )\n",
    "\n",
    "        self.norm2 = norm_layer(msa_out_channels)\n",
    "    def forward(self, input_features):\n",
    "        \n",
    "        x = self.norm1(input_features)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = self.self_att(query=x, key=x, value=x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        x = x + input_features\n",
    "        x = self.norm2(x)\n",
    "        #print(x.shape)\n",
    "        y = self.out_conv(x)\n",
    "        return x + y\n",
    "    \n",
    "class ConvCrossAttentionBlock(nn.Module):\n",
    "    '''\n",
    "    Implementation of convolutional cross-attention block\n",
    "    '''\n",
    "    def __init__(\n",
    "            self,\n",
    "            q_in_channels,\n",
    "            k_in_channels,\n",
    "            v_in_channels,\n",
    "            q_out_channels,\n",
    "            k_out_channels,\n",
    "            v_out_channels,\n",
    "            in_kernel_size,\n",
    "            in_padding,\n",
    "            in_stride,\n",
    "            \n",
    "            head_row_dim,\n",
    "            head_col_dim,\n",
    "            head_ch_dim,\n",
    "\n",
    "            dropout,\n",
    "            \n",
    "            norm_layer: Callable[..., torch.nn.Module],\n",
    "            ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.kv_inp_norm = norm_layer(k_in_channels)\n",
    "        self.q_inp_norm = norm_layer(q_in_channels)\n",
    "\n",
    "        self.cross_att = ConvMultiheadAttention(\n",
    "            q_in_channels=q_in_channels,\n",
    "            k_in_channels=k_in_channels,\n",
    "            v_in_channels=v_in_channels,\n",
    "            q_out_channels=q_out_channels,\n",
    "            k_out_channels=k_out_channels,\n",
    "            v_out_channels=v_out_channels,\n",
    "            in_kernel_size=in_kernel_size,\n",
    "            in_padding=in_padding,\n",
    "            in_stride=in_stride,\n",
    "            \n",
    "            head_row_dim=head_row_dim,\n",
    "            head_col_dim=head_col_dim,\n",
    "            head_ch_dim=head_ch_dim,\n",
    "        )\n",
    "        self.dropout = nn.Dropout2d(dropout)\n",
    "        self.out_norm = norm_layer(q_out_channels)\n",
    "\n",
    "    def forward(self, q, kv):\n",
    "        \n",
    "        x = self.kv_inp_norm(kv)\n",
    "        q = self.q_inp_norm(q)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = self.cross_att(query=q, key=x, value=x)\n",
    "        \n",
    "        x = self.dropout(x)\n",
    "        #print(x.shape, q.shape)\n",
    "        x = x + q\n",
    "        x = self.out_norm(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "class ConcatDim1(nn.Module):\n",
    "    '''\n",
    "    Implementation of concatenation. It is nececcary for various aggreagation strategies in UNet decoder\n",
    "    '''\n",
    "    def forward(self, *tensors):\n",
    "        return torch.cat(tensors, dim=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa0e5384",
   "metadata": {},
   "source": [
    "## Код для UnetAtt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d3e9abe",
   "metadata": {},
   "outputs": [],
   "source": [
    "class UnetAtt(SegmentationModel):\n",
    "    \"\"\"\n",
    "    U-Net is a fully convolutional neural network architecture designed for semantic image segmentation.\n",
    "\n",
    "    It consists of two main parts:\n",
    "\n",
    "    1. An encoder (downsampling path) that extracts increasingly abstract features\n",
    "    2. A decoder (upsampling path) that gradually recovers spatial details\n",
    "\n",
    "    The key is the use of skip connections between corresponding encoder and decoder layers.\n",
    "    These connections allow the decoder to access fine-grained details from earlier encoder layers,\n",
    "    which helps produce more precise segmentation masks.\n",
    "\n",
    "    The skip connections work by concatenating feature maps from the encoder directly into the decoder\n",
    "    at corresponding resolutions. This helps preserve important spatial information that would\n",
    "    otherwise be lost during the encoding process.\n",
    "\n",
    "    Args:\n",
    "        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n",
    "            to extract features of different spatial resolution\n",
    "        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features\n",
    "            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n",
    "            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n",
    "            Default is 5\n",
    "        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and\n",
    "            other pretrained weights (see table with available weights for each encoder_name)\n",
    "        decoder_channels: List of integers which specify **in_channels** parameter for convolutions used in decoder.\n",
    "            Length of the list should be the same as **encoder_depth**\n",
    "        decoder_use_norm:     Specifies normalization between Conv2D and activation.\n",
    "            Accepts the following types:\n",
    "            - **True**: Defaults to `\"batchnorm\"`.\n",
    "            - **False**: No normalization (`nn.Identity`).\n",
    "            - **str**: Specifies normalization type using default parameters. Available values:\n",
    "              `\"batchnorm\"`, `\"identity\"`, `\"layernorm\"`, `\"instancenorm\"`, `\"inplace\"`.\n",
    "            - **dict**: Fully customizable normalization settings. Structure:\n",
    "              ```python\n",
    "              {\"type\": <norm_type>, **kwargs}\n",
    "              ```\n",
    "              where `norm_name` corresponds to normalization type (see above), and `kwargs` are passed directly to the normalization layer as defined in PyTorch documentation.\n",
    "\n",
    "            **Example**:\n",
    "            ```python\n",
    "            decoder_use_norm={\"type\": \"layernorm\", \"eps\": 1e-2}\n",
    "            ```\n",
    "        decoder_attention_type: Attention module used in decoder of the model. Available options are\n",
    "            **None** and **scse** (https://arxiv.org/abs/1808.08127).\n",
    "        decoder_interpolation: Interpolation mode used in decoder of the model. Available options are\n",
    "            **\"nearest\"**, **\"bilinear\"**, **\"bicubic\"**, **\"area\"**, **\"nearest-exact\"**. Default is **\"nearest\"**.\n",
    "        in_channels: A number of input channels for the model, default is 3 (RGB images)\n",
    "        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n",
    "        activation: An activation function to apply after the final convolution layer.\n",
    "            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**,\n",
    "            **callable** and **None**. Default is **None**.\n",
    "        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build\n",
    "            on top of encoder if **aux_params** is not **None** (default). Supported params:\n",
    "                - classes (int): A number of classes\n",
    "                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n",
    "                - dropout (float): Dropout factor in [0, 1)\n",
    "                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\"\n",
    "                    (could be **None** to return logits)\n",
    "        kwargs: Arguments passed to the encoder class ``__init__()`` function. Applies only to ``timm`` models. Keys with ``None`` values are pruned before passing.\n",
    "\n",
    "    Returns:\n",
    "        ``torch.nn.Module``: Unet\n",
    "\n",
    "    Example:\n",
    "        .. code-block:: python\n",
    "\n",
    "            import torch\n",
    "            import segmentation_models_pytorch as smp\n",
    "\n",
    "            model = smp.Unet(\"resnet18\", encoder_weights=\"imagenet\", classes=5)\n",
    "            model.eval()\n",
    "\n",
    "            # generate random images\n",
    "            images = torch.rand(2, 3, 256, 256)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                mask = model(images)\n",
    "\n",
    "            print(mask.shape)\n",
    "            # torch.Size([2, 5, 256, 256])\n",
    "\n",
    "    .. _Unet:\n",
    "        https://arxiv.org/abs/1505.04597\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    requires_divisible_input_shape = False\n",
    "\n",
    "    @supports_config_loading\n",
    "    def __init__(\n",
    "        self,\n",
    "        decoder_layers_configs: Sequence,\n",
    "        transformer_branch_config: Dict,\n",
    "        encoder_name: str = \"resnet34\",\n",
    "        encoder_depth: int = 5,\n",
    "        encoder_weights: Optional[str] = \"imagenet\",\n",
    "        decoder_use_norm: Union[bool, str, Dict[str, Any]] = \"batchnorm\",\n",
    "        decoder_channels: Sequence[int] = (256, 128, 64, 32, 16),\n",
    "        decoder_interpolation: str = \"nearest\",\n",
    "        in_channels: int = 3,\n",
    "        classes: int = 1,\n",
    "        activation: Optional[Union[str, Callable]] = None,\n",
    "        aux_params: Optional[dict] = None,\n",
    "        **kwargs: dict[str, Any],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        decoder_use_batchnorm = kwargs.pop(\"decoder_use_batchnorm\", None)\n",
    "        if decoder_use_batchnorm is not None:\n",
    "            warnings.warn(\n",
    "                \"The usage of decoder_use_batchnorm is deprecated. Please modify your code for decoder_use_norm\",\n",
    "                DeprecationWarning,\n",
    "                stacklevel=2,\n",
    "            )\n",
    "            decoder_use_norm = decoder_use_batchnorm\n",
    "\n",
    "        self.encoder = get_encoder(\n",
    "            encoder_name,\n",
    "            in_channels=in_channels,\n",
    "            depth=encoder_depth,\n",
    "            weights=encoder_weights,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        add_center_block = encoder_name.startswith(\"vgg\")\n",
    "\n",
    "        self.decoder = UnetDecoderAtt(\n",
    "            encoder_channels=self.encoder.out_channels,\n",
    "            decoder_channels=decoder_channels,\n",
    "            decoder_layers_configs=decoder_layers_configs,\n",
    "            n_blocks=encoder_depth,\n",
    "            use_norm=decoder_use_norm,\n",
    "            add_center_block=add_center_block,\n",
    "            interpolation_mode=decoder_interpolation,\n",
    "        )\n",
    "\n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=decoder_channels[-1],\n",
    "            out_channels=classes,\n",
    "            activation=activation,\n",
    "            kernel_size=3,\n",
    "        )\n",
    "\n",
    "        if aux_params is not None:\n",
    "            self.classification_head = ClassificationHead(\n",
    "                in_channels=self.encoder.out_channels[-1], **aux_params\n",
    "            )\n",
    "        else:\n",
    "            self.classification_head = None\n",
    "\n",
    "        self.name = \"u-{}\".format(encoder_name)\n",
    "        self.initialize()\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\n",
    "\n",
    "        features = self.encoder(x)\n",
    "        decoder_output = self.decoder(features)\n",
    "\n",
    "        masks = self.segmentation_head(decoder_output)\n",
    "\n",
    "        if self.classification_head is not None:\n",
    "            labels = self.classification_head(features[-1])\n",
    "            return masks, labels\n",
    "\n",
    "        return masks\n",
    "\n",
    "class UnetDecoderBlockAtt(nn.Module):\n",
    "    \"\"\"A decoder block in the U-Net architecture that performs upsampling and feature fusion.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        config\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.interpolation_mode = config['interpolation_mode']\n",
    "        in_channels = config['in_channels']\n",
    "        out_channels = config['out_channels']\n",
    "        skip_channels = config['skip_channels']\n",
    "\n",
    "        agg_type = config['aggregation_layer']['layer']\n",
    "        att1_type = config['attention1']['layer']\n",
    "        \n",
    "        # настраиваем cross-attention\n",
    "        if agg_type == 'conv_cross_att':\n",
    "            if skip_channels != 0:\n",
    "                config['aggregation_layer']['params']['q_in_channels'] = in_channels\n",
    "                config['aggregation_layer']['params']['k_in_channels'] = skip_channels\n",
    "                config['aggregation_layer']['params']['v_in_channels'] = skip_channels\n",
    "                config['aggregation_layer']['params']['q_out_channels'] = in_channels\n",
    "                config['aggregation_layer']['params']['k_out_channels'] = skip_channels\n",
    "                config['aggregation_layer']['params']['v_out_channels'] = skip_channels\n",
    "\n",
    "        # настраиваем multihead self-attention, в зависимости от параметров слоя объединения\n",
    "        if att1_type == 'conv_msa':\n",
    "            if agg_type == 'conv_cross_att':\n",
    "                # надо добавить выбор q=features OR q=skip\n",
    "                if skip_channels != 0:\n",
    "                    att_channels = in_channels\n",
    "                else:\n",
    "                    att_channels = in_channels\n",
    "                config['attention1']['params']['msa_in_channels'] = att_channels\n",
    "                config['attention1']['params']['msa_intermediate_channels'] = att_channels\n",
    "                config['attention1']['params']['msa_out_channels'] = att_channels\n",
    "                config['attention1']['params']['out_conv_out_channels'] = att_channels\n",
    "            else:\n",
    "                config['attention1']['params']['msa_in_channels'] = in_channels + skip_channels\n",
    "                config['attention1']['params']['msa_intermediate_channels'] = in_channels + skip_channels\n",
    "                config['attention1']['params']['msa_out_channels'] = in_channels + skip_channels\n",
    "                config['attention1']['params']['out_conv_out_channels'] = in_channels + skip_channels\n",
    "\n",
    "        elif att1_type == 'win_msa':\n",
    "            if 'cross_att' in agg_type:\n",
    "                config['attention1']['params']['channels'] = in_channels\n",
    "            else:\n",
    "                config['attention1']['params']['channels'] = in_channels + skip_channels\n",
    "            #pos_enc_type = config['attention1']['params']['positional_encoding_block']\n",
    "            #config['attention1']['params']['positional_encoding_block'] = pos_enc_factory_dict[pos_enc_type]\n",
    "            \n",
    "        # получаем метод создания слоя агрегации признаков\n",
    "        create_aggregation = unet_aggregation_factory_dict[agg_type]\n",
    "        if skip_channels != 0:\n",
    "            self.aggregation_layer = create_aggregation(**config['aggregation_layer']['params'])\n",
    "        else:\n",
    "            self.aggregation_layer = nn.Identity()\n",
    "        \n",
    "        # получаем метод создания слоя внимания после агрегации\n",
    "        create_attention = unet_attention_factory_dict[att1_type]\n",
    "        self.attention1 = create_attention(**config['attention1']['params'])\n",
    "        # создаем сверточные слои после слоя внимания\n",
    "        conv_layers = []\n",
    "        for idx, params in enumerate(config['conv']):\n",
    "            if idx == 0:\n",
    "                if agg_type == 'conv_cross_att' and skip_channels != 0:\n",
    "                    in_conv_ch = in_channels\n",
    "                else:\n",
    "                    in_conv_ch = in_channels + skip_channels\n",
    "            else:\n",
    "                in_conv_ch = out_channels\n",
    "\n",
    "            conv = torchvision.ops.Conv2dNormActivation(\n",
    "                in_channels=in_conv_ch,\n",
    "                out_channels=out_channels,\n",
    "                **params\n",
    "                )\n",
    "            conv_layers.append(conv)\n",
    "        self.conv_layers = nn.Sequential(*conv_layers)\n",
    "\n",
    "        att2_type = config['attention2']['layer']\n",
    "        if att2_type == 'conv_msa':\n",
    "            config['attention2']['params']['msa_in_channels'] = out_channels\n",
    "            config['attention2']['params']['msa_intermediate_channels'] = out_channels\n",
    "            config['attention2']['params']['msa_out_channels'] = out_channels\n",
    "            config['attention2']['params']['out_conv_out_channels'] = out_channels\n",
    "        elif att2_type == 'win_msa':\n",
    "            config['attention2']['params']['channels'] = out_channels\n",
    "            #pos_enc_type = config['attention2']['params']['positional_encoding_block']\n",
    "            #config['attention2']['params']['positional_encoding_block'] = pos_enc_factory_dict[pos_enc_type]\n",
    "            \n",
    "        create_attention = unet_attention_factory_dict[att2_type]\n",
    "        self.attention2 = create_attention(**config['attention2']['params'])\n",
    "        \n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        feature_map: torch.Tensor,\n",
    "        target_height: int,\n",
    "        target_width: int,\n",
    "        skip_connection: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        feature_map = F.interpolate(\n",
    "            feature_map,\n",
    "            size=(target_height, target_width),\n",
    "            mode=self.interpolation_mode,\n",
    "        )\n",
    "        #print('DECODER LAYER!!!')\n",
    "        if skip_connection is not None:\n",
    "            #print(f'feat:{feature_map.shape},skip:{skip_connection.shape}')\n",
    "            feature_map = self.aggregation_layer(feature_map, skip_connection)\n",
    "            feature_map = self.attention1(feature_map)\n",
    "        #print(f'att_feat:{feature_map.shape}')\n",
    "        feature_map = self.conv_layers(feature_map)\n",
    "        feature_map = self.attention2(feature_map)\n",
    "        return feature_map\n",
    "\n",
    "class UnetDecoderAtt(nn.Module):\n",
    "    \"\"\"The decoder part of the U-Net architecture.\n",
    "\n",
    "    Takes encoded features from different stages of the encoder and progressively upsamples them while\n",
    "    combining with skip connections. This helps preserve fine-grained details in the final segmentation.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_channels: Sequence[int],\n",
    "        decoder_channels: Sequence[int],\n",
    "        decoder_layers_configs: Sequence[Dict],\n",
    "        n_blocks: int = 5,\n",
    "        add_center_block: bool = False,\n",
    "        interpolation_mode: str = \"nearest\",\n",
    "        use_norm:str = \"batchnorm\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        if n_blocks != len(decoder_channels):\n",
    "            raise ValueError(\n",
    "                \"Model depth is {}, but you provide `decoder_channels` for {} blocks.\".format(\n",
    "                    n_blocks, len(decoder_channels)\n",
    "                )\n",
    "            )\n",
    "        \n",
    "        if decoder_layers_configs is not None and (n_blocks != len(decoder_layers_configs)):\n",
    "            raise ValueError(\n",
    "                \"Model depth is {}, but you provide `attention_configs` for {} blocks.\".format(\n",
    "                    n_blocks, len(decoder_layers_configs)\n",
    "                )\n",
    "            )\n",
    "\n",
    "        # remove first skip with same spatial resolution\n",
    "        encoder_channels = encoder_channels[1:]\n",
    "        # reverse channels to start from head of encoder\n",
    "        encoder_channels = encoder_channels[::-1]\n",
    "\n",
    "        # computing blocks input and output channels\n",
    "        head_channels = encoder_channels[0]\n",
    "        in_channels = [head_channels] + list(decoder_channels[:-1])\n",
    "        skip_channels = list(encoder_channels[1:]) + [0]\n",
    "        out_channels = decoder_channels\n",
    "\n",
    "        if add_center_block:\n",
    "            self.center = smp.decoders.unet.decoder.UnetCenterBlock(\n",
    "                head_channels,\n",
    "                head_channels,\n",
    "                use_norm=use_norm,\n",
    "            )\n",
    "        else:\n",
    "            self.center = nn.Identity()\n",
    "\n",
    "        # combine decoder keyword arguments\n",
    "        self.blocks = nn.ModuleList()\n",
    "        for block_in_channels, block_skip_channels, block_out_channels, decoder_layer_config in zip(\n",
    "            in_channels, skip_channels, out_channels, decoder_layers_configs\n",
    "        ):\n",
    "            #print(f'in:{block_in_channels}, skip:{block_skip_channels}, out:{block_out_channels}')\n",
    "            #print('-------------')\n",
    "            decoder_layer_config['in_channels'] = block_in_channels\n",
    "            decoder_layer_config['skip_channels'] = block_skip_channels\n",
    "            decoder_layer_config['out_channels'] = block_out_channels\n",
    "            block = UnetDecoderBlockAtt(\n",
    "                decoder_layer_config\n",
    "            )\n",
    "            self.blocks.append(block)\n",
    "\n",
    "    def forward(self, features: List[torch.Tensor]) -> torch.Tensor:\n",
    "        # spatial shapes of features: [hw, hw/2, hw/4, hw/8, ...]\n",
    "        spatial_shapes = [feature.shape[2:] for feature in features]\n",
    "        spatial_shapes = spatial_shapes[::-1]\n",
    "\n",
    "        features = features[1:]  # remove first skip with same spatial resolution\n",
    "        features = features[::-1]  # reverse channels to start from head of encoder\n",
    "\n",
    "        head = features[0]\n",
    "        skip_connections = features[1:]\n",
    "\n",
    "        x = self.center(head)\n",
    "\n",
    "        for i, decoder_block in enumerate(self.blocks):\n",
    "            # upsample to the next spatial shape\n",
    "            height, width = spatial_shapes[i + 1]\n",
    "            skip_connection = skip_connections[i] if i < len(skip_connections) else None\n",
    "            x = decoder_block(x, height, width, skip_connection=skip_connection)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "079b9d59",
   "metadata": {},
   "source": [
    "# Адаптация FCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "21b14254",
   "metadata": {},
   "outputs": [],
   "source": [
    "class FCNDecoderBlock(nn.Module):\n",
    "    \"\"\"A decoder block in the FCN architecture that performs upsampling and feature fusion.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_channels: int,\n",
    "        out_channels: int,\n",
    "        use_norm: Union[bool, str, Dict[str, Any]] = \"batchnorm\",\n",
    "        attention_type: Optional[str] = None,\n",
    "        interpolation_mode: str = \"nearest\",\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.interpolation_mode = interpolation_mode\n",
    "        self.conv1 = md.Conv2dReLU(\n",
    "            in_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_norm=use_norm,\n",
    "        )\n",
    "        self.attention1 = md.Attention(\n",
    "            attention_type, in_channels=in_channels\n",
    "        )\n",
    "        self.conv2 = md.Conv2dReLU(\n",
    "            out_channels,\n",
    "            out_channels,\n",
    "            kernel_size=3,\n",
    "            padding=1,\n",
    "            use_norm=use_norm,\n",
    "        )\n",
    "        self.attention2 = md.Attention(attention_type, in_channels=out_channels)\n",
    "\n",
    "    def forward(\n",
    "        self,\n",
    "        feature_map: torch.Tensor,\n",
    "        target_height: int,\n",
    "        target_width: int,\n",
    "        skip_connection: Optional[torch.Tensor] = None,\n",
    "    ) -> torch.Tensor:\n",
    "        # сначала интерполяция и свертка\n",
    "        feature_map = F.interpolate(\n",
    "            feature_map,\n",
    "            size=(target_height, target_width),\n",
    "            mode=self.interpolation_mode,\n",
    "        )\n",
    "        feature_map = self.conv1(feature_map)\n",
    "        feature_map = self.attention1(feature_map)\n",
    "        \n",
    "        # потом сложение и выходная свертка\n",
    "        if skip_connection is not None:\n",
    "            feature_map = feature_map + skip_connection\n",
    "        feature_map = self.conv2(feature_map)\n",
    "        feature_map = self.attention2(feature_map)\n",
    "        \n",
    "        return feature_map\n",
    "    \n",
    "class FCNDecoder(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            encoder_channels: Sequence[int],\n",
    "            decoder_last_channel: Sequence[int],\n",
    "            n_blocks: int = 5,\n",
    "            use_norm: Union[bool, str, Dict[str, Any]] = \"batchnorm\",\n",
    "            attention_type: Optional[str] = None,\n",
    "            add_center_block: bool = False,\n",
    "            interpolation_mode: str = \"nearest\",\n",
    "        ):\n",
    "            super().__init__()\n",
    "            # remove first skip with same spatial resolution\n",
    "            encoder_channels = encoder_channels[1:]\n",
    "            # reverse channels to start from head of encoder\n",
    "            encoder_channels = encoder_channels[::-1]\n",
    "\n",
    "            # computing blocks input and output channels\n",
    "            head_channels = encoder_channels[0]\n",
    "            in_channels = encoder_channels\n",
    "            out_channels = encoder_channels[1:] + [decoder_last_channel]\n",
    "            \n",
    "            if add_center_block:\n",
    "                self.center = smp.decoders.unet.decoder.UnetCenterBlock(\n",
    "                    head_channels,\n",
    "                    head_channels//2,\n",
    "                    use_norm=use_norm,\n",
    "                )\n",
    "            else:\n",
    "                self.center = nn.Identity()\n",
    "\n",
    "            # combine decoder keyword arguments\n",
    "            self.blocks = nn.ModuleList()\n",
    "            for block_in_channels, block_out_channels in zip(\n",
    "                in_channels, out_channels\n",
    "            ):\n",
    "                block = FCNDecoderBlock(\n",
    "                    block_in_channels,\n",
    "                    block_out_channels,\n",
    "                    use_norm=use_norm,\n",
    "                    attention_type=attention_type,\n",
    "                    interpolation_mode=interpolation_mode,\n",
    "                )\n",
    "                self.blocks.append(block)\n",
    "\n",
    "    def forward(self, features: List[torch.Tensor]) -> torch.Tensor:\n",
    "        # spatial shapes of features: [hw, hw/2, hw/4, hw/8, ...]\n",
    "        spatial_shapes = [feature.shape[2:] for feature in features]\n",
    "        spatial_shapes = spatial_shapes[::-1]\n",
    "\n",
    "        features = features[1:]  # remove first skip with same spatial resolution\n",
    "        features = features[::-1]  # reverse channels to start from head of encoder\n",
    "        \n",
    "\n",
    "        head = features[0]\n",
    "        skip_connections = features[1:]\n",
    "        \n",
    "\n",
    "        x = self.center(head)\n",
    "\n",
    "        for i, decoder_block in enumerate(self.blocks):\n",
    "            # upsample to the next spatial shape\n",
    "            height, width = spatial_shapes[i + 1]\n",
    "            \n",
    "            skip_connection = skip_connections[i] if i < len(skip_connections) else None\n",
    "            \n",
    "            x = decoder_block(x, height, width, skip_connection=skip_connection)\n",
    "\n",
    "        return x\n",
    "\n",
    "class FCN(SegmentationModel):\n",
    "    \"\"\"\n",
    "    FCN is a fully convolutional neural network architecture designed for semantic image segmentation.\n",
    "\n",
    "    It consists of two main parts:\n",
    "\n",
    "    1. An encoder (downsampling path) that extracts increasingly abstract features\n",
    "    2. A decoder (upsampling path) that gradually recovers spatial details\n",
    "\n",
    "    The key is the use of skip connections between corresponding encoder and decoder layers.\n",
    "    These connections allow the decoder to access fine-grained details from earlier encoder layers,\n",
    "    which helps produce more precise segmentation masks.\n",
    "\n",
    "    The skip connections work by concatenating feature maps from the encoder directly into the decoder\n",
    "    at corresponding resolutions. This helps preserve important spatial information that would\n",
    "    otherwise be lost during the encoding process.\n",
    "\n",
    "    Args:\n",
    "        encoder_name: Name of the classification model that will be used as an encoder (a.k.a backbone)\n",
    "            to extract features of different spatial resolution\n",
    "        encoder_depth: A number of stages used in encoder in range [3, 5]. Each stage generate features\n",
    "            two times smaller in spatial dimensions than previous one (e.g. for depth 0 we will have features\n",
    "            with shapes [(N, C, H, W),], for depth 1 - [(N, C, H, W), (N, C, H // 2, W // 2)] and so on).\n",
    "            Default is 5\n",
    "        encoder_weights: One of **None** (random initialization), **\"imagenet\"** (pre-training on ImageNet) and\n",
    "            other pretrained weights (see table with available weights for each encoder_name)\n",
    "        decoder_channels: List of integers which specify **in_channels** parameter for convolutions used in decoder.\n",
    "            Length of the list should be the same as **encoder_depth**\n",
    "        decoder_use_norm:     Specifies normalization between Conv2D and activation.\n",
    "            Accepts the following types:\n",
    "            - **True**: Defaults to `\"batchnorm\"`.\n",
    "            - **False**: No normalization (`nn.Identity`).\n",
    "            - **str**: Specifies normalization type using default parameters. Available values:\n",
    "              `\"batchnorm\"`, `\"identity\"`, `\"layernorm\"`, `\"instancenorm\"`, `\"inplace\"`.\n",
    "            - **dict**: Fully customizable normalization settings. Structure:\n",
    "              ```python\n",
    "              {\"type\": <norm_type>, **kwargs}\n",
    "              ```\n",
    "              where `norm_name` corresponds to normalization type (see above), and `kwargs` are passed directly to the normalization layer as defined in PyTorch documentation.\n",
    "\n",
    "            **Example**:\n",
    "            ```python\n",
    "            decoder_use_norm={\"type\": \"layernorm\", \"eps\": 1e-2}\n",
    "            ```\n",
    "        decoder_attention_type: Attention module used in decoder of the model. Available options are\n",
    "            **None** and **scse** (https://arxiv.org/abs/1808.08127).\n",
    "        decoder_interpolation: Interpolation mode used in decoder of the model. Available options are\n",
    "            **\"nearest\"**, **\"bilinear\"**, **\"bicubic\"**, **\"area\"**, **\"nearest-exact\"**. Default is **\"nearest\"**.\n",
    "        in_channels: A number of input channels for the model, default is 3 (RGB images)\n",
    "        classes: A number of classes for output mask (or you can think as a number of channels of output mask)\n",
    "        activation: An activation function to apply after the final convolution layer.\n",
    "            Available options are **\"sigmoid\"**, **\"softmax\"**, **\"logsoftmax\"**, **\"tanh\"**, **\"identity\"**,\n",
    "            **callable** and **None**. Default is **None**.\n",
    "        aux_params: Dictionary with parameters of the auxiliary output (classification head). Auxiliary output is build\n",
    "            on top of encoder if **aux_params** is not **None** (default). Supported params:\n",
    "                - classes (int): A number of classes\n",
    "                - pooling (str): One of \"max\", \"avg\". Default is \"avg\"\n",
    "                - dropout (float): Dropout factor in [0, 1)\n",
    "                - activation (str): An activation function to apply \"sigmoid\"/\"softmax\"\n",
    "                    (could be **None** to return logits)\n",
    "        kwargs: Arguments passed to the encoder class ``__init__()`` function. Applies only to ``timm`` models. Keys with ``None`` values are pruned before passing.\n",
    "\n",
    "    Returns:\n",
    "        ``torch.nn.Module``: Unet\n",
    "\n",
    "    Example:\n",
    "        .. code-block:: python\n",
    "\n",
    "            import torch\n",
    "            import segmentation_models_pytorch as smp\n",
    "\n",
    "            model = smp.Unet(\"resnet18\", encoder_weights=\"imagenet\", classes=5)\n",
    "            model.eval()\n",
    "\n",
    "            # generate random images\n",
    "            images = torch.rand(2, 3, 256, 256)\n",
    "\n",
    "            with torch.inference_mode():\n",
    "                mask = model(images)\n",
    "\n",
    "            print(mask.shape)\n",
    "            # torch.Size([2, 5, 256, 256])\n",
    "\n",
    "    .. _Unet:\n",
    "        https://arxiv.org/abs/1505.04597\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    requires_divisible_input_shape = False\n",
    "\n",
    "    @supports_config_loading\n",
    "    def __init__(\n",
    "        self,\n",
    "        encoder_name: str = \"resnet34\",\n",
    "        encoder_depth: int = 5,\n",
    "        encoder_weights: Optional[str] = \"imagenet\",\n",
    "        decoder_use_norm: Union[bool, str, Dict[str, Any]] = \"batchnorm\",\n",
    "        decoder_last_channel: int = 16,\n",
    "        decoder_attention_type: Optional[str] = None,\n",
    "        decoder_interpolation: str = \"nearest\",\n",
    "        in_channels: int = 3,\n",
    "        classes: int = 1,\n",
    "        activation: Optional[Union[str, Callable]] = None,\n",
    "        aux_params: Optional[dict] = None,\n",
    "        **kwargs: dict[str, Any],\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        decoder_use_batchnorm = kwargs.pop(\"decoder_use_batchnorm\", None)\n",
    "        if decoder_use_batchnorm is not None:\n",
    "            warnings.warn(\n",
    "                \"The usage of decoder_use_batchnorm is deprecated. Please modify your code for decoder_use_norm\",\n",
    "                DeprecationWarning,\n",
    "                stacklevel=2,\n",
    "            )\n",
    "            decoder_use_norm = decoder_use_batchnorm\n",
    "\n",
    "        self.encoder = get_encoder(\n",
    "            encoder_name,\n",
    "            in_channels=in_channels,\n",
    "            depth=encoder_depth,\n",
    "            weights=encoder_weights,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        add_center_block = encoder_name.startswith(\"vgg\")\n",
    "\n",
    "        self.decoder = FCNDecoder(\n",
    "            encoder_channels=self.encoder.out_channels,\n",
    "            decoder_last_channel=decoder_last_channel,\n",
    "            n_blocks=encoder_depth,\n",
    "            use_norm=decoder_use_norm,\n",
    "            add_center_block=add_center_block,\n",
    "            attention_type=decoder_attention_type,\n",
    "            interpolation_mode=decoder_interpolation,\n",
    "        )\n",
    "\n",
    "        self.segmentation_head = SegmentationHead(\n",
    "            in_channels=decoder_last_channel,\n",
    "            out_channels=classes,\n",
    "            activation=activation,\n",
    "            kernel_size=3,\n",
    "        )\n",
    "\n",
    "        if aux_params is not None:\n",
    "            self.classification_head = ClassificationHead(\n",
    "                in_channels=decoder_last_channel, **aux_params\n",
    "            )\n",
    "        else:\n",
    "            self.classification_head = None\n",
    "\n",
    "        self.name = \"fcn-{}\".format(encoder_name)\n",
    "        self.initialize()\n",
    "\n",
    "#model = FCN(decoder_last_channel=32)\n",
    "#ret = model(torch.randn(1, 3, 224, 224))\n",
    "#ret.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4035171",
   "metadata": {},
   "source": [
    "# Фабрики для создания моделей по конфигурациям"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1960f571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def relace_input_layer(model:nn.Module, config:dict):\n",
    "    pass\n",
    "\n",
    "def create_weights_from_avg_ch(weight, new_in_channels):\n",
    "    return torch.cat([weight.mean(dim=1).unsqueeze(1)]*new_in_channels, dim=1)\n",
    "\n",
    "def cerate_weights_from_repeated_ch(weight, in_channels, new_in_channels):\n",
    "    ch_multiple = new_in_channels//in_channels\n",
    "    reminded_channels = new_in_channels%in_channels\n",
    "    # сначала набираем новые каналы путем подставления друг за другом (stack) каналов изначального изображения,\n",
    "    # а затем, если количество новых каналов не делится без остатка на количество изначальных, \n",
    "    # то набираем оставшиеся новые каналы из оставшихся изначальных    \n",
    "    new_weight = torch.cat(\n",
    "        [weight]*ch_multiple + [weight[:,:reminded_channels]], dim=1)\n",
    "    return new_weight\n",
    "\n",
    "def create_augmentation_transforms(transforms_dict:Dict[str, Dict]):\n",
    "    transforms_list = []\n",
    "    for name, transform_params in transforms_dict.items():\n",
    "        transform_creation_fn = transforms_factory_dict[name]\n",
    "        transforms_list.append(transform_creation_fn(**transform_params))\n",
    "    #return v2.Compose([v2.RandomOrder(transforms_list)])\n",
    "    return v2.RandomOrder(transforms_list)\n",
    "\n",
    "def create_model(config_dict, segmentation_nns_factory_dict):\n",
    "    model_name = config_dict['segmentation_nn']['nn_architecture']\n",
    "    if 'fpn' in model_name:\n",
    "        stride = config_dict['segmentation_nn']['input_layer_config']['params']['stride']\n",
    "        if isinstance(stride, (list, tuple)):\n",
    "            stride_val = stride[0]\n",
    "        elif isinstance(stride, (list, tuple)):\n",
    "            stride_val = stride\n",
    "        #if stride_val != 1:\n",
    "        config_dict['segmentation_nn']['params']['upsampling'] = stride_val\n",
    "    # создаем нейронную сеть из фабрики\n",
    "    model = segmentation_nns_factory_dict[model_name](**config_dict['segmentation_nn']['params'])\n",
    "    multispecter_bands_indices = config_dict['multispecter_bands_indices']\n",
    "    in_channels = len(multispecter_bands_indices)\n",
    "    # замена входного слоя, если кол-во каналов изображения не равно трем\n",
    "    input_conv = model.get_submodule(\n",
    "        config_dict['segmentation_nn']['input_layer_config']['layer_path']\n",
    "        )\n",
    "    if 'channels' in config_dict['segmentation_nn']['input_layer_config']['replace_type']:\n",
    "        new_input_conv = nn.Conv2d(\n",
    "            in_channels=in_channels,\n",
    "            out_channels=input_conv.out_channels,\n",
    "            kernel_size=input_conv.kernel_size,\n",
    "            #stride=conv1.stride,\n",
    "            stride=config_dict['segmentation_nn']['input_layer_config']['params']['stride'],\n",
    "            #padding=conv1.padding,\n",
    "            padding=config_dict['segmentation_nn']['input_layer_config']['params']['padding'],\n",
    "            dilation=input_conv.dilation,\n",
    "            groups=input_conv.groups,\n",
    "            bias=input_conv.bias is not None\n",
    "        )\n",
    "        if in_channels != 3:\n",
    "            # получаем входной слой, специфический для конкретной нейронной сети\n",
    "            \n",
    "            \n",
    "            if config_dict['segmentation_nn']['params']['encoder_weights'] is not None:\n",
    "                # выбор типа обнолвления весов\n",
    "                if config_dict['segmentation_nn']['input_layer_config']['weight_update_type'] == 'average_all':\n",
    "                    \n",
    "                    #new_weight = torch.cat([input_conv.weight.mean(dim=1).unsqueeze(1)]*in_channels, dim=1)\n",
    "                    new_weight = create_weights_from_avg_ch(input_conv.weight, in_channels)\n",
    "                    input_conv.weight = nn.Parameter(new_weight)\n",
    "\n",
    "                elif config_dict['segmentation_nn']['input_layer_config']['weight_update_type'] == 'repeate':\n",
    "                    '''\n",
    "                    ch_multiple = in_channels//input_conv.in_channels\n",
    "                    reminded_channels = in_channels%input_conv.in_channels\n",
    "                    new_weight = torch.cat(\n",
    "                        [input_conv.weight]*ch_multiple + [input_conv.weight[:,:reminded_channels]], dim=1)\n",
    "                    '''\n",
    "                    new_weight = cerate_weights_from_repeated_ch(input_conv.weight, input_conv.in_channels, in_channels)\n",
    "                    new_input_conv.weight = nn.Parameter(new_weight)\n",
    "        else:\n",
    "            # если у нас три канала на входе, то просто перезаписываем вес\n",
    "            new_input_conv.weight = nn.Parameter(input_conv.weight)\n",
    "\n",
    "        if input_conv.bias is not None:\n",
    "            new_input_conv.bias = input_conv.bias\n",
    "        # перезаписываем входной слой исходя из специфики оригинальной сети\n",
    "        model.set_submodule(\n",
    "                config_dict['segmentation_nn']['input_layer_config']['layer_path'],\n",
    "                new_input_conv\n",
    "                )\n",
    "\n",
    "    elif 'multisize_conv' in config_dict['segmentation_nn']['input_layer_config']['replace_type']:\n",
    "        multisize_params = config_dict['segmentation_nn']['input_layer_config']['params']\n",
    "        new_input_conv = MultisizeConv(**multisize_params)\n",
    "\n",
    "        # Если мы модифицируем входной слой.\n",
    "        if config_dict['segmentation_nn']['params']['encoder_weights'] is not None:\n",
    "            # вычленяем словрь с параметрами размеров ядер сверток.\n",
    "            kernel_sizes_dict = config_dict['segmentation_nn']['input_layer_config']['params']['kernel_size']\n",
    "            interpolated_kernels_dict = {}\n",
    "            # выполняем интерполяцию ядер свертки для каждого набора из новых ядер\n",
    "            for name, kernel_size in kernel_sizes_dict.items():\n",
    "                if isinstance(kernel_size, int):\n",
    "                    kernel_size = (kernel_size, kernel_size)\n",
    "                # получаем интерполированную версию ядер свертки\n",
    "                interpolated_kernels_dict[name] = [\n",
    "                    F.interpolate(input_conv.weight, size=kernel_size, mode='bicubic', antialias=True),\n",
    "                    input_conv.bias]\n",
    "                '''            \n",
    "                out_channels_dict = config_dict['segmentation_nn']['input_layer_config']['params']['out_channels']\n",
    "                for name, out_channels in out_channels_dict.items():\n",
    "                    weights = interpolated_kernels_dict[name][0]\n",
    "                    weights = create_weights_from_avg_ch(weights, in_channels)\n",
    "                    interpolated_kernels_dict[name][0] = weights\n",
    "                '''\n",
    "            #out_channels_dict = config_dict['segmentation_nn']['input_layer_config']['params']['out_channels']\n",
    "            for name in interpolated_kernels_dict.keys():\n",
    "                weights = interpolated_kernels_dict[name][0]\n",
    "                if config_dict['segmentation_nn']['input_layer_config']['weight_update_type'] == 'average_all':\n",
    "                    weights = create_weights_from_avg_ch(weights, new_in_channels=in_channels)\n",
    "                elif config_dict['segmentation_nn']['input_layer_config']['weight_update_type'] == 'repeat':\n",
    "                    weights = cerate_weights_from_repeated_ch(weights, in_channels=input_conv.in_channels, new_in_channels=in_channels)\n",
    "                \n",
    "                interpolated_kernels_dict[name][0] = weights\n",
    "                        \n",
    "            new_input_conv.update_weights(new_weights_dict=interpolated_kernels_dict)\n",
    "        if config_dict['segmentation_nn']['input_layer_config']['params']['aggregation_type'] == 'cat':\n",
    "            # Если тип агрегации выхода MultisizeConv - это конкатенация, то изменяем также второй сверточный слой,\n",
    "            # чтобы число его входных каналов соответствовало числу выходных первого слоя \n",
    "            raise NotImplementedError\n",
    "        # заменяем сходной слой по заранее определенному пути, который может варьировать в зависимости от архитектуры энкодера\n",
    "        model.set_submodule(\n",
    "                config_dict['segmentation_nn']['input_layer_config']['layer_path'],\n",
    "                new_input_conv\n",
    "                )\n",
    "    return model\n",
    "\n",
    "class MultisizeConv(nn.Module):\n",
    "    def __init__(\n",
    "            self,\n",
    "            in_channels:int,\n",
    "            out_channels:dict,\n",
    "            kernel_size:dict,\n",
    "            stride:dict,\n",
    "            padding:dict,\n",
    "            dilation:dict,\n",
    "            groups:dict,\n",
    "            bias:dict,\n",
    "            aggregation_type:str,\n",
    "            ):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.aggregation_type = aggregation_type\n",
    "        self.multisize_convs = nn.ModuleDict()\n",
    "        for conv_name in kernel_size.keys():\n",
    "            self.multisize_convs[conv_name] = nn.Conv2d(\n",
    "                in_channels=in_channels,\n",
    "                out_channels=out_channels[conv_name],\n",
    "                kernel_size=kernel_size[conv_name],\n",
    "                stride=stride[conv_name],\n",
    "                padding=padding[conv_name],\n",
    "                dilation=dilation[conv_name],\n",
    "                groups=groups[conv_name],\n",
    "                bias=bias[conv_name]\n",
    "                )\n",
    "        \n",
    "    def update_weights(self, new_weights_dict):\n",
    "        '''\n",
    "        На вход принимается словрь со структурой {'имя_свертки': (weight, bias)}\n",
    "        '''\n",
    "        for conv_name, (weight, bias) in new_weights_dict.items():\n",
    "            self.multisize_convs[conv_name].weight = nn.Parameter(weight)\n",
    "            if self.multisize_convs[conv_name].bias is not None:\n",
    "                self.multisize_convs[conv_name].bias = nn.Parameter(bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        outputs = []\n",
    "        for conv_name in self.multisize_convs.keys():\n",
    "            out = self.multisize_convs[conv_name](x)\n",
    "            #print(out.shape)\n",
    "            outputs.append(out)\n",
    "        \n",
    "        if self.aggregation_type == 'add':\n",
    "            outputs = torch.stack(outputs, dim=0)\n",
    "            outputs = outputs.sum(dim=0)\n",
    "        elif self.aggregation_type == 'cat':\n",
    "            outputs = torch.cat(outputs, dim=1)\n",
    "        else:\n",
    "            raise ValueError(f'self.aggregation_type should be either \"add\" or \"cat\". Got {self.aggregation_type}')\n",
    "        return outputs\n",
    "\n",
    "segmentation_nns_factory_dict = {\n",
    "    'unet': smp.Unet,\n",
    "    'att_unet': UnetAtt,\n",
    "    'fpn': smp.FPN,\n",
    "    'custom_fpn': FPNMod,\n",
    "    'unet++': UnetPlusPlusMod,\n",
    "    'fcn': FCN,\n",
    "    'custom_manet': MAnetMod,\n",
    "}\n",
    "\n",
    "unet_aggregation_factory_dict = {\n",
    "    'concat': ConcatDim1,\n",
    "    'conv_cross_att': ConvCrossAttentionBlock,\n",
    "    'cross-att': VisionTransformerBlock,\n",
    "    \n",
    "}\n",
    "\n",
    "pos_enc_factory_dict = {\n",
    "    'fixed_embeddings': FixedSizeLearnableEmbeddings,\n",
    "    'embedding_layer': EmbeddingLayer,\n",
    "    'none': nn.Identity\n",
    "}\n",
    "\n",
    "unet_attention_factory_dict = {\n",
    "    'conv_msa': ConvMSABlock,\n",
    "    'win_msa': LazyWindowVisionTransformerBlocks,\n",
    "    'none': nn.Identity,\n",
    "}\n",
    "\n",
    "criterion_factory_dict = {\n",
    "    'crossentropy': nn.CrossEntropyLoss,\n",
    "    'dice_crossentropy': DiceCELoss,\n",
    "    'dice': smp.losses.DiceLoss\n",
    "}\n",
    "\n",
    "optimizers_factory_dict = {\n",
    "    'adam': torch.optim.Adam,\n",
    "    'adamw': torch.optim.AdamW\n",
    "}\n",
    "\n",
    "lr_schedulers_factory_dict = {\n",
    "    'cosine_warm_restarts': torch.optim.lr_scheduler.CosineAnnealingWarmRestarts,\n",
    "    'plateau': torch.optim.lr_scheduler.ReduceLROnPlateau,\n",
    "    'cosine': torch.optim.lr_scheduler.CosineAnnealingLR,\n",
    "}\n",
    "\n",
    "transforms_factory_dict = {\n",
    "    'affine': v2.RandomAffine,\n",
    "    'perspective': v2.RandomPerspective,\n",
    "    'horizontal_flip': v2.RandomHorizontalFlip,\n",
    "    'vertical_flip': v2.RandomVerticalFlip,\n",
    "    'crop': v2.RandomCrop,\n",
    "    'gauss_noise': v2.GaussianNoise,\n",
    "    'gauss_blur': v2.GaussianBlur,\n",
    "    'elastic': v2.ElasticTransform,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "584d9632",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4680fb11",
   "metadata": {},
   "source": [
    "# Конфигурации"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "2291919a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'name_postfix': '',\n",
       " 'segmentation_nn': {'nn_architecture': 'custom_fpn',\n",
       "  'params': {'encoder_name': 'efficientnet-b0',\n",
       "   'encoder_depth': 5,\n",
       "   'encoder_weights': 'imagenet',\n",
       "   'decoder_pyramid_channels': 128,\n",
       "   'decoder_segmentation_channels': 128,\n",
       "   'decoder_merge_policy': 'add',\n",
       "   'decoder_dropout': 0.2,\n",
       "   'decoder_interpolation': 'nearest',\n",
       "   'in_channels': 3,\n",
       "   'classes': 11,\n",
       "   'activation': None,\n",
       "   'upsampling': 0,\n",
       "   'aux_params': None},\n",
       "  'input_layer_config': {'layer_path': 'encoder._conv_stem',\n",
       "   'replace_type': 'channels+stride',\n",
       "   'weight_update_type': 'average_all',\n",
       "   'params': {'stride': (1, 1), 'padding': (1, 1)}}},\n",
       " 'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
       " 'input_image_size': 96,\n",
       " 'loss': {'type': 'crossentropy',\n",
       "  'params': {'weight': None,\n",
       "   'ignore_index': -100,\n",
       "   'reduction': 'mean',\n",
       "   'label_smoothing': 0.15}},\n",
       " 'optimizer': {'type': 'adam', 'args': {}},\n",
       " 'lr_scheduler': {'type': 'cosine_warm_restarts',\n",
       "  'args': {'T_0': 25, 'T_mult': 1, 'eta_min': 0, 'last_epoch': -1},\n",
       "  'params': {'interval': 'epoch',\n",
       "   'frequency': 1,\n",
       "   'monitor': 'val_loss',\n",
       "   'strict': True,\n",
       "   'name': None}},\n",
       " 'device': 'cuda:0',\n",
       " 'batch_size': 16,\n",
       " 'epoch_num': 300,\n",
       " 'train_augmentations': {'gauss_noise': {'mean': 0.0,\n",
       "   'sigma': 0.0008,\n",
       "   'clip': False},\n",
       "  'affine': {'degrees': [0, 45],\n",
       "   'translate': [0, 0.3],\n",
       "   'scale': [0.7, 1.5],\n",
       "   'shear': [0, 0.2],\n",
       "   'fill': [0]},\n",
       "  'perspective': {'distortion_scale': 0.2, 'p': 0.3, 'fill': [0]},\n",
       "  'horizontal_flip': {'p': 0.5},\n",
       "  'vertical_flip': {'p': 0.5}},\n",
       " 'path_to_dataset_root': 'C:\\\\Users\\\\mokhail\\\\develop\\\\DATA\\\\DATA_FOR_TRAINIG_96'}"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unet_config_dict = {\n",
    "    'name_postfix': '',\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'unet',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': (256, 128, 128, 128, 128),\n",
    "            'decoder_attention_type': None,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96'\n",
    "}\n",
    "\n",
    "win_att1l_cat_agg_2_conv_unet_config_dict = {\n",
    "    'name_postfix': '1L_win_cat_agg',\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'att_unet',\n",
    "        'params': {\n",
    "            'decoder_layers_configs': [\n",
    "                {\n",
    "                    'interpolation_mode': \"nearest\",\n",
    "                    'aggregation_layer':{\n",
    "                        'layer': 'concat',\n",
    "                        'params': {},\n",
    "                    },\n",
    "                    'attention1': {\n",
    "                        'layer':'none',\n",
    "                        'params':{},\n",
    "                    },\n",
    "                    'conv': [\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                    ],\n",
    "                    'attention2': {\n",
    "                        'layer':'win_msa',\n",
    "                        'params':{\n",
    "                            'rows_in_win':6,\n",
    "                            'cols_in_win':6,\n",
    "                            'layer_num':1,\n",
    "                            'num_heads':8,\n",
    "                            'transformer_mlp_dim':8,\n",
    "                            'attention_dropout':0.2,\n",
    "                            'dropout':0.2,\n",
    "                            'positional_encoding_block':'embedding_layer',\n",
    "                            'positional_encoding_block_params':{},\n",
    "                            'transformer_block':VitEncoderBlock,\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    'interpolation_mode': \"nearest\",\n",
    "                    'aggregation_layer':{\n",
    "                        'layer': 'concat',\n",
    "                        'params': {},\n",
    "                    },\n",
    "                    'attention1': {\n",
    "                        'layer':'none',\n",
    "                        'params':{},\n",
    "                    },\n",
    "                    'conv': [\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                    ],\n",
    "                    'attention2': {\n",
    "                        'layer':'win_msa',\n",
    "                        'params':{\n",
    "                            'rows_in_win':12,\n",
    "                            'cols_in_win':12,\n",
    "                            'layer_num':1,\n",
    "                            'num_heads':8,\n",
    "                            'transformer_mlp_dim':8,\n",
    "                            'attention_dropout':0.2,\n",
    "                            'dropout':0.2,\n",
    "                            'positional_encoding_block':'embedding_layer',\n",
    "                            'positional_encoding_block_params':{},\n",
    "                            'transformer_block':VitEncoderBlock,\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    'interpolation_mode': \"nearest\",\n",
    "                    'aggregation_layer':{\n",
    "                        'layer': 'concat',\n",
    "                        'params': {},\n",
    "                    },\n",
    "                    'attention1': {\n",
    "                        'layer':'none',\n",
    "                        'params':{},\n",
    "                    },\n",
    "                    'conv': [\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                    ],\n",
    "                    'attention2': {\n",
    "                        'layer':'win_msa',\n",
    "                        'params':{\n",
    "                            'rows_in_win':24,\n",
    "                            'cols_in_win':24,\n",
    "                            'layer_num':1,\n",
    "                            'num_heads':8,\n",
    "                            'transformer_mlp_dim':8,\n",
    "                            'attention_dropout':0.2,\n",
    "                            'dropout':0.2,\n",
    "                            'positional_encoding_block':'embedding_layer',\n",
    "                            'positional_encoding_block_params':{},\n",
    "                            'transformer_block':VitEncoderBlock,\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    'interpolation_mode': \"nearest\",\n",
    "                    'aggregation_layer':{\n",
    "                        'layer': 'concat',\n",
    "                        'params': {},\n",
    "                    },\n",
    "                    'attention1': {\n",
    "                        'layer':'none',\n",
    "                        'params':{},\n",
    "                    },\n",
    "                    'conv': [\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                    ],\n",
    "                    'attention2': {\n",
    "                        'layer':'win_msa',\n",
    "                        'params':{\n",
    "                            'rows_in_win':24,\n",
    "                            'cols_in_win':24,\n",
    "                            'layer_num':1,\n",
    "                            'num_heads':8,\n",
    "                            'transformer_mlp_dim':8,\n",
    "                            'attention_dropout':0.2,\n",
    "                            'dropout':0.2,\n",
    "                            'positional_encoding_block':'embedding_layer',\n",
    "                            'positional_encoding_block_params':{},\n",
    "                            'transformer_block':VitEncoderBlock,\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    'interpolation_mode': \"nearest\",\n",
    "                    'aggregation_layer':{\n",
    "                        'layer': 'concat',\n",
    "                        'params': {},\n",
    "                    },\n",
    "                    'attention1': {\n",
    "                        'layer':'none',\n",
    "                        'params':{},\n",
    "                    },\n",
    "                    'conv': [\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                    ],\n",
    "                    'attention2': {\n",
    "                        'layer':'win_msa',\n",
    "                        'params':{\n",
    "                            'rows_in_win':24,\n",
    "                            'cols_in_win':24,\n",
    "                            'layer_num':1,\n",
    "                            'num_heads':8,\n",
    "                            'transformer_mlp_dim':8,\n",
    "                            'attention_dropout':0.2,\n",
    "                            'dropout':0.2,\n",
    "                            'positional_encoding_block':'embedding_layer',\n",
    "                            'positional_encoding_block_params':{},\n",
    "                            'transformer_block':VitEncoderBlock,\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "                \n",
    "            ],\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': (256, 128, 128, 64, 64),\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 4,\n",
    "    'epoch_num':4,\n",
    "    'train_augmentations': {\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    #'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96'    \n",
    "}\n",
    "\n",
    "win_att_cat_agg_2_conv_unet_config_dict = {\n",
    "    'name_postfix': 'win_cat_agg',\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'att_unet',\n",
    "        'params': {\n",
    "            'decoder_layers_configs': [\n",
    "                {\n",
    "                    'interpolation_mode': \"nearest\",\n",
    "                    'aggregation_layer':{\n",
    "                        'layer': 'concat',\n",
    "                        'params': {},\n",
    "                    },\n",
    "                    'attention1': {\n",
    "                        'layer':'win_msa',\n",
    "                        'params':{\n",
    "                            'rows_in_win':6,\n",
    "                            'cols_in_win':6,\n",
    "                            'layer_num':1,\n",
    "                            'num_heads':8,\n",
    "                            'transformer_mlp_dim':8,\n",
    "                            'attention_dropout':0.2,\n",
    "                            'dropout':0.2,\n",
    "                            'positional_encoding_block':'embedding_layer',\n",
    "                            'positional_encoding_block_params':{},\n",
    "                            'transformer_block':VitEncoderBlock,\n",
    "                        },\n",
    "                    },\n",
    "                    'conv': [\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                    ],\n",
    "                    'attention2': {\n",
    "                        'layer':'win_msa',\n",
    "                        'params':{\n",
    "                            'rows_in_win':6,\n",
    "                            'cols_in_win':6,\n",
    "                            'layer_num':1,\n",
    "                            'num_heads':8,\n",
    "                            'transformer_mlp_dim':8,\n",
    "                            'attention_dropout':0.2,\n",
    "                            'dropout':0.2,\n",
    "                            'positional_encoding_block':'embedding_layer',\n",
    "                            'positional_encoding_block_params':{},\n",
    "                            'transformer_block':VitEncoderBlock,\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    'interpolation_mode': \"nearest\",\n",
    "                    'aggregation_layer':{\n",
    "                        'layer': 'concat',\n",
    "                        'params': {},\n",
    "                    },\n",
    "                    'attention1': {\n",
    "                        'layer':'win_msa',\n",
    "                        'params':{\n",
    "                            'rows_in_win':12,\n",
    "                            'cols_in_win':12,\n",
    "                            'layer_num':1,\n",
    "                            'num_heads':8,\n",
    "                            'transformer_mlp_dim':8,\n",
    "                            'attention_dropout':0.2,\n",
    "                            'dropout':0.2,\n",
    "                            'positional_encoding_block':'embedding_layer',\n",
    "                            'positional_encoding_block_params':{},\n",
    "                            'transformer_block':VitEncoderBlock,\n",
    "                        },\n",
    "                    },\n",
    "                    'conv': [\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                    ],\n",
    "                    'attention2': {\n",
    "                        'layer':'win_msa',\n",
    "                        'params':{\n",
    "                            'rows_in_win':12,\n",
    "                            'cols_in_win':12,\n",
    "                            'layer_num':1,\n",
    "                            'num_heads':8,\n",
    "                            'transformer_mlp_dim':8,\n",
    "                            'attention_dropout':0.2,\n",
    "                            'dropout':0.2,\n",
    "                            'positional_encoding_block':'embedding_layer',\n",
    "                            'positional_encoding_block_params':{},\n",
    "                            'transformer_block':VitEncoderBlock,\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    'interpolation_mode': \"nearest\",\n",
    "                    'aggregation_layer':{\n",
    "                        'layer': 'concat',\n",
    "                        'params': {},\n",
    "                    },\n",
    "                    'attention1': {\n",
    "                        'layer':'win_msa',\n",
    "                        'params':{\n",
    "                            'rows_in_win':24,\n",
    "                            'cols_in_win':24,\n",
    "                            'layer_num':1,\n",
    "                            'num_heads':8,\n",
    "                            'transformer_mlp_dim':8,\n",
    "                            'attention_dropout':0.2,\n",
    "                            'dropout':0.2,\n",
    "                            'positional_encoding_block':'embedding_layer',\n",
    "                            'positional_encoding_block_params':{},\n",
    "                            'transformer_block':VitEncoderBlock,\n",
    "                        },\n",
    "                    },\n",
    "                    'conv': [\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                    ],\n",
    "                    'attention2': {\n",
    "                        'layer':'win_msa',\n",
    "                        'params':{\n",
    "                            'rows_in_win':24,\n",
    "                            'cols_in_win':24,\n",
    "                            'layer_num':1,\n",
    "                            'num_heads':8,\n",
    "                            'transformer_mlp_dim':8,\n",
    "                            'attention_dropout':0.2,\n",
    "                            'dropout':0.2,\n",
    "                            'positional_encoding_block':'embedding_layer',\n",
    "                            'positional_encoding_block_params':{},\n",
    "                            'transformer_block':VitEncoderBlock,\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    'interpolation_mode': \"nearest\",\n",
    "                    'aggregation_layer':{\n",
    "                        'layer': 'concat',\n",
    "                        'params': {},\n",
    "                    },\n",
    "                    'attention1': {\n",
    "                        'layer':'win_msa',\n",
    "                        'params':{\n",
    "                            'rows_in_win':24,\n",
    "                            'cols_in_win':24,\n",
    "                            'layer_num':1,\n",
    "                            'num_heads':8,\n",
    "                            'transformer_mlp_dim':8,\n",
    "                            'attention_dropout':0.2,\n",
    "                            'dropout':0.2,\n",
    "                            'positional_encoding_block':'embedding_layer',\n",
    "                            'positional_encoding_block_params':{},\n",
    "                            'transformer_block':VitEncoderBlock,\n",
    "                        },\n",
    "                    },\n",
    "                    'conv': [\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                    ],\n",
    "                    'attention2': {\n",
    "                        'layer':'win_msa',\n",
    "                        'params':{\n",
    "                            'rows_in_win':24,\n",
    "                            'cols_in_win':24,\n",
    "                            'layer_num':1,\n",
    "                            'num_heads':8,\n",
    "                            'transformer_mlp_dim':8,\n",
    "                            'attention_dropout':0.2,\n",
    "                            'dropout':0.2,\n",
    "                            'positional_encoding_block':'embedding_layer',\n",
    "                            'positional_encoding_block_params':{},\n",
    "                            'transformer_block':VitEncoderBlock,\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "                {\n",
    "                    'interpolation_mode': \"nearest\",\n",
    "                    'aggregation_layer':{\n",
    "                        'layer': 'concat',\n",
    "                        'params': {},\n",
    "                    },\n",
    "                    'attention1': {\n",
    "                        'layer':'win_msa',\n",
    "                        'params':{\n",
    "                            'rows_in_win':24,\n",
    "                            'cols_in_win':24,\n",
    "                            'layer_num':1,\n",
    "                            'num_heads':8,\n",
    "                            'transformer_mlp_dim':8,\n",
    "                            'attention_dropout':0.2,\n",
    "                            'dropout':0.2,\n",
    "                            'positional_encoding_block':'embedding_layer',\n",
    "                            'positional_encoding_block_params':{},\n",
    "                            'transformer_block':VitEncoderBlock,\n",
    "                        },\n",
    "                    },\n",
    "                    'conv': [\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                    ],\n",
    "                    'attention2': {\n",
    "                        'layer':'win_msa',\n",
    "                        'params':{\n",
    "                            'rows_in_win':24,\n",
    "                            'cols_in_win':24,\n",
    "                            'layer_num':1,\n",
    "                            'num_heads':8,\n",
    "                            'transformer_mlp_dim':8,\n",
    "                            'attention_dropout':0.2,\n",
    "                            'dropout':0.2,\n",
    "                            'positional_encoding_block':'embedding_layer',\n",
    "                            'positional_encoding_block_params':{},\n",
    "                            'transformer_block':VitEncoderBlock,\n",
    "                        },\n",
    "                    },\n",
    "                },\n",
    "                \n",
    "            ],\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': (256, 128, 128, 64, 64),\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 4,\n",
    "    'epoch_num':4,\n",
    "    'train_augmentations': {\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    #'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96'    \n",
    "}\n",
    "\n",
    "att_cat_agg_2_conv_unet_config_dict = {\n",
    "    'name_postfix': 'cat_agg_2conv',\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'att_unet',\n",
    "        'params': {\n",
    "            'decoder_layers_configs': [\n",
    "                {\n",
    "                    'interpolation_mode': \"nearest\",\n",
    "                    'aggregation_layer':{\n",
    "                        'layer': 'concat',\n",
    "                        'params': {},\n",
    "                    },\n",
    "                    'attention1': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'conv': [\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                    ],\n",
    "                    'attention2': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'interpolation_mode': \"nearest\",\n",
    "                    'aggregation_layer':{\n",
    "                        'layer': 'conv_cross_att',\n",
    "                        'params': {\n",
    "                            'in_kernel_size':3,\n",
    "                            'in_padding':1,\n",
    "                            'in_stride':1,\n",
    "                            \"head_row_dim\":6,\n",
    "                            'head_col_dim':6,\n",
    "                            'head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'attention1': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'conv': [\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                    ],\n",
    "                    'attention2': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'interpolation_mode': \"nearest\",\n",
    "                    'aggregation_layer':{\n",
    "                        'layer': 'conv_cross_att',\n",
    "                        'params': {\n",
    "                            'in_kernel_size':3,\n",
    "                            'in_padding':1,\n",
    "                            'in_stride':1,\n",
    "                            \"head_row_dim\":6,\n",
    "                            'head_col_dim':6,\n",
    "                            'head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'attention1': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'conv': [\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                    ],\n",
    "                    'attention2': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'interpolation_mode': \"nearest\",\n",
    "                    'aggregation_layer':{\n",
    "                        'layer': 'conv_cross_att',\n",
    "                        'params': {\n",
    "                            'in_kernel_size':3,\n",
    "                            'in_padding':1,\n",
    "                            'in_stride':1,\n",
    "                            \"head_row_dim\":6,\n",
    "                            'head_col_dim':6,\n",
    "                            'head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'attention1': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'conv': [\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                    ],\n",
    "                    'attention2': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'interpolation_mode': \"nearest\",\n",
    "                    'aggregation_layer':{\n",
    "                        'layer': 'conv_cross_att',\n",
    "                        'params': {\n",
    "                            'in_kernel_size':3,\n",
    "                            'in_padding':1,\n",
    "                            'in_stride':1,\n",
    "                            \"head_row_dim\":6,\n",
    "                            'head_col_dim':6,\n",
    "                            'head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'attention1': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'conv': [\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                    ],\n",
    "                    'attention2': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    }\n",
    "                },\n",
    "            ],\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': (256, 128, 128, 64, 64),\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 4,\n",
    "    'epoch_num':4,\n",
    "    'train_augmentations': {\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    #'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96'    \n",
    "}\n",
    "\n",
    "att_cross_agg_2_conv_unet_config_dict = {\n",
    "    'name_postfix': 'cross_agg_2conv',\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'att_unet',\n",
    "        'params': {\n",
    "            'decoder_layers_configs': [\n",
    "                {\n",
    "                    'interpolation_mode': \"nearest\",\n",
    "                    'aggregation_layer':{\n",
    "                        'layer': 'conv_cross_att',\n",
    "                        'params': {\n",
    "                            'in_kernel_size':3,\n",
    "                            'in_padding':1,\n",
    "                            'in_stride':1,\n",
    "                            \"head_row_dim\":6,\n",
    "                            'head_col_dim':6,\n",
    "                            'head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'attention1': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'conv': [\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                    ],\n",
    "                    'attention2': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'interpolation_mode': \"nearest\",\n",
    "                    'aggregation_layer':{\n",
    "                        'layer': 'conv_cross_att',\n",
    "                        'params': {\n",
    "                            'in_kernel_size':3,\n",
    "                            'in_padding':1,\n",
    "                            'in_stride':1,\n",
    "                            \"head_row_dim\":6,\n",
    "                            'head_col_dim':6,\n",
    "                            'head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'attention1': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'conv': [\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                    ],\n",
    "                    'attention2': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'interpolation_mode': \"nearest\",\n",
    "                    'aggregation_layer':{\n",
    "                        'layer': 'conv_cross_att',\n",
    "                        'params': {\n",
    "                            'in_kernel_size':3,\n",
    "                            'in_padding':1,\n",
    "                            'in_stride':1,\n",
    "                            \"head_row_dim\":6,\n",
    "                            'head_col_dim':6,\n",
    "                            'head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'attention1': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'conv': [\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                    ],\n",
    "                    'attention2': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'interpolation_mode': \"nearest\",\n",
    "                    'aggregation_layer':{\n",
    "                        'layer': 'conv_cross_att',\n",
    "                        'params': {\n",
    "                            'in_kernel_size':3,\n",
    "                            'in_padding':1,\n",
    "                            'in_stride':1,\n",
    "                            \"head_row_dim\":6,\n",
    "                            'head_col_dim':6,\n",
    "                            'head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'attention1': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'conv': [\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                    ],\n",
    "                    'attention2': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    }\n",
    "                },\n",
    "                {\n",
    "                    'interpolation_mode': \"nearest\",\n",
    "                    'aggregation_layer':{\n",
    "                        'layer': 'conv_cross_att',\n",
    "                        'params': {\n",
    "                            'in_kernel_size':3,\n",
    "                            'in_padding':1,\n",
    "                            'in_stride':1,\n",
    "                            \"head_row_dim\":6,\n",
    "                            'head_col_dim':6,\n",
    "                            'head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'attention1': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    },\n",
    "                    'conv': [\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                        {\n",
    "                            'kernel_size': 3,\n",
    "                            'stride':  1,\n",
    "                            'padding': 1,\n",
    "                            'groups': 1,\n",
    "                            'norm_layer': nn.BatchNorm2d,\n",
    "                            'activation_layer': nn.ReLU,\n",
    "                            'dilation': 1,\n",
    "                            'inplace': True,\n",
    "                            'bias': True,\n",
    "                        },\n",
    "                    ],\n",
    "                    'attention2': {\n",
    "                        'layer':'conv_msa',\n",
    "                        'params':{\n",
    "                            'msa_in_kernel_size':3,\n",
    "                            'msa_in_padding':1,\n",
    "                            'msa_in_stride':1,\n",
    "                            'msa_head_row_dim':6,\n",
    "                            'msa_head_col_dim':6,\n",
    "                            'msa_head_ch_dim':4,\n",
    "                            'dropout':0.2,\n",
    "                            'out_conv_hidden_channels':512,\n",
    "                            'out_conv_kernel_size':1,\n",
    "                            'out_conv_padding':0,\n",
    "                            'out_conv_stride':1,\n",
    "                            'out_conv_act':nn.SiLU,\n",
    "                            'norm_layer':nn.BatchNorm2d,\n",
    "                        },\n",
    "                    }\n",
    "                },\n",
    "            ],\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': (256, 128, 128, 64, 64),\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 8,\n",
    "    'epoch_num': 300,\n",
    "    'train_augmentations': {\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    #'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96'    \n",
    "}\n",
    "\n",
    "unetpp_config_dict = {\n",
    "    'name_postfix': '',\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'unet++',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': [256, 128, 128, 128, 128],\n",
    "            'decoder_attention_type': None,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "       'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96'\n",
    "}\n",
    "\n",
    "scse_unet_config_dict = {\n",
    "    'name_postfix': 'scse_att',\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'unet',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': (256, 128, 128, 128, 128),\n",
    "            'decoder_attention_type': 'scse',\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'average_all',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "       'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96'\n",
    "}\n",
    "\n",
    "unet_mit_config_dict = {\n",
    "    'name_postfix': '',\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'unet',\n",
    "        'params': {\n",
    "            'encoder_name': \"mit_b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': (256, 128, 128, 128, 128),\n",
    "            'decoder_attention_type': None,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder.patch_embed1.proj',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'average_all',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (3, 3),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "       'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96'\n",
    "}\n",
    "\n",
    "unet_maxvit_config_dict = {\n",
    "    'name_postfix': '',\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'unet',\n",
    "        'params': {\n",
    "            'encoder_name': 'tu-maxvit_tiny_rw_224',\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': (256, 128, 128, 128, 128),\n",
    "            'decoder_attention_type': None,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "            'img_size':96,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder.model.stem.conv1',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'average_all',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "       'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    #'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    'path_to_dataset_root': r'/home/mikhail_u/develop/DATA/DATA_FOR_TRAINIG_96'\n",
    "}\n",
    "\n",
    "net_hgnetv2_config_dict = {\n",
    "    'name_postfix': '',\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'unet',\n",
    "        'params': {\n",
    "            'encoder_name': 'tu-hgnetv2_b1',\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': (256, 128, 128, 128, 128),\n",
    "            'decoder_attention_type': None,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "            #'img_size':96,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder.model.stem.stem1.conv',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'average_all',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "       'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    #'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    'path_to_dataset_root': r'/home/mikhail_u/develop/DATA/DATA_FOR_TRAINIG_96'\n",
    "}\n",
    "\n",
    "net_mambaout_config_dict = {\n",
    "    'name_postfix': '',\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'unet',\n",
    "        'params': {\n",
    "            'encoder_name': 'tu-mambaout_small',\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': (256, 128, 128, 128, 128),\n",
    "            'decoder_attention_type': None,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "            #'img_size':96,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder.model.stem.conv1',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'average_all',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "       'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    #'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    'path_to_dataset_root': r'/home/mikhail_u/develop/DATA/DATA_FOR_TRAINIG_96'\n",
    "}\n",
    "\n",
    "unet_hrnet_config_dict = {\n",
    "    'name_postfix': '',\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'unet',\n",
    "        'params': {\n",
    "            'encoder_name': 'tu-hrnet_w18',\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': (256, 128, 128, 128, 128),\n",
    "            'decoder_attention_type': None,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "            #'img_size':96,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder.model.conv1',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'average_all',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "       'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    #'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    'path_to_dataset_root': r'/home/mikhail_u/develop/DATA/DATA_FOR_TRAINIG_96'\n",
    "}\n",
    "\n",
    "unet_cspresnext_config_dict = {\n",
    "    'name_postfix': '',\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'unet',\n",
    "        'params': {\n",
    "            'encoder_name': 'tu-cspresnext50',\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': (256, 128, 128, 128, 128),\n",
    "            'decoder_attention_type': None,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "            #'img_size':96,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder.model.stem_conv1.conv',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'average_all',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                }\n",
    "        }\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'}\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "       'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'/home/mikhail_u/develop/DATA/DATA_FOR_TRAINIG_96'\n",
    "}\n",
    "\n",
    "fpn_config_dict = {\n",
    "    'name_postfix': '',\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'custom_fpn',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_pyramid_channels': 128,\n",
    "            'decoder_segmentation_channels': 128,\n",
    "            'decoder_merge_policy': \"add\",\n",
    "            'decoder_dropout': 0.2,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'upsampling': 0,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                },\n",
    "        },\n",
    "    },\n",
    "    'multispecter_bands_indices': [1, 2, 3, 7, 'ndvi'],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'},\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "       'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "}\n",
    "\n",
    "fcn_config_dict = {\n",
    "    'name_postfix': '',\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'fcn',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': 'batchnorm',\n",
    "            'decoder_last_channel': 16,\n",
    "            'decoder_attention_type': None,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation':  None,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                },\n",
    "        },\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "        #'params': {},\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "       'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "}\n",
    "\n",
    "fpn_config_dict_dice_ce = {\n",
    "    'name_postfix': '',\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'custom_fpn',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_pyramid_channels': 128,\n",
    "            'decoder_segmentation_channels': 128,\n",
    "            'decoder_merge_policy': \"add\",\n",
    "            'decoder_dropout': 0.2,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'upsampling': 0,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                },\n",
    "        },\n",
    "    },\n",
    "    'multispecter_bands_indices': [1, 2, 3, 7, 'ndvi'],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'dice_crossentropy',\n",
    "        #'params': {'weight': 'classes'},\n",
    "        'params': {\n",
    "            'ce_weight':None,\n",
    "            'ce_ignore_index':-100,\n",
    "            'ce_reducion':'mean',\n",
    "            'ce_label_smoothing':0.15,\n",
    "            'dice_mode':'multiclass',\n",
    "            'dice_classes': None,\n",
    "            'dice_log_loss':False,\n",
    "            'dice_from_logits':True,\n",
    "            'dice_smooth':0.15,\n",
    "            'dice_ignore_index':-100,\n",
    "            'dice_eps': 1e-7,\n",
    "            'losses_weight': [0.5, 0.5],\n",
    "            'is_trainable_weights': True,\n",
    "            'weights_processing_type': 'softmax',\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "       'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "}\n",
    "\n",
    "fpn_config_dict_dice = {\n",
    "    'name_postfix': '',\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'custom_fpn',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_pyramid_channels': 128,\n",
    "            'decoder_segmentation_channels': 128,\n",
    "            'decoder_merge_policy': \"add\",\n",
    "            'decoder_dropout': 0.2,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'upsampling': 0,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                },\n",
    "        },\n",
    "    },\n",
    "    'multispecter_bands_indices': [1, 2, 3, 7, 'ndvi'],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'dice',\n",
    "        #'params': {'weight': 'classes'},\n",
    "        'params': {\n",
    "            'mode':'multiclass',\n",
    "            'classes': None,\n",
    "            'log_loss':False,\n",
    "            'from_logits':True,\n",
    "            'smooth':0.15,\n",
    "            'ignore_index':-100,\n",
    "            'eps': 1e-7,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "       'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "}\n",
    "\n",
    "fpn_config_dict_ce = {\n",
    "    'name_postfix': '',\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'custom_fpn',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_pyramid_channels': 128,\n",
    "            'decoder_segmentation_channels': 128,\n",
    "            'decoder_merge_policy': \"add\",\n",
    "            'decoder_dropout': 0.2,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'upsampling': 0,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'average_all',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                },\n",
    "        },\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'},\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size':16,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "       'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    #'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "}\n",
    "\n",
    "fpn_multisize_input_config_dict_ce = {\n",
    "    'name_postfix': '',\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'custom_fpn',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_pyramid_channels': 128,\n",
    "            'decoder_segmentation_channels': 128,\n",
    "            'decoder_merge_policy': \"add\",\n",
    "            'decoder_dropout': 0.2,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'upsampling': 0,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'multisize_conv',\n",
    "            'weight_update_type': 'repeat', # avearge_all OR repeate\n",
    "            'params':{\n",
    "                'in_channels': 3,\n",
    "                'out_channels': {\n",
    "                    '1x1': 32,\n",
    "                    '3x3': 32,\n",
    "                    #'5x5': 32, \n",
    "                },\n",
    "                'kernel_size': {\n",
    "                    '1x1': 1,\n",
    "                    '3x3': 3,\n",
    "                    #'5x5': 5, \n",
    "                },\n",
    "                'stride': {\n",
    "                    '1x1': 1,\n",
    "                    '3x3': 1,\n",
    "                    #'5x5': 1, \n",
    "                },\n",
    "                'padding': {\n",
    "                    '1x1': 0,\n",
    "                    '3x3': 1,\n",
    "                    #'5x5': 2, \n",
    "                },\n",
    "                'dilation': {\n",
    "                    '1x1': 1,\n",
    "                    '3x3': 1,\n",
    "                    #'5x5': 1, \n",
    "                },\n",
    "                'groups': {\n",
    "                    '1x1': 1,\n",
    "                    '3x3': 1,\n",
    "                    #'5x5': 1, \n",
    "                },\n",
    "                'bias': {\n",
    "                    '1x1': False,\n",
    "                    '3x3': False,\n",
    "                    #'5x5': False, \n",
    "                },\n",
    "                'aggregation_type': 'add',\n",
    "            },\n",
    "        },\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'},\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "       'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    #'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "}\n",
    "\n",
    "fpn_mit_config_dict_ce = {\n",
    "    'name_postfix': '',\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'custom_fpn',\n",
    "        'params': {\n",
    "            'encoder_name': \"mit_b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_pyramid_channels': 128,\n",
    "            'decoder_segmentation_channels': 128,\n",
    "            'decoder_merge_policy': \"add\",\n",
    "            'decoder_dropout': 0.2,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'upsampling': 0,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                },\n",
    "        },\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'},\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size': 16,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "       'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "}\n",
    "\n",
    "custom_manet_config_dict = {\n",
    "    'name_postfix': '',\n",
    "    'segmentation_nn': {\n",
    "        'nn_architecture': 'custom_manet',\n",
    "        'params': {\n",
    "            'encoder_name': \"efficientnet-b0\",\n",
    "            'encoder_depth': 5,\n",
    "            'encoder_weights': \"imagenet\",\n",
    "            'decoder_use_norm': \"batchnorm\",\n",
    "            'decoder_channels': [256, 128, 64, 64, 64],\n",
    "            'decoder_pab_channels': 64,\n",
    "            'decoder_interpolation': \"nearest\",\n",
    "            'in_channels': 3,\n",
    "            'classes': 11,\n",
    "            'activation': None,\n",
    "            'aux_params': None,\n",
    "        },\n",
    "        'input_layer_config': {\n",
    "            'layer_path': 'encoder._conv_stem',\n",
    "            'replace_type': 'channels+stride',\n",
    "            'weight_update_type': 'repeate',\n",
    "            'params':{\n",
    "                'stride': (1, 1),\n",
    "                'padding': (1, 1),\n",
    "                },\n",
    "        },\n",
    "    },\n",
    "    'multispecter_bands_indices': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12,],\n",
    "    'input_image_size': 96,\n",
    "    'loss': {\n",
    "        'type': 'crossentropy',\n",
    "        #'params': {'weight': 'classes'},\n",
    "        'params': {\n",
    "            'weight': None,\n",
    "            'ignore_index': -100,\n",
    "            'reduction': \"mean\",\n",
    "            'label_smoothing': 0.15,\n",
    "            },\n",
    "    },\n",
    "    'optimizer': {\n",
    "        'type': 'adam',\n",
    "        'args': {}\n",
    "    },\n",
    "    'lr_scheduler': {\n",
    "        'type': 'cosine_warm_restarts',\n",
    "        'args': {\n",
    "            'T_0': 25,\n",
    "            'T_mult': 1,\n",
    "            'eta_min': 0,\n",
    "            'last_epoch': -1\n",
    "        },\n",
    "        'params':{\n",
    "            'interval': 'epoch',\n",
    "            'frequency': 1,\n",
    "            'monitor': 'val_loss',\n",
    "            'strict': True,\n",
    "            'name': None,\n",
    "        },\n",
    "    },\n",
    "    'device': 'cuda:0',\n",
    "    'batch_size':8,\n",
    "    'epoch_num':300,\n",
    "    'train_augmentations': {\n",
    "       'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "    },\n",
    "    'path_to_dataset_root': r'C:\\Users\\admin\\python_programming\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "    #'path_to_dataset_root': r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96',\n",
    "}\n",
    "\n",
    "config_dict = deepcopy(fpn_config_dict_ce)\n",
    "\n",
    "config_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ae8eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "configs_to_save = [\n",
    "    unet_config_dict,\n",
    "    win_att1l_cat_agg_2_conv_unet_config_dict,\n",
    "    win_att_cat_agg_2_conv_unet_config_dict,\n",
    "    att_cross_agg_2_conv_unet_config_dict,\n",
    "    att_cat_agg_2_conv_unet_config_dict,\n",
    "    unetpp_config_dict,\n",
    "    unet_mit_config_dict,\n",
    "    unet_maxvit_config_dict,\n",
    "    net_hgnetv2_config_dict,\n",
    "    net_mambaout_config_dict,\n",
    "    unet_hrnet_config_dict,\n",
    "    unet_cspresnext_config_dict,\n",
    "    fpn_config_dict,\n",
    "    fcn_config_dict,\n",
    "    fpn_config_dict_dice_ce,\n",
    "    fpn_config_dict_dice,\n",
    "    fpn_config_dict_ce,\n",
    "    fpn_multisize_input_config_dict_ce,\n",
    "    custom_manet_config_dict,\n",
    "]\n",
    "for config_dict in configs_to_save:\n",
    "    name_postfix = config_dict[\"name_postfix\"]\n",
    "    model_name = f'{config_dict[\"segmentation_nn\"][\"nn_architecture\"]}_{config_dict[\"segmentation_nn\"][\"params\"][\"encoder_name\"]}'\n",
    "    if name_postfix is not None and len(name_postfix) != 0:\n",
    "        model_name = f'{model_name}_{name_postfix}'\n",
    "        \n",
    "    path_to_save = os.path.join('training_configs', f'{model_name}.yaml')\n",
    "    with open(path_to_save, 'w', encoding='utf-8') as fd:\n",
    "        yaml.dump(config_dict, fd, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e2989727",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "name_postfix is not None or len(name_postfix) != 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d8ee3a",
   "metadata": {},
   "source": [
    "# Создание модели"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "5037b82a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 13, 96, 96]) torch.Size([16, 11, 96, 96])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'custom_fpn_efficientnet-b0_ 2025-10-05T00-56-04'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#with open(r'training_configs\\custom_fpn_ce_efficientnet-b0_RGB.yaml') as fd:\n",
    "#    config_dict = yaml.load(fd, yaml.Loader)\n",
    "\n",
    "#config_dict['path_to_dataset_root'] = r'C:\\Users\\mokhail\\develop\\DATA\\DATA_FOR_TRAINIG_96'\n",
    "#config_dict\n",
    "\n",
    "path_to_dataset_root = config_dict['path_to_dataset_root']\n",
    "\n",
    "path_to_dataset_info_csv = os.path.join(path_to_dataset_root, 'data_info_table.csv')\n",
    "path_to_surface_classes_json = os.path.join(path_to_dataset_root, 'surface_classes.json')\n",
    "\n",
    "input_image_size = config_dict['input_image_size']\n",
    "multispecter_bands_indices = config_dict['multispecter_bands_indices']\n",
    "device = config_dict['device']\n",
    "\n",
    "# чтение списка имен классов поверхностей\n",
    "with open(path_to_surface_classes_json) as fd:\n",
    "    surface_classes_list = json.load(fd)\n",
    "# чтение таблицы с информацией о каждом изображении в выборке\n",
    "images_df = pd.read_csv(path_to_dataset_info_csv)\n",
    "\n",
    "path_to_partition_json = os.path.join(path_to_dataset_root, 'dataset_partition.json')\n",
    "# чтение словаря со списками квадратов, находящихся в обучающей и тестовой выборке\n",
    "with open(path_to_partition_json) as fd:\n",
    "    partition_dict = json.load(fd)\n",
    "\n",
    "# формирование pandas DataFrame-ов с информацией об изображениях обучающей и тестовой выборках\n",
    "train_images_df = []\n",
    "for train_square in partition_dict['train_squares']:\n",
    "    train_images_df.append(images_df[images_df['square_id']==train_square])\n",
    "train_images_df = pd.concat(train_images_df, ignore_index=True)\n",
    "\n",
    "test_images_df = []\n",
    "for test_square in partition_dict['test_squares']:\n",
    "    test_images_df.append(images_df[images_df['square_id']==test_square])\n",
    "test_images_df = pd.concat(test_images_df, ignore_index=True)\n",
    "\n",
    "#train_images_df, test_images_df = train_test_split(images_df, test_size=0.3, random_state=0)\n",
    "\n",
    "class_num = images_df['class_num'].iloc[0]\n",
    "\n",
    "# формирование словаря, отображающейго имя класса поверхности в индекс класса\n",
    "class_name2idx_dict = {n:i for i, n in enumerate(surface_classes_list)}\n",
    "\n",
    "# вычисление распределений пикселей в классах поверхностей \n",
    "classes_pixels_distribution_df = images_df[surface_classes_list]\n",
    "classes_pixels_num = classes_pixels_distribution_df.sum()\n",
    "classes_weights = classes_pixels_num / classes_pixels_num.sum()\n",
    "classes_weights = classes_weights[surface_classes_list].to_numpy().astype(np.float32)\n",
    "\n",
    "input_image_size = config_dict['input_image_size']\n",
    "'''\n",
    "train_transforms = v2.Compose(\n",
    "    [v2.Resize((input_image_size,input_image_size), antialias=True),v2.ToDtype(torch.float32, scale=True)])\n",
    "test_transforms = v2.Compose(\n",
    "    [v2.Resize((input_image_size,input_image_size), antialias=True),v2.ToDtype(torch.float32, scale=True)])\n",
    "'''\n",
    "train_transforms = create_augmentation_transforms(config_dict['train_augmentations'])\n",
    "test_transforms = nn.Identity()\n",
    "# если ф-ция потерь перекрестная энтропия, то проверяем, есть ли там веса классов\n",
    "if config_dict['loss']['type'] == 'crossentropy':\n",
    "    # если в параметрах функции потерь стоит строка 'classes', надо передать в функцию вектор весов классов\n",
    "    if 'weight' in config_dict['loss']['params']:\n",
    "        if isinstance(config_dict['loss']['params']['weight'], (list, tuple)):\n",
    "            config_dict['loss']['params']['weight'] = torch.tensor(config_dict['loss']['params']['weight'])\n",
    "        \n",
    "        elif config_dict['loss']['params']['weight'] is not None:\n",
    "            config_dict['loss']['params']['weight'] = torch.tensor(classes_weights)\n",
    "\n",
    "# создание функции потерь\n",
    "criterion = criterion_factory_dict[config_dict['loss']['type']](**config_dict['loss']['params'])\n",
    "\n",
    "# если ф-ция потерь перекрестная энтропия, то проверяем, есть ли там веса классов\n",
    "if config_dict['loss']['type'] == 'crossentropy':\n",
    "    # если в параметрах функции потерь стоит строка 'classes', надо передать в функцию вектор весов классов\n",
    "    if 'weight' in config_dict['loss']['params']:\n",
    "        if isinstance(config_dict['loss']['params']['weight'], torch.Tensor):\n",
    "            config_dict['loss']['params']['weight'] = config_dict['loss']['params']['weight'].cpu().tolist()\n",
    "\n",
    "model = create_model(config_dict, segmentation_nns_factory_dict)\n",
    "model = model.to(device)\n",
    "\n",
    "# создаем датасеты и даталоадеры\n",
    "train_dataset = SegmentationDataset(path_to_dataset_root=path_to_dataset_root, samples_df=train_images_df, channel_indices=multispecter_bands_indices, transforms=train_transforms, dtype=torch.float32, device=device)\n",
    "test_dataset = SegmentationDataset(path_to_dataset_root=path_to_dataset_root, samples_df=test_images_df, channel_indices=multispecter_bands_indices, transforms=test_transforms, dtype=torch.float32, device=device)\n",
    "#train_dataset = SegmentationDatasetApplSurf(path_to_dataset_root=path_to_dataset_root, samples_df=test_images_df, channel_indices=channel_indices, name2class_idx_dict=class_name2idx_dict, applicable_surfaces_dict=applicable_surfaces_dict, transforms=test_transforms, device=device)\n",
    "#test_dataset = SegmentationDatasetApplSurf(path_to_dataset_root=path_to_dataset_root, samples_df=test_images_df, channel_indices=channel_indices, name2class_idx_dict=class_name2idx_dict, applicable_surfaces_dict=applicable_surfaces_dict, transforms=test_transforms, device=device)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=config_dict['batch_size'], shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=config_dict['batch_size'])\n",
    "\n",
    "# тестовое чтение данных\n",
    "for data, labels in test_loader:\n",
    "    break\n",
    "    pred = model(data)\n",
    "    loss = criterion(pred, labels)\n",
    "    loss.backward()\n",
    "    \n",
    "# тестовая обработка данных нейронной сетью\n",
    "ret = model(data)\n",
    "print(data.shape, ret.shape)\n",
    "\n",
    "createion_time_str = datetime.now().strftime('%Y-%m-%dT%H-%M-%S')\n",
    "nn_arch_str = config_dict[\"segmentation_nn\"][\"nn_architecture\"]\n",
    "nn_encoder_str = config_dict[\"segmentation_nn\"][\"params\"][\"encoder_name\"]\n",
    "name_postfix = config_dict[\"name_postfix\"]\n",
    "if name_postfix is not None:\n",
    "    model_name = f'{nn_arch_str}_{nn_encoder_str}_{name_postfix} {createion_time_str}'\n",
    "else:\n",
    "    model_name = f'{nn_arch_str}_{nn_encoder_str} {createion_time_str}'\n",
    "model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "5651a262",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FPNMod(\n",
       "  (encoder): EfficientNetEncoder(\n",
       "    (_conv_stem): Conv2d(3, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (_bn0): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "    (_blocks): ModuleList(\n",
       "      (0): MBConvBlock(\n",
       "        (_expand_conv): Identity()\n",
       "        (_bn0): Identity()\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          32, 32, kernel_size=(3, 3), stride=[1, 1], groups=32, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(32, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          32, 8, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          8, 32, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          32, 16, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(16, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): SiLU()\n",
       "      )\n",
       "      (1): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          16, 96, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          96, 96, kernel_size=(3, 3), stride=[2, 2], groups=96, bias=False\n",
       "          (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(96, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          96, 4, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          4, 96, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          96, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): SiLU()\n",
       "      )\n",
       "      (2): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          144, 144, kernel_size=(3, 3), stride=(1, 1), groups=144, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          144, 24, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(24, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): SiLU()\n",
       "      )\n",
       "      (3): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          24, 144, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          144, 144, kernel_size=(5, 5), stride=[2, 2], groups=144, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 2, 1, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(144, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          144, 6, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          6, 144, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          144, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): SiLU()\n",
       "      )\n",
       "      (4): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          240, 240, kernel_size=(5, 5), stride=(1, 1), groups=240, bias=False\n",
       "          (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(40, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): SiLU()\n",
       "      )\n",
       "      (5): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          240, 240, kernel_size=(3, 3), stride=[2, 2], groups=240, bias=False\n",
       "          (static_padding): ZeroPad2d((0, 1, 0, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(240, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          240, 10, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          10, 240, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          240, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): SiLU()\n",
       "      )\n",
       "      (6-7): 2 x MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          480, 480, kernel_size=(3, 3), stride=(1, 1), groups=480, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          480, 80, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(80, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): SiLU()\n",
       "      )\n",
       "      (8): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          80, 480, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          480, 480, kernel_size=(5, 5), stride=[1, 1], groups=480, bias=False\n",
       "          (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(480, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          480, 20, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          20, 480, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          480, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): SiLU()\n",
       "      )\n",
       "      (9-10): 2 x MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          672, 672, kernel_size=(5, 5), stride=(1, 1), groups=672, bias=False\n",
       "          (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          672, 112, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(112, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): SiLU()\n",
       "      )\n",
       "      (11): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          112, 672, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          672, 672, kernel_size=(5, 5), stride=[2, 2], groups=672, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 2, 1, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(672, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          672, 28, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          28, 672, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          672, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): SiLU()\n",
       "      )\n",
       "      (12-14): 3 x MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(5, 5), stride=(1, 1), groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d((2, 2, 2, 2))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 192, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(192, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): SiLU()\n",
       "      )\n",
       "      (15): MBConvBlock(\n",
       "        (_expand_conv): Conv2dStaticSamePadding(\n",
       "          192, 1152, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn0): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_depthwise_conv): Conv2dStaticSamePadding(\n",
       "          1152, 1152, kernel_size=(3, 3), stride=[1, 1], groups=1152, bias=False\n",
       "          (static_padding): ZeroPad2d((1, 1, 1, 1))\n",
       "        )\n",
       "        (_bn1): BatchNorm2d(1152, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_se_reduce): Conv2dStaticSamePadding(\n",
       "          1152, 48, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_se_expand): Conv2dStaticSamePadding(\n",
       "          48, 1152, kernel_size=(1, 1), stride=(1, 1)\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_project_conv): Conv2dStaticSamePadding(\n",
       "          1152, 320, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "          (static_padding): Identity()\n",
       "        )\n",
       "        (_bn2): BatchNorm2d(320, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "        (_swish): SiLU()\n",
       "      )\n",
       "    )\n",
       "    (_conv_head): Conv2dStaticSamePadding(\n",
       "      320, 1280, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
       "      (static_padding): Identity()\n",
       "    )\n",
       "    (_bn1): BatchNorm2d(1280, eps=0.001, momentum=0.010000000000000009, affine=True, track_running_stats=True)\n",
       "    (_avg_pooling): AdaptiveAvgPool2d(output_size=1)\n",
       "    (_dropout): Dropout(p=0.2, inplace=False)\n",
       "    (_swish): SiLU()\n",
       "  )\n",
       "  (decoder): FPNDecoderMod(\n",
       "    (p6): Conv2d(320, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (p5): FPNModBlock(\n",
       "      (skip_conv): Conv2d(112, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (p4): FPNModBlock(\n",
       "      (skip_conv): Conv2d(40, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (p3): FPNModBlock(\n",
       "      (skip_conv): Conv2d(24, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (p2): FPNModBlock(\n",
       "      (skip_conv): Conv2d(32, 128, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "    (seg_blocks): ModuleList(\n",
       "      (0): SegmentationBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (1): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (3): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1): SegmentationBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (1): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (2): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (2): SegmentationBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "          (1): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (3-4): 2 x SegmentationBlock(\n",
       "        (block): Sequential(\n",
       "          (0): Conv3x3GNReLU(\n",
       "            (block): Sequential(\n",
       "              (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "              (1): GroupNorm(32, 128, eps=1e-05, affine=True)\n",
       "              (2): ReLU(inplace=True)\n",
       "            )\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (merge): MergeBlock()\n",
       "    (dropout): Dropout2d(p=0.2, inplace=True)\n",
       "  )\n",
       "  (segmentation_head): SegmentationHead(\n",
       "    (0): Conv2d(128, 11, kernel_size=(1, 1), stride=(1, 1))\n",
       "    (1): Identity()\n",
       "    (2): Activation(\n",
       "      (activation): Identity()\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "684c0a23",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d6f4c339",
   "metadata": {},
   "source": [
    "# Обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9e6b7cea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:658: Checkpoint directory C:\\Users\\mokhail\\develop\\MultispectralSegmentation\\saving_dir\\att_unet_efficientnet-b0_1L_win_cat_agg 2025-10-01T03-13-25 exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | UnetAtt          | 7.2 M  | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "7.2 M     Trainable params\n",
      "0         Non-trainable params\n",
      "7.2 M     Total params\n",
      "28.947    Total estimated model params size (MB)\n",
      "399       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############################\n",
      "att_unet_efficientnet-b0_1L_win_cat_agg 2025-10-01T03-13-25\n",
      "#############################\n",
      "\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1:  25%|██▍       | 105/424 [1:18:11<3:57:31,  0.02it/s, v_num=0, val_loss=2.160, val_iou_UNLABELED=0.000, val_iou_buildings_territory=0.000, val_iou_natural_ground=0.000, val_iou_natural_grow=0.000212, val_iou_natural_wetland=0.00556, val_iou_natural_wood=0.469, val_iou_quasi_natural_ground=0.000, val_iou_quasi_natural_grow=0.181, val_iou_quasi_natural_wetland=0.000, val_iou_transport=0.000, val_iou_water=0.000, val_iou_mean=0.0596, val_precision_UNLABELED=0.000, val_precision_buildings_territory=0.000, val_precision_natural_ground=0.000, val_precision_natural_grow=0.176, val_precision_natural_wetland=0.0708, val_precision_natural_wood=0.470, val_precision_quasi_natural_ground=0.000, val_precision_quasi_natural_grow=0.499, val_precision_quasi_natural_wetland=0.000, val_precision_transport=0.000, val_precision_water=0.000, val_precision_mean=0.111, val_recall_UNLABELED=0.000, val_recall_buildings_territory=0.000, val_recall_natural_ground=0.000, val_recall_natural_grow=0.176, val_recall_natural_wetland=0.0708, val_recall_natural_wood=0.470, val_recall_quasi_natural_ground=0.000, val_recall_quasi_natural_grow=0.499, val_recall_quasi_natural_wetland=0.000, val_recall_transport=0.000, val_recall_water=0.000, val_recall_mean=0.111, train_loss=1.440, train_iou_UNLABELED=0.915, train_iou_buildings_territory=0.416, train_iou_natural_ground=0.000923, train_iou_natural_grow=0.0818, train_iou_natural_wetland=0.0944, train_iou_natural_wood=0.659, train_iou_quasi_natural_ground=0.0111, train_iou_quasi_natural_grow=0.316, train_iou_quasi_natural_wetland=0.000167, train_iou_transport=0.00602, train_iou_water=0.407, train_iou_mean=0.264, train_precision_UNLABELED=0.948, train_precision_buildings_territory=0.567, train_precision_natural_ground=0.00459, train_precision_natural_grow=0.223, train_precision_natural_wetland=0.283, train_precision_natural_wood=0.719, train_precision_quasi_natural_ground=0.045, train_precision_quasi_natural_grow=0.462, train_precision_quasi_natural_wetland=0.00118, train_precision_transport=0.0704, train_precision_water=0.683, train_precision_mean=0.364, train_recall_UNLABELED=0.948, train_recall_buildings_territory=0.567, train_recall_natural_ground=0.00459, train_recall_natural_grow=0.223, train_recall_natural_wetland=0.283, train_recall_natural_wood=0.719, train_recall_quasi_natural_ground=0.045, train_recall_quasi_natural_grow=0.462, train_recall_quasi_natural_wetland=0.00118, train_recall_transport=0.0704, train_recall_water=0.683, train_recall_mean=0.364]\n",
      "Epoch 1:  16%|█▌        | 68/424 [00:21<01:54,  3.12it/s, v_num=0, val_loss=2.160, val_iou_UNLABELED=0.199, val_iou_buildings_territory=0.000, val_iou_natural_ground=0.000, val_iou_natural_grow=0.000, val_iou_natural_wetland=0.0836, val_iou_natural_wood=0.141, val_iou_quasi_natural_ground=0.000, val_iou_quasi_natural_grow=0.241, val_iou_quasi_natural_wetland=0.000, val_iou_transport=0.000, val_iou_water=0.000, val_iou_mean=0.0604, val_precision_UNLABELED=0.585, val_precision_buildings_territory=0.000, val_precision_natural_ground=0.000, val_precision_natural_grow=0.000, val_precision_natural_wetland=0.0853, val_precision_natural_wood=0.451, val_precision_quasi_natural_ground=0.000, val_precision_quasi_natural_grow=0.350, val_precision_quasi_natural_wetland=0.000, val_precision_transport=0.000, val_precision_water=0.000, val_precision_mean=0.134, val_recall_UNLABELED=0.585, val_recall_buildings_territory=0.000, val_recall_natural_ground=0.000, val_recall_natural_grow=0.000, val_recall_natural_wetland=0.0853, val_recall_natural_wood=0.451, val_recall_quasi_natural_ground=0.000, val_recall_quasi_natural_grow=0.350, val_recall_quasi_natural_wetland=0.000, val_recall_transport=0.000, val_recall_water=0.000, val_recall_mean=0.134, train_loss=1.430, train_iou_UNLABELED=0.919, train_iou_buildings_territory=0.414, train_iou_natural_ground=0.001, train_iou_natural_grow=0.0977, train_iou_natural_wetland=0.114, train_iou_natural_wood=0.665, train_iou_quasi_natural_ground=0.00511, train_iou_quasi_natural_grow=0.301, train_iou_quasi_natural_wetland=0.000, train_iou_transport=0.00455, train_iou_water=0.513, train_iou_mean=0.276, train_precision_UNLABELED=0.952, train_precision_buildings_territory=0.572, train_precision_natural_ground=0.0029, train_precision_natural_grow=0.247, train_precision_natural_wetland=0.293, train_precision_natural_wood=0.728, train_precision_quasi_natural_ground=0.0491, train_precision_quasi_natural_grow=0.450, train_precision_quasi_natural_wetland=0.000, train_precision_transport=0.0572, train_precision_water=0.766, train_precision_mean=0.374, train_recall_UNLABELED=0.952, train_recall_buildings_territory=0.572, train_recall_natural_ground=0.0029, train_recall_natural_grow=0.247, train_recall_natural_wetland=0.293, train_recall_natural_wood=0.728, train_recall_quasi_natural_ground=0.0491, train_recall_quasi_natural_grow=0.450, train_recall_quasi_natural_wetland=0.000, train_recall_transport=0.0572, train_recall_water=0.766, train_recall_mean=0.374]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Detected KeyboardInterrupt, attempting graceful shutdown ...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'exit' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:48\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     47\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer.strategy.launcher.launch(trainer_fn, *args, trainer=trainer, **kwargs)\n\u001b[32m---> \u001b[39m\u001b[32m48\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     50\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:599\u001b[39m, in \u001b[36mTrainer._fit_impl\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    593\u001b[39m ckpt_path = \u001b[38;5;28mself\u001b[39m._checkpoint_connector._select_ckpt_path(\n\u001b[32m    594\u001b[39m     \u001b[38;5;28mself\u001b[39m.state.fn,\n\u001b[32m    595\u001b[39m     ckpt_path,\n\u001b[32m    596\u001b[39m     model_provided=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    597\u001b[39m     model_connected=\u001b[38;5;28mself\u001b[39m.lightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    598\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m599\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m=\u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    601\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.state.stopped\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1012\u001b[39m, in \u001b[36mTrainer._run\u001b[39m\u001b[34m(self, model, ckpt_path)\u001b[39m\n\u001b[32m   1009\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1010\u001b[39m \u001b[38;5;66;03m# RUN THE TRAINER\u001b[39;00m\n\u001b[32m   1011\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1012\u001b[39m results = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_run_stage\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1014\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m   1015\u001b[39m \u001b[38;5;66;03m# POST-Training CLEAN UP\u001b[39;00m\n\u001b[32m   1016\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:1056\u001b[39m, in \u001b[36mTrainer._run_stage\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1055\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.autograd.set_detect_anomaly(\u001b[38;5;28mself\u001b[39m._detect_anomaly):\n\u001b[32m-> \u001b[39m\u001b[32m1056\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfit_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1057\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:216\u001b[39m, in \u001b[36m_FitLoop.run\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    215\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_start()\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    217\u001b[39m \u001b[38;5;28mself\u001b[39m.on_advance_end()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:455\u001b[39m, in \u001b[36m_FitLoop.advance\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    454\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._data_fetcher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m455\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mepoch_loop\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_data_fetcher\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:152\u001b[39m, in \u001b[36m_TrainingEpochLoop.run\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43madvance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_fetcher\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m     \u001b[38;5;28mself\u001b[39m.on_advance_end(data_fetcher)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\loops\\training_epoch_loop.py:344\u001b[39m, in \u001b[36m_TrainingEpochLoop.advance\u001b[39m\u001b[34m(self, data_fetcher)\u001b[39m\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m trainer.lightning_module.automatic_optimization:\n\u001b[32m    343\u001b[39m     \u001b[38;5;66;03m# in automatic optimization, there can only be one optimizer\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m     batch_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mautomatic_optimization\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizers\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    345\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:192\u001b[39m, in \u001b[36m_AutomaticOptimization.run\u001b[39m\u001b[34m(self, optimizer, batch_idx, kwargs)\u001b[39m\n\u001b[32m    187\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    188\u001b[39m \u001b[38;5;66;03m# BACKWARD PASS\u001b[39;00m\n\u001b[32m    189\u001b[39m \u001b[38;5;66;03m# ------------------------------\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[38;5;66;03m# gradient update with accumulated gradients\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    194\u001b[39m result = closure.consume_result()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:270\u001b[39m, in \u001b[36m_AutomaticOptimization._optimizer_step\u001b[39m\u001b[34m(self, batch_idx, train_step_and_backward_closure)\u001b[39m\n\u001b[32m    269\u001b[39m \u001b[38;5;66;03m# model hook\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m270\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43moptimizer_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcurrent_epoch\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_idx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[43m    \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    276\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtrain_step_and_backward_closure\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    277\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    279\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_accumulate:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:176\u001b[39m, in \u001b[36m_call_lightning_module_hook\u001b[39m\u001b[34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[39m\n\u001b[32m    175\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    178\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\core\\module.py:1328\u001b[39m, in \u001b[36mLightningModule.optimizer_step\u001b[39m\u001b[34m(self, epoch, batch_idx, optimizer, optimizer_closure)\u001b[39m\n\u001b[32m   1304\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Override this method to adjust the default way the :class:`~lightning.pytorch.trainer.trainer.Trainer` calls\u001b[39;00m\n\u001b[32m   1305\u001b[39m \u001b[33;03mthe optimizer.\u001b[39;00m\n\u001b[32m   1306\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1326\u001b[39m \n\u001b[32m   1327\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1328\u001b[39m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43moptimizer_closure\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\core\\optimizer.py:154\u001b[39m, in \u001b[36mLightningOptimizer.step\u001b[39m\u001b[34m(self, closure, **kwargs)\u001b[39m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m._strategy \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m154\u001b[39m step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_strategy\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_optimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;28mself\u001b[39m._on_after_step()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py:239\u001b[39m, in \u001b[36mStrategy.optimizer_step\u001b[39m\u001b[34m(self, optimizer, closure, model, **kwargs)\u001b[39m\n\u001b[32m    238\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, pl.LightningModule)\n\u001b[32m--> \u001b[39m\u001b[32m239\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mprecision_plugin\u001b[49m\u001b[43m.\u001b[49m\u001b[43moptimizer_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\plugins\\precision\\precision.py:123\u001b[39m, in \u001b[36mPrecision.optimizer_step\u001b[39m\u001b[34m(self, optimizer, model, closure, **kwargs)\u001b[39m\n\u001b[32m    122\u001b[39m closure = partial(\u001b[38;5;28mself\u001b[39m._wrap_closure, model, optimizer, closure)\n\u001b[32m--> \u001b[39m\u001b[32m123\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43moptimizer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m=\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:140\u001b[39m, in \u001b[36mLRScheduler.__init__.<locals>.patch_track_step_called.<locals>.wrap_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    139\u001b[39m opt._opt_called = \u001b[38;5;28;01mTrue\u001b[39;00m  \u001b[38;5;66;03m# type: ignore[union-attr]\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__get__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mopt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopt\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__class__\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\torch\\optim\\optimizer.py:493\u001b[39m, in \u001b[36mOptimizer.profile_hook_step.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    489\u001b[39m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    490\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfunc\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m must return None or a tuple of (new_args, new_kwargs), but got \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    491\u001b[39m             )\n\u001b[32m--> \u001b[39m\u001b[32m493\u001b[39m out = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[38;5;28mself\u001b[39m._optimizer_step_code()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\torch\\optim\\optimizer.py:91\u001b[39m, in \u001b[36m_use_grad_for_differentiable.<locals>._use_grad\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m     90\u001b[39m     torch._dynamo.graph_break()\n\u001b[32m---> \u001b[39m\u001b[32m91\u001b[39m     ret = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     92\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\torch\\optim\\adam.py:223\u001b[39m, in \u001b[36mAdam.step\u001b[39m\u001b[34m(self, closure)\u001b[39m\n\u001b[32m    222\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m torch.enable_grad():\n\u001b[32m--> \u001b[39m\u001b[32m223\u001b[39m         loss = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    225\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m group \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.param_groups:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\plugins\\precision\\precision.py:109\u001b[39m, in \u001b[36mPrecision._wrap_closure\u001b[39m\u001b[34m(self, model, optimizer, closure)\u001b[39m\n\u001b[32m    102\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"This double-closure allows makes sure the ``closure`` is executed before the ``on_before_optimizer_step``\u001b[39;00m\n\u001b[32m    103\u001b[39m \u001b[33;03mhook is called.\u001b[39;00m\n\u001b[32m    104\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m    107\u001b[39m \n\u001b[32m    108\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m closure_result = \u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    110\u001b[39m \u001b[38;5;28mself\u001b[39m._after_closure(model, optimizer)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:146\u001b[39m, in \u001b[36mClosure.__call__\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    144\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__call__\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> Optional[Tensor]:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     \u001b[38;5;28mself\u001b[39m._result = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclosure\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    147\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._result.loss\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\torch\\utils\\_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    115\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:131\u001b[39m, in \u001b[36mClosure.closure\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    128\u001b[39m \u001b[38;5;129m@override\u001b[39m\n\u001b[32m    129\u001b[39m \u001b[38;5;129m@torch\u001b[39m.enable_grad()\n\u001b[32m    130\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mclosure\u001b[39m(\u001b[38;5;28mself\u001b[39m, *args: Any, **kwargs: Any) -> ClosureResult:\n\u001b[32m--> \u001b[39m\u001b[32m131\u001b[39m     step_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_step_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    133\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m step_output.closure_loss \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\loops\\optimization\\automatic.py:319\u001b[39m, in \u001b[36m_AutomaticOptimization._training_step\u001b[39m\u001b[34m(self, kwargs)\u001b[39m\n\u001b[32m    317\u001b[39m trainer = \u001b[38;5;28mself\u001b[39m.trainer\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m training_step_output = \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_strategy_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mtraining_step\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;28mself\u001b[39m.trainer.strategy.post_training_step()  \u001b[38;5;66;03m# unused hook - call anyway for backward compatibility\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:328\u001b[39m, in \u001b[36m_call_strategy_hook\u001b[39m\u001b[34m(trainer, hook_name, *args, **kwargs)\u001b[39m\n\u001b[32m    327\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m trainer.profiler.profile(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m[Strategy]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer.strategy.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m):\n\u001b[32m--> \u001b[39m\u001b[32m328\u001b[39m     output = \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\strategies\\strategy.py:391\u001b[39m, in \u001b[36mStrategy.training_step\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    390\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_redirection(\u001b[38;5;28mself\u001b[39m.model, \u001b[38;5;28mself\u001b[39m.lightning_module, \u001b[33m\"\u001b[39m\u001b[33mtraining_step\u001b[39m\u001b[33m\"\u001b[39m, *args, **kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m391\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlightning_module\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 68\u001b[39m, in \u001b[36mLightningSegmentationModule.training_step\u001b[39m\u001b[34m(self, batch, batch_idx)\u001b[39m\n\u001b[32m     67\u001b[39m data, true_labels = batch\n\u001b[32m---> \u001b[39m\u001b[32m68\u001b[39m pred = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m loss = \u001b[38;5;28mself\u001b[39m.criterion(pred, true_labels)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\segmentation_models_pytorch\\base\\model.py:67\u001b[39m, in \u001b[36mSegmentationModel.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m     66\u001b[39m features = \u001b[38;5;28mself\u001b[39m.encoder(x)\n\u001b[32m---> \u001b[39m\u001b[32m67\u001b[39m decoder_output = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     69\u001b[39m masks = \u001b[38;5;28mself\u001b[39m.segmentation_head(decoder_output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 363\u001b[39m, in \u001b[36mUnetDecoderAtt.forward\u001b[39m\u001b[34m(self, features)\u001b[39m\n\u001b[32m    362\u001b[39m     skip_connection = skip_connections[i] \u001b[38;5;28;01mif\u001b[39;00m i < \u001b[38;5;28mlen\u001b[39m(skip_connections) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m363\u001b[39m     x = \u001b[43mdecoder_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mwidth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mskip_connection\u001b[49m\u001b[43m=\u001b[49m\u001b[43mskip_connection\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    365\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 273\u001b[39m, in \u001b[36mUnetDecoderBlockAtt.forward\u001b[39m\u001b[34m(self, feature_map, target_height, target_width, skip_connection)\u001b[39m\n\u001b[32m    272\u001b[39m feature_map = \u001b[38;5;28mself\u001b[39m.conv_layers(feature_map)\n\u001b[32m--> \u001b[39m\u001b[32m273\u001b[39m feature_map = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mattention2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeature_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    274\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m feature_map\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 142\u001b[39m, in \u001b[36mLazyWindowVisionTransformerBlocks.forward\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m    140\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer_name, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.transformer_layers.items():\n\u001b[32m--> \u001b[39m\u001b[32m142\u001b[39m     window = \u001b[43mlayer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwindow\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    143\u001b[39m windows_outs.append(window.unsqueeze(\u001b[32m0\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1738\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1739\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1747\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1748\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1749\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1750\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1752\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[37]\u001b[39m\u001b[32m, line 66\u001b[39m\n\u001b[32m     62\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(path_to_config, \u001b[33m'\u001b[39m\u001b[33mw\u001b[39m\u001b[33m'\u001b[39m, encoding=\u001b[33m'\u001b[39m\u001b[33mutf-8\u001b[39m\u001b[33m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m fd:\n\u001b[32m     63\u001b[39m     \u001b[38;5;66;03m#json.dump(config_dict, fd, indent=4)\u001b[39;00m\n\u001b[32m     64\u001b[39m     yaml.dump(config_dict, fd, indent=\u001b[32m4\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m66\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43msegmentation_module\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\trainer\\trainer.py:561\u001b[39m, in \u001b[36mTrainer.fit\u001b[39m\u001b[34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[39m\n\u001b[32m    559\u001b[39m \u001b[38;5;28mself\u001b[39m.training = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    560\u001b[39m \u001b[38;5;28mself\u001b[39m.should_stop = \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m561\u001b[39m \u001b[43mcall\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    562\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[32m    563\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\trainer\\call.py:65\u001b[39m, in \u001b[36m_call_and_handle_interrupt\u001b[39m\u001b[34m(trainer, trainer_fn, *args, **kwargs)\u001b[39m\n\u001b[32m     63\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(launcher, _SubprocessScriptLauncher):\n\u001b[32m     64\u001b[39m         launcher.kill(_get_sigkill_signal())\n\u001b[32m---> \u001b[39m\u001b[32m65\u001b[39m     \u001b[43mexit\u001b[49m(\u001b[32m1\u001b[39m)\n\u001b[32m     67\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mBaseException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exception:\n\u001b[32m     68\u001b[39m     _interrupt(trainer, exception)\n",
      "\u001b[31mNameError\u001b[39m: name 'exit' is not defined"
     ]
    }
   ],
   "source": [
    "epoch_num = config_dict['epoch_num']\n",
    "\n",
    "print('#############################')\n",
    "print(model_name)\n",
    "print('#############################')\n",
    "print()\n",
    "\n",
    "# создаем список словарей с информацией о вычисляемых метриках с помощью multiclass confusion matrix\n",
    "# см. подробнее ддокументацию к функции compute_metric_from_confusion\n",
    "metrics_dict = {\n",
    "    'train': {\n",
    "        'iou': classification.JaccardIndex(task='multiclass', average='none', num_classes=len(class_name2idx_dict)).to(device),\n",
    "        'precision': classification.Precision(task='multiclass', average='none', num_classes=len(class_name2idx_dict)).to(device),\n",
    "        'recall': classification.Precision(task='multiclass', average='none', num_classes=len(class_name2idx_dict)).to(device),\n",
    "        'confusion': classification.ConfusionMatrix(task='multiclass', num_classes=len(class_name2idx_dict)).to(device),\n",
    "    },\n",
    "    'val': {\n",
    "        'iou': classification.JaccardIndex(task='multiclass', average='none', num_classes=len(class_name2idx_dict)).to(device),\n",
    "        'precision': classification.Precision(task='multiclass', average='none', num_classes=len(class_name2idx_dict)).to(device),\n",
    "        'recall': classification.Precision(task='multiclass', average='none', num_classes=len(class_name2idx_dict)).to(device),\n",
    "        'confusion': classification.ConfusionMatrix(task='multiclass', num_classes=len(class_name2idx_dict)).to(device),\n",
    "    }\n",
    "}\n",
    "\n",
    "optimizer_cfg = {\n",
    "    'optmizer': optimizers_factory_dict[config_dict['optimizer']['type']],\n",
    "    'optimizer_args':config_dict['optimizer']['args'],\n",
    "    'lr_scheduler': lr_schedulers_factory_dict[config_dict['lr_scheduler']['type']],\n",
    "    'lr_scheduler_args': config_dict['lr_scheduler']['args'],\n",
    "    'lr_scheduler_params': config_dict['lr_scheduler']['params']\n",
    "\n",
    "}\n",
    "\n",
    "# Создаем модуль Lightning\n",
    "segmentation_module = LightningSegmentationModule(model, criterion, optimizer_cfg, metrics_dict, class_name2idx_dict)\n",
    "\n",
    "# задаем путь до папки с логгерами и создаем логгер, записывающий результаты в csv\n",
    "path_to_saving_dir = 'saving_dir'\n",
    "\n",
    "#csv_logger = CSVLogger(save_dir = path_to_saving_dir, name=model_name, flush_logs_every_n_steps=1,)\n",
    "csv_logger = CSVLoggerMetricsAndConfusion(save_dir = path_to_saving_dir, name=model_name, flush_logs_every_n_steps=1,)\n",
    "\n",
    "\n",
    "# создаем объект, записывающий в чекпоинт лучшую модель\n",
    "path_to_save_model_dir = os.path.join(path_to_saving_dir, model_name)\n",
    "os.makedirs(path_to_save_model_dir, exist_ok=True)\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    mode=\"max\",\n",
    "    filename=model_name+\"-{epoch:02d}-{val_iou_mean:.3}\",\n",
    "    dirpath=path_to_save_model_dir, \n",
    "    save_top_k=1, monitor=\"val_iou_mean\"\n",
    "    )\n",
    "\n",
    "trainer = L.Trainer(logger=[csv_logger],\n",
    "        max_epochs=epoch_num, \n",
    "        callbacks=[checkpoint_callback],\n",
    "        accelerator = 'gpu'\n",
    "        )\n",
    "\n",
    "# сохраняем конфигурацию\n",
    "path_to_config = os.path.join(path_to_save_model_dir, 'training_config.yaml')\n",
    "with open(path_to_config, 'w', encoding='utf-8') as fd:\n",
    "    #json.dump(config_dict, fd, indent=4)\n",
    "    yaml.dump(config_dict, fd, indent=4)\n",
    "\n",
    "trainer.fit(segmentation_module , train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5ea282e",
   "metadata": {},
   "source": [
    "# Черновики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ffb6f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Список трансформеров, входные слои которых можно безопасно изменять \n",
    "[\n",
    "    'mit', # (7,7) Ньюансы с FPN\n",
    "    'tu-davit', # (7,7) Dual-Attention vision transformer + \n",
    "    'tu-efficientvit_b0',#(3,3) +\n",
    "    'tu-fastvit', #(3,3)\n",
    "    'tu-hgnet',#(3,3)\n",
    "    'tu-hgnetv2',#(3,3)\n",
    "    'tu-mambaout',#(3,3)\n",
    "    'tu-maxvit',#(3,3)\n",
    "    'tu-mvitv2', #(7,7) +\n",
    "    'tu-nextvit',#(3,3)\n",
    "    'tu-poolformer',#(7,7)\n",
    "    'tu-poolformerv2',#(7,7)\n",
    "    'tu-pvt_v2',#(7,7)\n",
    "    'tu-repvit',#(3,3)\n",
    "    'tu-sam2_hiera',#(7,7)\n",
    "    'tu-tiny_vit',#(3,3)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5674eec",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = smp.MAnet()\n",
    "model.encoder.conv1.stride=(1,1)\n",
    "\n",
    "def custom_forward(self, x):\n",
    "    \"\"\"Sequentially pass `x` trough model`s encoder, decoder and heads\"\"\"\n",
    "    features = self.encoder(x)\n",
    "    '''\n",
    "    for f in features:\n",
    "        print(f.shape)\n",
    "    print()\n",
    "    '''\n",
    "\n",
    "    decoder_output = self.decoder(features)\n",
    "    #print(decoder_output.shape)\n",
    "\n",
    "    masks = self.segmentation_head(decoder_output)\n",
    "\n",
    "    if self.classification_head is not None:\n",
    "        labels = self.classification_head(features[-1])\n",
    "        return masks, labels\n",
    "\n",
    "    return masks\n",
    "\n",
    "\n",
    "def decoder_custom_forward(self, features: List[torch.Tensor]) -> torch.Tensor:\n",
    "    features = features[1:]  # remove first skip with same spatial resolution\n",
    "    features = features[::-1]  # reverse channels to start from head of encoder\n",
    "\n",
    "    head = features[0]\n",
    "    skips = features[1:]\n",
    "    print('Features shapes')\n",
    "    print(f'head:{head.shape}')\n",
    "    print('Skips:')\n",
    "    for skip in skips:\n",
    "        print(skip.shape)\n",
    "\n",
    "    print('----------------------')\n",
    "\n",
    "    x = self.center(head)\n",
    "    print(f'Center:{x.shape}')\n",
    "\n",
    "    for i, decoder_block in enumerate(self.blocks):\n",
    "        skip = skips[i] if i < len(skips) else None\n",
    "        x = decoder_block(x, skip)\n",
    "        if skip is not None:\n",
    "            print(f'x:{x.shape}; skip:{skip.shape}')\n",
    "        else:\n",
    "            print(f'x:{x.shape}; skip:{skip}')\n",
    "\n",
    "    return x\n",
    "\n",
    "model.forward = types.MethodType(custom_forward, model)\n",
    "model.decoder.forward = types.MethodType(decoder_custom_forward, model.decoder)\n",
    "\n",
    "ret = model(torch.randn(1, 3, 96, 96))\n",
    "ret.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c98c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "idx_in_batch = 1\n",
    "#transform = v2.RandomPerspective(distortion_scale=0.3, p=1.0, fill={tv_tensors.Image:0.0, tv_tensors.Mask:0})\n",
    "transform = v2.RandomAffine(degrees=[0, 30], translate=[0.0, 0.3], scale=[0.3, 0.5], shear=[0.0, 0.4], fill=[0])\n",
    "#transform = v2.RandomResize(min_size=96, max_size=256)\n",
    "#transform = v2.RandomRotation(degrees=(0, 45))\n",
    "\n",
    "transforms_factory_dict = {\n",
    "    'affine': v2.RandomAffine,\n",
    "    'perspective': v2.RandomPerspective,\n",
    "    'horizontal_flip': v2.RandomHorizontalFlip,\n",
    "    'vertical_flip': v2.RandomVerticalFlip,\n",
    "    'crop': v2.RandomCrop,\n",
    "    'gauss_noise': v2.GaussianNoise,\n",
    "    'gauss_blur': v2.GaussianBlur,\n",
    "    'elastic': v2.ElasticTransform,\n",
    "}\n",
    "\n",
    "def create_transforms(transforms_dict:Dict[str, Dict]):\n",
    "    transforms_list = []\n",
    "    for name, transform_params in transforms_dict.items():\n",
    "        transform_creation_fn = transforms_factory_dict[name]\n",
    "        transforms_list.append(transform_creation_fn(**transform_params))\n",
    "    #return v2.Compose([v2.RandomOrder(transforms_list)])\n",
    "    return v2.RandomOrder(transforms_list)\n",
    "        \n",
    "transforms_dict = {\n",
    "        'gauss_noise':{\n",
    "            'mean': 0.0,\n",
    "            'sigma': 0.0008,\n",
    "            'clip': False,\n",
    "        },\n",
    "        'affine':{\n",
    "            'degrees': [0, 45],\n",
    "            'translate': [0, 0.3],\n",
    "            'scale': [0.7, 1.5],\n",
    "            'shear': [0, 0.2],\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'perspective':{\n",
    "            'distortion_scale': 0.2,\n",
    "            'p': 0.3,\n",
    "            'fill': [0],\n",
    "        },\n",
    "        'horizontal_flip': {\n",
    "            'p': 0.5,\n",
    "        },\n",
    "        'vertical_flip':{\n",
    "            'p': 0.5,\n",
    "        },\n",
    "}\n",
    "\n",
    "transforms = create_transforms(transforms_dict)\n",
    "transforms\n",
    "\n",
    "mask = tv_tensors.Mask(labels[idx_in_batch])\n",
    "\n",
    "to_transform = {'image':data[idx_in_batch], 'mask':mask}\n",
    "\n",
    "out = transforms(to_transform)\n",
    "img_tr, mask_tr = out['image'].detach().cpu(), out['mask'].detach().cpu()\n",
    "\n",
    "img = data[idx_in_batch].detach().cpu()\n",
    "mask = labels[idx_in_batch].detach().cpu()\n",
    "\n",
    "fig, axs = plt.subplots(2,2)\n",
    "axs[0,0].imshow(img[idx_in_batch])\n",
    "axs[0,1].imshow(mask)\n",
    "axs[1,0].imshow(img_tr[idx_in_batch])\n",
    "axs[1,1].imshow(mask_tr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f7f0022",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "model = smp.Unet(encoder_name='resnet34')\n",
    "#input_weight = model.encoder._conv_stem.weight.detach()#.numpy()\n",
    "input_weight = model.encoder.conv1.weight.detach()#.numpy()\n",
    "interpolated_weight = F.interpolate(input_weight, size=(11, 11), mode='bicubic', antialias=True, align_corners=False)\n",
    "\n",
    "print(input_weight.shape)\n",
    "print(interpolated_weight.shape)\n",
    "\n",
    "filter_index = 9\n",
    "conv_filter = input_weight[filter_index]\n",
    "interpolated_conv_filter = interpolated_weight[filter_index]\n",
    "fig1, axs1 = plt.subplots(1,3)\n",
    "for idx, img in enumerate(conv_filter):\n",
    "    axs1[idx].imshow(img)\n",
    "\n",
    "fig2, axs2 = plt.subplots(1,3)\n",
    "for idx, img in enumerate(interpolated_conv_filter):\n",
    "    axs2[idx].imshow(img)\n",
    "\n",
    "\n",
    "filter_index += 1\n",
    "conv_filter = input_weight[filter_index]\n",
    "interpolated_conv_filter = interpolated_weight[filter_index]\n",
    "fig3, axs3 = plt.subplots(1,3)\n",
    "for idx, img in enumerate(conv_filter):\n",
    "    axs3[idx].imshow(img)\n",
    "\n",
    "fig4, axs4 = plt.subplots(1,3)\n",
    "for idx, img in enumerate(interpolated_conv_filter):\n",
    "    axs4[idx].imshow(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "35c0200a",
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = [1, 2, 3, 4, 5, 6, 7, 8, 11, 12, 'ndvi', 'ndbi', 'ndwi', 'ndre']\n",
    "basic_indices = [1, 2, 3, 7]\n",
    "rest_indices = set(indices) - set(basic_indices)\n",
    "rest_indices = list(rest_indices)\n",
    "\n",
    "rest_indices = [x for x in rest_indices if isinstance(x, int)] + [x for x in rest_indices if isinstance(x, str)]\n",
    "#for combination in combinations()\n",
    "for k in range(len(rest_indices)):\n",
    "    k+=1\n",
    "    for combination_of_indices in combinations(rest_indices, k):\n",
    "        indices_to_test = basic_indices + list(combination_of_indices)\n",
    "        config_dict['multispecter_bands_indices'] = indices_to_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c05f400",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 5, 6, 8, 11, 12, 'ndre', 'ndvi', 'ndwi', 'ndbi']"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class PABBlock(nn.Module):\n",
    "    def __init__(self, in_channels: int, pab_channels: int = 64):\n",
    "        super().__init__()\n",
    "\n",
    "        # Series of 1x1 conv to generate attention feature maps\n",
    "        self.pab_channels = pab_channels\n",
    "        self.in_channels = in_channels\n",
    "        self.top_conv = nn.Conv2d(in_channels, pab_channels, kernel_size=1)\n",
    "        self.center_conv = nn.Conv2d(in_channels, pab_channels, kernel_size=1)\n",
    "        self.bottom_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "        self.map_softmax = nn.Softmax(dim=1)\n",
    "        self.out_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        batch_size, _, height, width = x.shape\n",
    "\n",
    "        x_top = self.top_conv(x)\n",
    "        x_center = self.center_conv(x)\n",
    "        x_bottom = self.bottom_conv(x)\n",
    "\n",
    "        x_top = x_top.flatten(2)\n",
    "        x_center = x_center.flatten(2).transpose(1, 2)\n",
    "        x_bottom = x_bottom.flatten(2).transpose(1, 2)\n",
    "\n",
    "        sp_map = torch.matmul(x_center, x_top)\n",
    "        sp_map = self.map_softmax(sp_map.view(batch_size, -1))\n",
    "        sp_map = sp_map.view(batch_size, height * width, height * width)\n",
    "\n",
    "        sp_map = torch.matmul(sp_map, x_bottom)\n",
    "        sp_map = sp_map.reshape(batch_size, self.in_channels, height, width)\n",
    "\n",
    "        x = x + sp_map\n",
    "        x = self.out_conv(x)\n",
    "        return x\n",
    "    \n",
    "\n",
    "block = PABBlock(in_channels=64, pab_channels=64)\n",
    "\n",
    "\n",
    "pab_channels = 256\n",
    "in_channels = 128\n",
    "top_conv = nn.Conv2d(in_channels, pab_channels, kernel_size=1)\n",
    "center_conv = nn.Conv2d(in_channels, pab_channels, kernel_size=1)\n",
    "bottom_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "map_softmax = nn.Softmax(dim=1)\n",
    "out_conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1)\n",
    "\n",
    "x = torch.randn(1, in_channels, 12, 12)\n",
    "batch_size, _, height, width = x.shape\n",
    "\n",
    "x_top = top_conv(x)\n",
    "x_center = center_conv(x)\n",
    "x_bottom = bottom_conv(x)\n",
    "print('After conv:')\n",
    "print(f'x_top={x_top.shape}, x_center={x_center.shape}, x_bottom={x_bottom.shape},')\n",
    "\n",
    "x_top = x_top.flatten(2)\n",
    "x_center = x_center.flatten(2).transpose(1, 2)\n",
    "x_bottom = x_bottom.flatten(2).transpose(1, 2)\n",
    "print('After reshape and transpose:')\n",
    "print(f'x_top_r={x_top.shape}, x_center_rt={x_center.shape}, x_bottom_rt={x_bottom.shape},')\n",
    "\n",
    "sp_map = torch.matmul(x_center, x_top)\n",
    "print(f'sp_map={sp_map.shape} (x_center_rt × x_top_r)')\n",
    "sp_map = map_softmax(sp_map.view(batch_size, -1))\n",
    "print(f'sp_map after softmax={sp_map.shape}')\n",
    "sp_map = sp_map.view(batch_size, height * width, height * width)\n",
    "print(f'sp_map after reshape={sp_map.shape}')\n",
    "sp_map = torch.matmul(sp_map, x_bottom)\n",
    "print(f'sp_map × x_bottom = {sp_map.shape}')\n",
    "sp_map = sp_map.reshape(batch_size, in_channels, height, width)\n",
    "print(f'sp_map after reshape = {sp_map.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "330f7a52",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
