{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import tv_tensors\n",
    "from torchvision import models\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "import timm\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import combinations, product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[-0.4237,  1.0368,  0.3404, -2.2965, -0.0451,  0.6876, -0.1397, -2.0801,\n",
      "         -0.0359, -0.6495,  1.5594, -1.8002, -0.3564],\n",
      "        [ 1.0143,  1.4856, -1.4325, -0.5906,  1.1216, -0.4022,  0.9596, -0.7967,\n",
      "         -0.0740,  0.8291,  1.8039, -0.4125,  0.2324],\n",
      "        [ 0.0145, -0.3426, -0.8420, -0.2245, -1.2749,  0.7201,  0.2085, -2.6879,\n",
      "          0.5349,  0.9610,  0.4518, -0.9975,  1.5043],\n",
      "        [-0.3448, -0.9648,  0.2235,  1.1171, -0.0175, -0.2327, -1.2988,  1.0650,\n",
      "         -2.1954,  1.0556, -0.6778,  0.9061, -1.8265],\n",
      "        [ 2.3559,  2.3506,  0.1013,  0.3301,  0.4932,  0.6986,  1.2312,  1.6762,\n",
      "          0.2768,  1.3317, -1.4021, -0.9260, -0.0714],\n",
      "        [ 0.7392, -2.7855, -1.5009,  0.6287,  0.5828,  0.5585,  0.5708, -0.2304,\n",
      "         -0.0236, -0.2406, -1.1984, -0.9421,  1.9661],\n",
      "        [-0.4917, -0.4664,  1.5144,  1.6990, -1.2259,  1.4083, -1.3333, -0.4681,\n",
      "         -0.7043, -0.8508,  0.7150, -0.8222,  1.3543],\n",
      "        [ 0.1234,  0.3513,  1.2059, -0.7312, -1.8620,  0.2681, -1.2020,  0.6246,\n",
      "          0.3395,  0.9490,  1.3864,  1.5470, -0.3867],\n",
      "        [-0.3845,  1.0710, -1.1151, -1.2921, -0.5656, -1.6061, -0.6918,  0.3642,\n",
      "         -0.6649,  1.4710,  2.2689, -0.5308, -0.3674],\n",
      "        [-1.9279, -0.3958,  2.3120,  1.2718,  0.5595,  0.1854,  0.8258, -1.0826,\n",
      "          0.8462, -1.6091,  1.9245, -0.9122, -1.0912],\n",
      "        [ 1.0817,  0.6024,  0.8370,  0.2438,  0.2607,  1.7354, -0.4023, -0.2545,\n",
      "          1.2009,  0.7583,  0.3962,  0.2994, -1.2969],\n",
      "        [-0.0612, -0.2771,  0.7951, -0.1261, -1.0813,  1.2748, -2.0116, -0.0529,\n",
      "          0.5265,  1.0103,  0.8258, -0.5985, -1.6650],\n",
      "        [-0.6299,  0.6432, -0.0483,  0.7259,  0.9445,  1.3120,  0.9949,  0.8055,\n",
      "          1.5522, -0.3855, -0.3447,  0.0840, -1.0990]], requires_grad=True)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.4237,  1.0368,  0.3404,  ..., -1.2259,  1.4083, -1.3333],\n",
       "        [-0.3564, -0.4237,  1.0368,  ...,  1.6990, -1.2259,  1.4083],\n",
       "        [-1.8002, -0.3564, -0.4237,  ...,  1.5144,  1.6990, -1.2259],\n",
       "        ...,\n",
       "        [ 0.9490,  1.3864,  1.5470,  ..., -0.4237,  1.0368,  0.3404],\n",
       "        [ 0.3395,  0.9490,  1.3864,  ..., -0.3564, -0.4237,  1.0368],\n",
       "        [ 0.6246,  0.3395,  0.9490,  ..., -1.8002, -0.3564, -0.4237]])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "window_size=7\n",
    "\n",
    "\n",
    "B = nn.Parameter(torch.randn(2*window_size-1, 2*window_size-1))\n",
    "print(B)\n",
    "x = torch.arange(1,window_size+1,1/window_size)\n",
    "x = (x[None, :]-x[:, None]).int()\n",
    "\n",
    "y = torch.concat([torch.arange(1,window_size+1)] * window_size)\n",
    "y = (y[None, :]-y[:, None])\n",
    "#x[None, :] == x.unsqueeze(0)\n",
    "#x[:, None] == x.unsqueeze(1)\n",
    "nn.Parameter((B[x, y]), requires_grad=False)\n",
    "#pd.DataFrame(data=y.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([49, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x[:, None].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'b_rgb': [1, 2, 3], 'st_1': (1, 1), 'w_rnd': None},\n",
       " {'b_rgb': [1, 2, 3], 'st_1': (1, 1), 'w_pr': 'imagenet'},\n",
       " {'b_rgb': [1, 2, 3], 'st_2': (2, 2), 'w_rnd': None},\n",
       " {'b_rgb': [1, 2, 3], 'st_2': (2, 2), 'w_pr': 'imagenet'},\n",
       " {'b_10m': [1, 2, 3, 7], 'st_1': (1, 1), 'w_rnd': None},\n",
       " {'b_10m': [1, 2, 3, 7], 'st_1': (1, 1), 'w_pr': 'imagenet'},\n",
       " {'b_10m': [1, 2, 3, 7], 'st_2': (2, 2), 'w_rnd': None},\n",
       " {'b_10m': [1, 2, 3, 7], 'st_2': (2, 2), 'w_pr': 'imagenet'},\n",
       " {'b_10-20m': [1, 2, 3, 4, 5, 6, 7, 11, 12], 'st_1': (1, 1), 'w_rnd': None},\n",
       " {'b_10-20m': [1, 2, 3, 4, 5, 6, 7, 11, 12],\n",
       "  'st_1': (1, 1),\n",
       "  'w_pr': 'imagenet'},\n",
       " {'b_10-20m': [1, 2, 3, 4, 5, 6, 7, 11, 12], 'st_2': (2, 2), 'w_rnd': None},\n",
       " {'b_10-20m': [1, 2, 3, 4, 5, 6, 7, 11, 12],\n",
       "  'st_2': (2, 2),\n",
       "  'w_pr': 'imagenet'},\n",
       " {'b_full_sp': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
       "  'st_1': (1, 1),\n",
       "  'w_rnd': None},\n",
       " {'b_full_sp': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
       "  'st_1': (1, 1),\n",
       "  'w_pr': 'imagenet'},\n",
       " {'b_full_sp': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
       "  'st_2': (2, 2),\n",
       "  'w_rnd': None},\n",
       " {'b_full_sp': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
       "  'st_2': (2, 2),\n",
       "  'w_pr': 'imagenet'},\n",
       " {'b_rgb-ndvi': [1, 2, 3, 'ndvi'], 'st_1': (1, 1), 'w_rnd': None},\n",
       " {'b_rgb-ndvi': [1, 2, 3, 'ndvi'], 'st_1': (1, 1), 'w_pr': 'imagenet'},\n",
       " {'b_rgb-ndvi': [1, 2, 3, 'ndvi'], 'st_2': (2, 2), 'w_rnd': None},\n",
       " {'b_rgb-ndvi': [1, 2, 3, 'ndvi'], 'st_2': (2, 2), 'w_pr': 'imagenet'},\n",
       " {'b_rgb-ndwi': [1, 2, 3, 'ndwi'], 'st_1': (1, 1), 'w_rnd': None},\n",
       " {'b_rgb-ndwi': [1, 2, 3, 'ndwi'], 'st_1': (1, 1), 'w_pr': 'imagenet'},\n",
       " {'b_rgb-ndwi': [1, 2, 3, 'ndwi'], 'st_2': (2, 2), 'w_rnd': None},\n",
       " {'b_rgb-ndwi': [1, 2, 3, 'ndwi'], 'st_2': (2, 2), 'w_pr': 'imagenet'},\n",
       " {'b_rgb-ndbi': [1, 2, 3, 'ndbi'], 'st_1': (1, 1), 'w_rnd': None},\n",
       " {'b_rgb-ndbi': [1, 2, 3, 'ndbi'], 'st_1': (1, 1), 'w_pr': 'imagenet'},\n",
       " {'b_rgb-ndbi': [1, 2, 3, 'ndbi'], 'st_2': (2, 2), 'w_rnd': None},\n",
       " {'b_rgb-ndbi': [1, 2, 3, 'ndbi'], 'st_2': (2, 2), 'w_pr': 'imagenet'},\n",
       " {'b_rgb-ndre': [1, 2, 3, 'ndre'], 'st_1': (1, 1), 'w_rnd': None},\n",
       " {'b_rgb-ndre': [1, 2, 3, 'ndre'], 'st_1': (1, 1), 'w_pr': 'imagenet'},\n",
       " {'b_rgb-ndre': [1, 2, 3, 'ndre'], 'st_2': (2, 2), 'w_rnd': None},\n",
       " {'b_rgb-ndre': [1, 2, 3, 'ndre'], 'st_2': (2, 2), 'w_pr': 'imagenet'},\n",
       " {'b_rgb-allind': [1, 2, 3, 'ndvi', 'ndwi', 'ndbi', 'ndre'],\n",
       "  'st_1': (1, 1),\n",
       "  'w_rnd': None},\n",
       " {'b_rgb-allind': [1, 2, 3, 'ndvi', 'ndwi', 'ndbi', 'ndre'],\n",
       "  'st_1': (1, 1),\n",
       "  'w_pr': 'imagenet'},\n",
       " {'b_rgb-allind': [1, 2, 3, 'ndvi', 'ndwi', 'ndbi', 'ndre'],\n",
       "  'st_2': (2, 2),\n",
       "  'w_rnd': None},\n",
       " {'b_rgb-allind': [1, 2, 3, 'ndvi', 'ndwi', 'ndbi', 'ndre'],\n",
       "  'st_2': (2, 2),\n",
       "  'w_pr': 'imagenet'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bands_combinations_list = [\n",
    "    ('b_rgb', [1, 2, 3]),\n",
    "    ('b_10m', [1, 2, 3, 7]),\n",
    "    ('b_10-20m', [1, 2, 3, 4, 5, 6, 7, 11, 12]),\n",
    "    ('b_full_sp', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]),\n",
    "    ('b_rgb-ndvi', [1, 2, 3, 'ndvi']),\n",
    "    ('b_rgb-ndwi', [1, 2, 3, 'ndwi']),\n",
    "    ('b_rgb-ndbi', [1, 2, 3, 'ndbi']),\n",
    "    ('b_rgb-ndre', [1, 2, 3, 'ndre']),\n",
    "    ('b_rgb-allind', [1, 2, 3, 'ndvi', 'ndwi', 'ndbi', 'ndre']),\n",
    "]\n",
    "\n",
    "inconv_strides_list = {\n",
    "    ('st_1', (1, 1)),\n",
    "    ('st_2', (2, 2)),\n",
    "}\n",
    "pretrained_list = {\n",
    "    ('w_rnd', None),\n",
    "    ('w_pr', 'imagenet'),\n",
    "    \n",
    "}\n",
    "all_combinations_list = list(product(bands_combinations_list, inconv_strides_list, pretrained_list))\n",
    "\n",
    "all_combinations_list = [{n:v for n, v in entry} for entry in all_combinations_list]\n",
    "\n",
    "\n",
    "\n",
    "all_combinations_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PIDOR': ''}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "d = {'PIDOR': ''}\n",
    "with open('saving_dir/pidor', 'w') as fd:\n",
    "    yaml.dump(d, fd)\n",
    "\n",
    "with open('saving_dir/pidor') as fd:\n",
    "    pi_dor = yaml.load(fd, yaml.Loader)\n",
    "\n",
    "pi_dor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for combination in all_combinations_list:\n",
    "    name_postfix = config_dict['name_postfix']\n",
    "    for comb_name, comb_val in combination:\n",
    "        if comb_name.startswith('b_'):\n",
    "            config_dict['multispecter_bands_indices'] = comb_val\n",
    "        elif comb_name.startswith('st_'):\n",
    "            config_dict['segmentation_nn']['input_layer_config']['stride'] = comb_val\n",
    "        elif comb_name.startswith('w_'):\n",
    "            config_dict['segmentation_nn']['params']['encoder_weights'] = comb_val\n",
    "        name_postfix = f'{name_postfix}_{comb_name}'\n",
    "\n",
    "    config_dict['name_postfix'] = name_postfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.segmentation.deeplabv3_resnet50(weights=models.segmentation.DeepLabV3_ResNet50_Weights.DEFAULT)\n",
    "print(model.classifier[-1].weight)\n",
    "for layer in model.classifier.children():\n",
    "    for m in layer.modules():\n",
    "        m.reset_parameters()\n",
    "print(model.classifier[-1].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UNLABELED',\n",
       " 'buildings_territory',\n",
       " 'natural_ground',\n",
       " 'natural_grow',\n",
       " 'natural_wetland',\n",
       " 'natural_wood',\n",
       " 'quasi_natural_grow',\n",
       " 'transport',\n",
       " 'water']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_dataset_root = r'I:\\LANDCOVER_DATA\\MULTISPECTRAL_SATELLITE_DATA\\DATA_FOR_TRAINIG'\n",
    "path_to_dataset_info_csv = os.path.join(path_to_dataset_root, 'data_info_table.csv')\n",
    "\n",
    "images_df = pd.read_csv(path_to_dataset_info_csv)\n",
    "path_to_surface_classes_json = os.path.join(path_to_dataset_root, 'surface_classes.json')\n",
    "with open(path_to_surface_classes_json) as fd:\n",
    "    surface_classes_list = json.load(fd)\n",
    "surface_classes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(1, 3, 3)\n",
    "dir(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 150, 150])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FCNSegmentationWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    def forward(self, x):\n",
    "        return self.model(x)['out']\n",
    "    \n",
    "class MultispectralNN(nn.Module):\n",
    "    def __init__(self, main_model, preprocessing_block):\n",
    "        super().__init__()\n",
    "        self.preprocessing_block = preprocessing_block\n",
    "        self.main_model = main_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.preprocessing_block(x)\n",
    "        return self.model(x)\n",
    "    \n",
    "\n",
    "in_channels = 13\n",
    "cols = rows = 150\n",
    "input_tensor = torch.randn(1, 13, cols, rows)\n",
    "conv = nn.Conv2d(in_channels, 64, kernel_size=[1,1])\n",
    "conv(input_tensor).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 150, 150])\n",
      "torch.Size([1, 9, 150, 150])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 150, 150])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultispectralFuseOut(nn.Module):\n",
    "    def __init__(self, main_model, multispectral_preprocessing_block, preprocessing_out_dim, class_num):\n",
    "        super().__init__()\n",
    "        self.multispectral_preprocessing_block = multispectral_preprocessing_block\n",
    "        self.main_model = main_model\n",
    "        self.multispectral_preout_block = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=preprocessing_out_dim,out_channels=class_num, kernel_size=1),\n",
    "            nn.BatchNorm2d(class_num),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fusion_block = nn.Sequential(\n",
    "            #nn.Dropout2d(0.3),\n",
    "            nn.ChannelShuffle(groups=2),\n",
    "            nn.Conv2d(in_channels=class_num*2, out_channels=class_num, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        multispectral_preprocessed_out = self.multispectral_preprocessing_block(x)\n",
    "        multispectral_out = self.multispectral_preout_block(multispectral_preprocessed_out)\n",
    "        print(multispectral_preprocessed_out.shape)\n",
    "        print(multispectral_out.shape)\n",
    "        main_out = self.main_model(multispectral_preprocessed_out)\n",
    "        concat_out = torch.cat([multispectral_out, main_out], dim=1)\n",
    "\n",
    "        return self.fusion_block(concat_out)\n",
    "    \n",
    "preprocess1_layer = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=13, out_channels=13, kernel_size=1),\n",
    "    nn.BatchNorm2d(13)\n",
    ")\n",
    "model = models.segmentation.fcn_resnet50()\n",
    "conv1 = model.backbone.conv1\n",
    "\n",
    "weights = conv1.weight\n",
    "new_weight = torch.cat([weights.mean(dim=1).unsqueeze(1)]*13, dim=1)\n",
    "new_conv1 = nn.Conv2d(\n",
    "    in_channels=13,\n",
    "    out_channels=conv1.out_channels,\n",
    "    kernel_size=conv1.kernel_size,\n",
    "    stride=conv1.stride,\n",
    "    padding=conv1.padding,\n",
    "    dilation=conv1.dilation,\n",
    "    groups=conv1.groups,\n",
    "    bias=conv1.bias is not None\n",
    ")\n",
    "new_conv1.weight = nn.Parameter(new_weight)\n",
    "if conv1.bias is not None:\n",
    "    new_conv1.bias = model.backbone.conv1.bias\n",
    "model.backbone.conv1 = new_conv1\n",
    "model.classifier = models.segmentation.fcn.FCNHead(in_channels=2048, channels=9)\n",
    "model = FCNSegmentationWrapper(model)\n",
    "\n",
    "model = MultispectralFuseOut(model, preprocess1_layer, 13, 9)\n",
    "ret = model(torch.randn(1, 13, 150, 150))\n",
    "ret.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 1.,  2.],\n",
       "           [ 3.,  4.]],\n",
       " \n",
       "          [[ 5.,  6.],\n",
       "           [ 7.,  8.]],\n",
       " \n",
       "          [[ 9., 10.],\n",
       "           [11., 12.]]]], grad_fn=<ConvolutionBackward0>),\n",
       " tensor([[[[ 0.,  1.],\n",
       "           [ 2.,  3.]],\n",
       " \n",
       "          [[ 4.,  5.],\n",
       "           [ 6.,  7.]],\n",
       " \n",
       "          [[ 8.,  9.],\n",
       "           [10., 11.]]]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.ones((1, 3, 2, 2))\n",
    "t2 = torch.arange(0, 12,dtype=torch.float32).view(1, 3, 2, 2)\n",
    "t = torch.cat([t1, t2], dim=1)\n",
    "conv = nn.Conv2d(in_channels=6, out_channels=3, kernel_size=1, groups=6//2, bias=False)\n",
    "torch.nn.init.ones_(conv.weight)\n",
    "shuffle = nn.ChannelShuffle(groups=2)\n",
    "h = shuffle(t)\n",
    "\n",
    "conv(h), t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.],\n",
       "          [ 2.,  3.]],\n",
       "\n",
       "         [[ 4.,  5.],\n",
       "          [ 6.,  7.]],\n",
       "\n",
       "         [[ 8.,  9.],\n",
       "          [10., 11.]]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание парных мультиспектральных индексов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 150, 150])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MakeChannelsCombinations(nn.Module):\n",
    "    def __init__(self, combinations_list):\n",
    "        super().__init__()\n",
    "        self.combinations_list = combinations_list\n",
    "    def forward(self, x):\n",
    "        return x[:,self.combinations_list]\n",
    "\n",
    "class SpectralDiffIndexModule(nn.Module):\n",
    "    def __init__(self, channel_indices, channels_in_index, out_channels):\n",
    "        super().__init__()\n",
    "        self.channel_indices = channel_indices\n",
    "        combinations_list = list(combinations(channel_indices, channels_in_index))\n",
    "        \n",
    "        self.combinations_list = np.array(combinations_list).reshape(-1).tolist()\n",
    "        in_channels = len(self.combinations_list)\n",
    "        self.make_channels_combinations = MakeChannelsCombinations(self.combinations_list)\n",
    "        self.numerator = nn.Conv2d(in_channels=in_channels, out_channels=in_channels//channels_in_index, kernel_size=1, groups=in_channels//channels_in_index, bias=False)\n",
    "        self.denominator = nn.Conv2d(in_channels=in_channels, out_channels=in_channels//channels_in_index, kernel_size=1, groups=in_channels//channels_in_index, bias=False)\n",
    "        self.out_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels//channels_in_index, out_channels=out_channels, kernel_size=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            #nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        channels_combinations = x[:,self.combinations_list]\n",
    "        numerator_results = self.numerator(channels_combinations)\n",
    "        denominator_results = self.denominator(channels_combinations)\n",
    "        indices = numerator_results / (denominator_results+1e-7)\n",
    "        output = self.out_block(indices)\n",
    "        return output\n",
    "\n",
    "channel_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "channel_indices\n",
    "combinations_list = list(combinations(channel_indices, 2))\n",
    "data = torch.randn(1, len(channel_indices), 150, 150)\n",
    "\n",
    "combinations_list = np.array(combinations_list).reshape(-1).tolist()\n",
    "in_channels = len(combinations_list)\n",
    "channels_combinations = data[:,combinations_list]\n",
    "'''\n",
    "channels_combinations = []\n",
    "for combination in combinations_list:\n",
    "    channels_combination = data[:,combination]\n",
    "    channels_combinations.append()\n",
    "\n",
    "channels_combinations = torch.cat(channels_combinations, dim=1)\n",
    "channels_combinations.shape\n",
    "'''\n",
    "channels_combinations.shape\n",
    "\n",
    "numerator = nn.Conv2d(in_channels=in_channels, out_channels=in_channels//2, kernel_size=1, groups=in_channels//2, bias=False)\n",
    "denominator = nn.Conv2d(in_channels=in_channels, out_channels=in_channels//2, kernel_size=1, groups=in_channels//2, bias=False)\n",
    "#out_conv = nn.Conv2d(in_channels//2, )\n",
    "numerator_results = numerator(channels_combinations)\n",
    "denominator_results = denominator(channels_combinations)\n",
    "res = numerator_results / (denominator_results+1e-7)\n",
    "res.shape\n",
    "\n",
    "preprocess = SpectralDiffIndexModule(channel_indices=channel_indices, channels_in_index=2, out_channels=8)\n",
    "res = preprocess(data)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([78, 2, 1, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerator.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
