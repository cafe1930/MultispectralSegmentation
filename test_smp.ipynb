{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import tv_tensors\n",
    "from torchvision import models\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from itertools import combinations, product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'b_rgb': [1, 2, 3], 'st_1': (1, 1), 'w_rnd': None},\n",
       " {'b_rgb': [1, 2, 3], 'st_1': (1, 1), 'w_pr': 'imagenet'},\n",
       " {'b_rgb': [1, 2, 3], 'st_2': (2, 2), 'w_rnd': None},\n",
       " {'b_rgb': [1, 2, 3], 'st_2': (2, 2), 'w_pr': 'imagenet'},\n",
       " {'b_10m': [1, 2, 3, 7], 'st_1': (1, 1), 'w_rnd': None},\n",
       " {'b_10m': [1, 2, 3, 7], 'st_1': (1, 1), 'w_pr': 'imagenet'},\n",
       " {'b_10m': [1, 2, 3, 7], 'st_2': (2, 2), 'w_rnd': None},\n",
       " {'b_10m': [1, 2, 3, 7], 'st_2': (2, 2), 'w_pr': 'imagenet'},\n",
       " {'b_10-20m': [1, 2, 3, 4, 5, 6, 7, 11, 12], 'st_1': (1, 1), 'w_rnd': None},\n",
       " {'b_10-20m': [1, 2, 3, 4, 5, 6, 7, 11, 12],\n",
       "  'st_1': (1, 1),\n",
       "  'w_pr': 'imagenet'},\n",
       " {'b_10-20m': [1, 2, 3, 4, 5, 6, 7, 11, 12], 'st_2': (2, 2), 'w_rnd': None},\n",
       " {'b_10-20m': [1, 2, 3, 4, 5, 6, 7, 11, 12],\n",
       "  'st_2': (2, 2),\n",
       "  'w_pr': 'imagenet'},\n",
       " {'b_full_sp': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
       "  'st_1': (1, 1),\n",
       "  'w_rnd': None},\n",
       " {'b_full_sp': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
       "  'st_1': (1, 1),\n",
       "  'w_pr': 'imagenet'},\n",
       " {'b_full_sp': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
       "  'st_2': (2, 2),\n",
       "  'w_rnd': None},\n",
       " {'b_full_sp': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12],\n",
       "  'st_2': (2, 2),\n",
       "  'w_pr': 'imagenet'},\n",
       " {'b_rgb-ndvi': [1, 2, 3, 'ndvi'], 'st_1': (1, 1), 'w_rnd': None},\n",
       " {'b_rgb-ndvi': [1, 2, 3, 'ndvi'], 'st_1': (1, 1), 'w_pr': 'imagenet'},\n",
       " {'b_rgb-ndvi': [1, 2, 3, 'ndvi'], 'st_2': (2, 2), 'w_rnd': None},\n",
       " {'b_rgb-ndvi': [1, 2, 3, 'ndvi'], 'st_2': (2, 2), 'w_pr': 'imagenet'},\n",
       " {'b_rgb-ndwi': [1, 2, 3, 'ndwi'], 'st_1': (1, 1), 'w_rnd': None},\n",
       " {'b_rgb-ndwi': [1, 2, 3, 'ndwi'], 'st_1': (1, 1), 'w_pr': 'imagenet'},\n",
       " {'b_rgb-ndwi': [1, 2, 3, 'ndwi'], 'st_2': (2, 2), 'w_rnd': None},\n",
       " {'b_rgb-ndwi': [1, 2, 3, 'ndwi'], 'st_2': (2, 2), 'w_pr': 'imagenet'},\n",
       " {'b_rgb-ndbi': [1, 2, 3, 'ndbi'], 'st_1': (1, 1), 'w_rnd': None},\n",
       " {'b_rgb-ndbi': [1, 2, 3, 'ndbi'], 'st_1': (1, 1), 'w_pr': 'imagenet'},\n",
       " {'b_rgb-ndbi': [1, 2, 3, 'ndbi'], 'st_2': (2, 2), 'w_rnd': None},\n",
       " {'b_rgb-ndbi': [1, 2, 3, 'ndbi'], 'st_2': (2, 2), 'w_pr': 'imagenet'},\n",
       " {'b_rgb-ndre': [1, 2, 3, 'ndre'], 'st_1': (1, 1), 'w_rnd': None},\n",
       " {'b_rgb-ndre': [1, 2, 3, 'ndre'], 'st_1': (1, 1), 'w_pr': 'imagenet'},\n",
       " {'b_rgb-ndre': [1, 2, 3, 'ndre'], 'st_2': (2, 2), 'w_rnd': None},\n",
       " {'b_rgb-ndre': [1, 2, 3, 'ndre'], 'st_2': (2, 2), 'w_pr': 'imagenet'},\n",
       " {'b_rgb-allind': [1, 2, 3, 'ndvi', 'ndwi', 'ndbi', 'ndre'],\n",
       "  'st_1': (1, 1),\n",
       "  'w_rnd': None},\n",
       " {'b_rgb-allind': [1, 2, 3, 'ndvi', 'ndwi', 'ndbi', 'ndre'],\n",
       "  'st_1': (1, 1),\n",
       "  'w_pr': 'imagenet'},\n",
       " {'b_rgb-allind': [1, 2, 3, 'ndvi', 'ndwi', 'ndbi', 'ndre'],\n",
       "  'st_2': (2, 2),\n",
       "  'w_rnd': None},\n",
       " {'b_rgb-allind': [1, 2, 3, 'ndvi', 'ndwi', 'ndbi', 'ndre'],\n",
       "  'st_2': (2, 2),\n",
       "  'w_pr': 'imagenet'}]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bands_combinations_list = [\n",
    "    ('b_rgb', [1, 2, 3]),\n",
    "    ('b_10m', [1, 2, 3, 7]),\n",
    "    ('b_10-20m', [1, 2, 3, 4, 5, 6, 7, 11, 12]),\n",
    "    ('b_full_sp', [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]),\n",
    "    ('b_rgb-ndvi', [1, 2, 3, 'ndvi']),\n",
    "    ('b_rgb-ndwi', [1, 2, 3, 'ndwi']),\n",
    "    ('b_rgb-ndbi', [1, 2, 3, 'ndbi']),\n",
    "    ('b_rgb-ndre', [1, 2, 3, 'ndre']),\n",
    "    ('b_rgb-allind', [1, 2, 3, 'ndvi', 'ndwi', 'ndbi', 'ndre']),\n",
    "]\n",
    "\n",
    "inconv_strides_list = {\n",
    "    ('st_1', (1, 1)),\n",
    "    ('st_2', (2, 2)),\n",
    "}\n",
    "pretrained_list = {\n",
    "    ('w_rnd', None),\n",
    "    ('w_pr', 'imagenet'),\n",
    "    \n",
    "}\n",
    "all_combinations_list = list(product(bands_combinations_list, inconv_strides_list, pretrained_list))\n",
    "\n",
    "all_combinations_list = [{n:v for n, v in entry} for entry in all_combinations_list]\n",
    "\n",
    "\n",
    "\n",
    "all_combinations_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'PIDOR': ''}"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import yaml\n",
    "d = {'PIDOR': ''}\n",
    "with open('saving_dir/pidor', 'w') as fd:\n",
    "    yaml.dump(d, fd)\n",
    "\n",
    "with open('saving_dir/pidor') as fd:\n",
    "    pi_dor = yaml.load(fd, yaml.Loader)\n",
    "\n",
    "pi_dor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for combination in all_combinations_list:\n",
    "    name_postfix = config_dict['name_postfix']\n",
    "    for comb_name, comb_val in combination:\n",
    "        if comb_name.startswith('b_'):\n",
    "            config_dict['multispecter_bands_indices'] = comb_val\n",
    "        elif comb_name.startswith('st_'):\n",
    "            config_dict['segmentation_nn']['input_layer_config']['stride'] = comb_val\n",
    "        elif comb_name.startswith('w_'):\n",
    "            config_dict['segmentation_nn']['params']['encoder_weights'] = comb_val\n",
    "        name_postfix = f'{name_postfix}_{comb_name}'\n",
    "\n",
    "    config_dict['name_postfix'] = name_postfix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = models.segmentation.deeplabv3_resnet50(weights=models.segmentation.DeepLabV3_ResNet50_Weights.DEFAULT)\n",
    "print(model.classifier[-1].weight)\n",
    "for layer in model.classifier.children():\n",
    "    for m in layer.modules():\n",
    "        m.reset_parameters()\n",
    "print(model.classifier[-1].weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['UNLABELED',\n",
       " 'buildings_territory',\n",
       " 'natural_ground',\n",
       " 'natural_grow',\n",
       " 'natural_wetland',\n",
       " 'natural_wood',\n",
       " 'quasi_natural_grow',\n",
       " 'transport',\n",
       " 'water']"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_dataset_root = r'I:\\LANDCOVER_DATA\\MULTISPECTRAL_SATELLITE_DATA\\DATA_FOR_TRAINIG'\n",
    "path_to_dataset_info_csv = os.path.join(path_to_dataset_root, 'data_info_table.csv')\n",
    "\n",
    "images_df = pd.read_csv(path_to_dataset_info_csv)\n",
    "path_to_surface_classes_json = os.path.join(path_to_dataset_root, 'surface_classes.json')\n",
    "with open(path_to_surface_classes_json) as fd:\n",
    "    surface_classes_list = json.load(fd)\n",
    "surface_classes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(1, 3, 3)\n",
    "dir(conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 64, 150, 150])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class FCNSegmentationWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    def forward(self, x):\n",
    "        return self.model(x)['out']\n",
    "    \n",
    "class MultispectralNN(nn.Module):\n",
    "    def __init__(self, main_model, preprocessing_block):\n",
    "        super().__init__()\n",
    "        self.preprocessing_block = preprocessing_block\n",
    "        self.main_model = main_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.preprocessing_block(x)\n",
    "        return self.model(x)\n",
    "    \n",
    "\n",
    "in_channels = 13\n",
    "cols = rows = 150\n",
    "input_tensor = torch.randn(1, 13, cols, rows)\n",
    "conv = nn.Conv2d(in_channels, 64, kernel_size=[1,1])\n",
    "conv(input_tensor).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 13, 150, 150])\n",
      "torch.Size([1, 9, 150, 150])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 9, 150, 150])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MultispectralFuseOut(nn.Module):\n",
    "    def __init__(self, main_model, multispectral_preprocessing_block, preprocessing_out_dim, class_num):\n",
    "        super().__init__()\n",
    "        self.multispectral_preprocessing_block = multispectral_preprocessing_block\n",
    "        self.main_model = main_model\n",
    "        self.multispectral_preout_block = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=preprocessing_out_dim,out_channels=class_num, kernel_size=1),\n",
    "            nn.BatchNorm2d(class_num),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fusion_block = nn.Sequential(\n",
    "            #nn.Dropout2d(0.3),\n",
    "            nn.ChannelShuffle(groups=2),\n",
    "            nn.Conv2d(in_channels=class_num*2, out_channels=class_num, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        multispectral_preprocessed_out = self.multispectral_preprocessing_block(x)\n",
    "        multispectral_out = self.multispectral_preout_block(multispectral_preprocessed_out)\n",
    "        print(multispectral_preprocessed_out.shape)\n",
    "        print(multispectral_out.shape)\n",
    "        main_out = self.main_model(multispectral_preprocessed_out)\n",
    "        concat_out = torch.cat([multispectral_out, main_out], dim=1)\n",
    "\n",
    "        return self.fusion_block(concat_out)\n",
    "    \n",
    "preprocess1_layer = nn.Sequential(\n",
    "    nn.Conv2d(in_channels=13, out_channels=13, kernel_size=1),\n",
    "    nn.BatchNorm2d(13)\n",
    ")\n",
    "model = models.segmentation.fcn_resnet50()\n",
    "conv1 = model.backbone.conv1\n",
    "\n",
    "weights = conv1.weight\n",
    "new_weight = torch.cat([weights.mean(dim=1).unsqueeze(1)]*13, dim=1)\n",
    "new_conv1 = nn.Conv2d(\n",
    "    in_channels=13,\n",
    "    out_channels=conv1.out_channels,\n",
    "    kernel_size=conv1.kernel_size,\n",
    "    stride=conv1.stride,\n",
    "    padding=conv1.padding,\n",
    "    dilation=conv1.dilation,\n",
    "    groups=conv1.groups,\n",
    "    bias=conv1.bias is not None\n",
    ")\n",
    "new_conv1.weight = nn.Parameter(new_weight)\n",
    "if conv1.bias is not None:\n",
    "    new_conv1.bias = model.backbone.conv1.bias\n",
    "model.backbone.conv1 = new_conv1\n",
    "model.classifier = models.segmentation.fcn.FCNHead(in_channels=2048, channels=9)\n",
    "model = FCNSegmentationWrapper(model)\n",
    "\n",
    "model = MultispectralFuseOut(model, preprocess1_layer, 13, 9)\n",
    "ret = model(torch.randn(1, 13, 150, 150))\n",
    "ret.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[[ 1.,  2.],\n",
       "           [ 3.,  4.]],\n",
       " \n",
       "          [[ 5.,  6.],\n",
       "           [ 7.,  8.]],\n",
       " \n",
       "          [[ 9., 10.],\n",
       "           [11., 12.]]]], grad_fn=<ConvolutionBackward0>),\n",
       " tensor([[[[ 0.,  1.],\n",
       "           [ 2.,  3.]],\n",
       " \n",
       "          [[ 4.,  5.],\n",
       "           [ 6.,  7.]],\n",
       " \n",
       "          [[ 8.,  9.],\n",
       "           [10., 11.]]]]))"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = torch.ones((1, 3, 2, 2))\n",
    "t2 = torch.arange(0, 12,dtype=torch.float32).view(1, 3, 2, 2)\n",
    "t = torch.cat([t1, t2], dim=1)\n",
    "conv = nn.Conv2d(in_channels=6, out_channels=3, kernel_size=1, groups=6//2, bias=False)\n",
    "torch.nn.init.ones_(conv.weight)\n",
    "shuffle = nn.ChannelShuffle(groups=2)\n",
    "h = shuffle(t)\n",
    "\n",
    "conv(h), t2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.,  1.],\n",
       "          [ 2.,  3.]],\n",
       "\n",
       "         [[ 4.,  5.],\n",
       "          [ 6.,  7.]],\n",
       "\n",
       "         [[ 8.,  9.],\n",
       "          [10., 11.]]]])"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание парных мультиспектральных индексов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 8, 150, 150])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class MakeChannelsCombinations(nn.Module):\n",
    "    def __init__(self, combinations_list):\n",
    "        super().__init__()\n",
    "        self.combinations_list = combinations_list\n",
    "    def forward(self, x):\n",
    "        return x[:,self.combinations_list]\n",
    "\n",
    "class SpectralDiffIndexModule(nn.Module):\n",
    "    def __init__(self, channel_indices, channels_in_index, out_channels):\n",
    "        super().__init__()\n",
    "        self.channel_indices = channel_indices\n",
    "        combinations_list = list(combinations(channel_indices, channels_in_index))\n",
    "        \n",
    "        self.combinations_list = np.array(combinations_list).reshape(-1).tolist()\n",
    "        in_channels = len(self.combinations_list)\n",
    "        self.make_channels_combinations = MakeChannelsCombinations(self.combinations_list)\n",
    "        self.numerator = nn.Conv2d(in_channels=in_channels, out_channels=in_channels//channels_in_index, kernel_size=1, groups=in_channels//channels_in_index, bias=False)\n",
    "        self.denominator = nn.Conv2d(in_channels=in_channels, out_channels=in_channels//channels_in_index, kernel_size=1, groups=in_channels//channels_in_index, bias=False)\n",
    "        self.out_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels//channels_in_index, out_channels=out_channels, kernel_size=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            #nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        channels_combinations = x[:,self.combinations_list]\n",
    "        numerator_results = self.numerator(channels_combinations)\n",
    "        denominator_results = self.denominator(channels_combinations)\n",
    "        indices = numerator_results / (denominator_results+1e-7)\n",
    "        output = self.out_block(indices)\n",
    "        return output\n",
    "\n",
    "channel_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "channel_indices\n",
    "combinations_list = list(combinations(channel_indices, 2))\n",
    "data = torch.randn(1, len(channel_indices), 150, 150)\n",
    "\n",
    "combinations_list = np.array(combinations_list).reshape(-1).tolist()\n",
    "in_channels = len(combinations_list)\n",
    "channels_combinations = data[:,combinations_list]\n",
    "'''\n",
    "channels_combinations = []\n",
    "for combination in combinations_list:\n",
    "    channels_combination = data[:,combination]\n",
    "    channels_combinations.append()\n",
    "\n",
    "channels_combinations = torch.cat(channels_combinations, dim=1)\n",
    "channels_combinations.shape\n",
    "'''\n",
    "channels_combinations.shape\n",
    "\n",
    "numerator = nn.Conv2d(in_channels=in_channels, out_channels=in_channels//2, kernel_size=1, groups=in_channels//2, bias=False)\n",
    "denominator = nn.Conv2d(in_channels=in_channels, out_channels=in_channels//2, kernel_size=1, groups=in_channels//2, bias=False)\n",
    "#out_conv = nn.Conv2d(in_channels//2, )\n",
    "numerator_results = numerator(channels_combinations)\n",
    "denominator_results = denominator(channels_combinations)\n",
    "res = numerator_results / (denominator_results+1e-7)\n",
    "res.shape\n",
    "\n",
    "preprocess = SpectralDiffIndexModule(channel_indices=channel_indices, channels_in_index=2, out_channels=8)\n",
    "res = preprocess(data)\n",
    "res.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([78, 2, 1, 1])"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "numerator.weight.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
