{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import tv_tensors\n",
    "from torchvision import models\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "class SegmentationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path_to_dataset_root, samples_df,channel_indices, transforms, device):\n",
    "        '''\n",
    "        path_to_dataset - путь до корневой папки с датасетом\n",
    "        instance_names_list - список имен экземпляров БЕЗ РАСШИРЕНИЯ!\n",
    "        transforms - аугментация изображений\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.path_to_dataset_root = path_to_dataset_root\n",
    "        self.samples_df = samples_df\n",
    "        self.channel_indices = channel_indices\n",
    "        self.transforms = transforms\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples_df.iloc[idx]\n",
    "\n",
    "        file_name = sample['file_name']\n",
    "\n",
    "        path_to_image = os.path.join(self.path_to_dataset_root, 'images', f'{file_name}.npy')\n",
    "        path_to_labels = os.path.join(self.path_to_dataset_root, 'labels', f'{file_name}.npy')\n",
    "\n",
    "        image = torch.as_tensor(np.load(path_to_image), dtype=torch.int16)[self.channel_indices]\n",
    "        #image = np.load(path_to_image)\n",
    "        # метки читаем как одноканальное изображение\n",
    "        label = torch.as_tensor(np.load(path_to_labels), dtype=torch.uint8).long()\n",
    "        \n",
    "        \n",
    "        image = tv_tensors.Image(image, device=self.device)\n",
    "        label = tv_tensors.Mask(label, device=self.device)\n",
    "\n",
    "        transforms_dict = {'image':image, 'mask':label}\n",
    "        transformed = self.transforms(transforms_dict)\n",
    "        return transformed['image'], transformed['mask']#, image\n",
    "    \n",
    "class SegmentationDatasetAppl(torch.utils.data.Dataset):\n",
    "    def __init__(self, path_to_dataset_root, samples_df,channel_indices, name2class_idx_dict, applicable_surfaces_dict, transforms, device):\n",
    "        super().__init__()\n",
    "        self.path_to_dataset_root = path_to_dataset_root\n",
    "        self.samples_df = samples_df\n",
    "        self.channel_indices = channel_indices\n",
    "        self.transforms = transforms\n",
    "        self.device = device\n",
    "        self.applicable_surfaces_dict = applicable_surfaces_dict\n",
    "        self.idx2appl = {int(v): 'appl' if v else 'non_appl' for v in applicable_surfaces_dict.values()}\n",
    "        self.applicable_indices = [name2class_idx_dict[cl] for cl, ap in applicable_surfaces_dict.items() if ap]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples_df.iloc[idx]\n",
    "\n",
    "        file_name = sample['file_name']\n",
    "\n",
    "        path_to_image = os.path.join(self.path_to_dataset_root, 'images', f'{file_name}.npy')\n",
    "        path_to_labels = os.path.join(self.path_to_dataset_root, 'labels', f'{file_name}.npy')\n",
    "\n",
    "        image = torch.as_tensor(np.load(path_to_image), dtype=torch.int16)[self.channel_indices]\n",
    "        #image = np.load(path_to_image)\n",
    "        # метки читаем как одноканальное изображение\n",
    "        label = torch.as_tensor(np.load(path_to_labels), dtype=torch.uint8).long()\n",
    "\n",
    "        applicable_filter = label == self.applicable_indices[0]\n",
    "        for appl_i in self.applicable_indices[1:]:\n",
    "            applicable_filter = applicable_filter | (label == appl_i)\n",
    "        applicable_label = torch.where(applicable_filter==True, 1, 0)\n",
    "        \n",
    "        \n",
    "        image = tv_tensors.Image(image, device=self.device)\n",
    "        applicable_label = tv_tensors.Mask(applicable_label, device=self.device)\n",
    "\n",
    "        transforms_dict = {'image':image, 'mask':applicable_label}\n",
    "        transformed = self.transforms(transforms_dict)\n",
    "        return transformed['image'], transformed['mask']#, image\n",
    "    \n",
    "class FCNSegmentationWrapper(nn.Module):\n",
    "    def __init__(self, model):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    def forward(self, x):\n",
    "        return self.model(x)['out']\n",
    "    \n",
    "class MultispectralNN(nn.Module):\n",
    "    def __init__(self, main_model, preprocessing_block):\n",
    "        super().__init__()\n",
    "        self.preprocessing_block = preprocessing_block\n",
    "        self.main_model = main_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.preprocessing_block(x)\n",
    "        return self.main_model(x)\n",
    "    \n",
    "class MultispectralNNOutProcess(nn.Module):\n",
    "    def __init__(self, main_model, preprocessing_block, out_block):\n",
    "        super().__init__()\n",
    "        self.preprocessing_block = preprocessing_block\n",
    "        self.main_model = main_model\n",
    "        self.out_block = out_block\n",
    "\n",
    "    def forward(self, x):\n",
    "        preprocessed = self.preprocessing_block(x)\n",
    "        result = self.main_model(x)\n",
    "        return self.out_block(result, preprocessed)\n",
    "    \n",
    "class MultispectralDataOutput(nn.Module):\n",
    "    def __init__(self, in_channels, class_num):\n",
    "        super().__init__()\n",
    "        self.low_level_conv = nn.Conv1d(in_channels, class_num, kernel_size=1)\n",
    "        self.out_conv = nn.Conv1d(class_num*2, class_num, kernel_size=1)\n",
    "    def forward(self, cnn_out, low_level_out):\n",
    "        low_level_results = self.low_level_conv(low_level_out)\n",
    "        nn.ChannelShuffle(groups=2)\n",
    "\n",
    "class MultispectralFuseOut(nn.Module):\n",
    "    def __init__(self, main_model, multispectral_preprocessing_block, fusion_type, preprocessing_out_dim, class_num):\n",
    "        super().__init__()\n",
    "        self.multispectral_preprocessing_block = multispectral_preprocessing_block\n",
    "        self.main_model = main_model\n",
    "\n",
    "        self.multispectral_preout_block = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=preprocessing_out_dim,out_channels=class_num, kernel_size=1),\n",
    "            nn.BatchNorm2d(class_num),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fusion_type = fusion_type\n",
    "        if fusion_type == 'shuffle':\n",
    "            self.fusion_block = nn.Sequential(\n",
    "                #nn.Dropout2d(0.3),\n",
    "                nn.ChannelShuffle(groups=2),\n",
    "                nn.Conv2d(in_channels=class_num*2, out_channels=class_num, kernel_size=1, groups=class_num)\n",
    "            )\n",
    "        elif fusion_type == 'concat':\n",
    "            self.fusion_block = nn.Conv2d(in_channels=class_num*2, out_channels=class_num, kernel_size=1)\n",
    "        elif fusion_type == 'add':\n",
    "            self.fusion_block = nn.Conv2d(in_channels=class_num, out_channels=class_num, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        multispectral_preprocessed_out = self.multispectral_preprocessing_block(x)\n",
    "        multispectral_out = self.multispectral_preout_block(multispectral_preprocessed_out)\n",
    "        #print(multispectral_preprocessed_out.shape)\n",
    "        #print(multispectral_out.shape)\n",
    "        main_out = self.main_model(multispectral_preprocessed_out)\n",
    "        if self.fusion_type == 'add':\n",
    "            concat_out = multispectral_out + main_out\n",
    "        else:\n",
    "            concat_out = torch.cat([multispectral_out, main_out], dim=1)\n",
    "        \n",
    "\n",
    "        return self.fusion_block(concat_out)\n",
    "\n",
    "class MultitaskLoss(nn.Module):\n",
    "\n",
    "    def __init__(self, loss1, loss2):\n",
    "        super().__init__()\n",
    "        self.loss1 = loss1\n",
    "        self.loss2 = loss2\n",
    "        \n",
    "    def forward(self, pred1, pred2, target1, target2):\n",
    "        loss_val1 = self.loss1(pred1, target1)\n",
    "        loss_val2 = self.loss1(pred2, target2)\n",
    "        return loss_val1 + loss_val2\n",
    "        #applicable_target = \n",
    "        #coarse_target = \n",
    "\n",
    "class MultitaskModel(nn.Module):\n",
    "    def __init__(self, model, nn_output_size, appl_class_num, surf_class_num):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.applicable_head = nn.Sequential(\n",
    "            #nn.Conv2d(nn_output_size, out_channels=nn_output_size//4, kernel_size=3, padding=1),\n",
    "            #nn.BatchNorm2d(nn_output_size//4),\n",
    "            #nn.Dropout2d(p=0.3),\n",
    "            #nn.Conv2d(nn_output_size//4, appl_class_num, kernel_size=1)\n",
    "            nn.Conv2d(nn_output_size, appl_class_num, kernel_size=1)\n",
    "        )\n",
    "        self.surf_head = nn.Sequential(\n",
    "            #nn.Conv2d(nn_output_size, out_channels=nn_output_size//4, kernel_size=3, padding=1),\n",
    "            #nn.BatchNorm2d(nn_output_size//4),\n",
    "            #nn.Dropout2d(p=0.3),\n",
    "            #nn.Conv2d(nn_output_size//4, surf_class_num, kernel_size=1)\n",
    "            nn.Conv2d(nn_output_size, surf_class_num, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        h = self.model(x)\n",
    "        appl_out = self.applicable_head(h)\n",
    "        surf_out = self.surf_head(h)\n",
    "        return appl_out, surf_out\n",
    "\n",
    "class SpectralDiffIndexModule(nn.Module):\n",
    "    def __init__(self, channel_indices_list, channels_in_index, out_channels):\n",
    "        super().__init__()\n",
    "        self.channel_indices_list = channel_indices_list\n",
    "        combinations_list = list(combinations(channel_indices_list, channels_in_index))\n",
    "        \n",
    "        self.combinations_list = np.array(combinations_list).reshape(-1).tolist()\n",
    "        in_channels = len(self.combinations_list)\n",
    "        #self.make_channels_combinations = MakeChannelsCombinations(self.combinations_list)\n",
    "        self.numerator = nn.Conv2d(in_channels=in_channels, out_channels=in_channels//channels_in_index, kernel_size=1, groups=in_channels//channels_in_index, bias=False)\n",
    "        self.denominator = nn.Conv2d(in_channels=in_channels, out_channels=in_channels//channels_in_index, kernel_size=1, groups=in_channels//channels_in_index, bias=False)\n",
    "        self.out_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels//channels_in_index, out_channels=out_channels, kernel_size=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            #nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        channels_combinations = x[:,self.combinations_list]\n",
    "        numerator_results = self.numerator(channels_combinations)\n",
    "        denominator_results = self.denominator(channels_combinations)\n",
    "        indices = numerator_results / (denominator_results+1e-7)\n",
    "        output = self.out_block(indices)\n",
    "        return output\n",
    "\n",
    "def decode_confusion_matrix_2x2(confusion_matrix):\n",
    "    tp = confusion_matrix[1, 1]\n",
    "    tn = confusion_matrix[0, 0]\n",
    "    fp = confusion_matrix[0, 1]\n",
    "    fn = confusion_matrix[1, 0]\n",
    "    return {'tp':tp, 'tn': tn, 'fp': fp, 'fn': fn}\n",
    "\n",
    "def compute_accuracy_from_confusion(multiclass_confusion_matrix):\n",
    "    confusion_sum = multiclass_confusion_matrix.sum(axis=0)\n",
    "    tp, tn, fp, fn = decode_confusion_matrix_2x2(confusion_sum)\n",
    "    accuracy = 0\n",
    "    if tp+tn+fp+fn != 0:\n",
    "        accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "    return accuracy\n",
    "\n",
    "def compute_metric_from_confusion(multiclass_confusion_matrix, metric_params_dict, idx2class_name_dict=None):\n",
    "    #print(f'metric_params_dict={metric_params_dict}')\n",
    "    mean_metric = 0\n",
    "    # {class_name: iou_val}\n",
    "    metric_dict = {}\n",
    "    actual_classes_num = 0\n",
    "    metric_name = metric_params_dict['name']\n",
    "    if metric_name == 'accuracy':\n",
    "        confusion_sum = multiclass_confusion_matrix.sum(axis=0)\n",
    "        confusion_vals_dict = decode_confusion_matrix_2x2(confusion_sum)\n",
    "        numerator = [confusion_vals_dict[v] for v in metric_params_dict['numerator']]\n",
    "        denominator = [confusion_vals_dict[v] for v in metric_params_dict['denominator']]\n",
    "        accuracy = 0\n",
    "        if np.sum(denominator) != 0:\n",
    "            class_metric = np.sum(numerator)/np.sum(denominator)\n",
    "        metric_dict[metric_name] = class_metric\n",
    "        return metric_dict\n",
    "\n",
    "    for idx, class_confusion in enumerate(multiclass_confusion_matrix):\n",
    "        #print(f'class_conf_shape={class_confusion}')\n",
    "        confusion_vals_dict = decode_confusion_matrix_2x2(class_confusion)\n",
    "        #print(f'confusion_vals_dict={confusion_vals_dict}')\n",
    "        #print(f'conf_sum={class_confusion.sum()};tn={confusion_vals_dict[\"tn\"]}')\n",
    "        if class_confusion.sum() != confusion_vals_dict['tn']:\n",
    "            actual_classes_num += 1\n",
    "\n",
    "        #print(f'actual_classes_num={actual_classes_num}')\n",
    "        class_metric = 0\n",
    "        numerator = [confusion_vals_dict[v] for v in metric_params_dict['numerator']]\n",
    "        denominator = [confusion_vals_dict[v] for v in metric_params_dict['denominator']]\n",
    "        #print(f'numerator={numerator}')\n",
    "        #print(f'denominator={denominator}')\n",
    "        if np.sum(denominator) != 0:\n",
    "            class_metric = np.sum(numerator)/np.sum(denominator)\n",
    "        mean_metric += class_metric\n",
    "        class_name = f'{metric_name}_{idx}'\n",
    "        if idx2class_name_dict is not None:\n",
    "            class_name = f'{metric_name}_{idx2class_name_dict[idx]}'\n",
    "        metric_dict[class_name] = class_metric\n",
    "    if actual_classes_num == 0:\n",
    "        metric_dict[f'{metric_name}_mean'] = 0\n",
    "    else:\n",
    "        metric_dict[f'{metric_name}_mean'] = mean_metric/actual_classes_num\n",
    "    return metric_dict\n",
    "\n",
    "def compute_pred_mask(pred):\n",
    "    pred = pred.detach()\n",
    "    _, pred_mask = pred.max(dim=1)\n",
    "    return pred_mask.cpu().numpy()\n",
    "\n",
    "class SegmentationModule(L.LightningModule):\n",
    "    def __init__(self, model, criterion, metrics_info_list, name2class_idx_dict) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.name2class_idx_dict = name2class_idx_dict\n",
    "        self.class_idx2name_dict = {v:k for k, v in name2class_idx_dict.items()}\n",
    "        self.train_confusion_matrix = np.zeros((len(self.name2class_idx_dict), 2, 2), dtype=np.int64)\n",
    "        self.val_confusion_matrix = np.zeros((len(self.name2class_idx_dict), 2, 2), dtype=np.int64)\n",
    "        self.metrics_info_list = metrics_info_list\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "    def compute_pred_lbels(self, pred):\n",
    "        return pred.max(dim=1)\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, true_labels = batch\n",
    "        pred = self.model(data)\n",
    "        loss = self.criterion(pred, true_labels)\n",
    "        \n",
    "        pred_labels = compute_pred_mask(pred)\n",
    "        true_labels = true_labels.detach().cpu().numpy()\n",
    "        batch_confusion_matrix = metrics.multilabel_confusion_matrix(true_labels.reshape(-1), pred_labels.reshape(-1),labels=list(self.class_idx2name_dict.keys()))\n",
    "        #print(f'train_batch_conf_type={batch_confusion_matrix.dtype}')\n",
    "        #print(batch_confusion_matrix)\n",
    "        self.train_confusion_matrix += batch_confusion_matrix.astype(np.int64)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data, true_labels = batch\n",
    "        pred = self.model(data)\n",
    "        loss = self.criterion(pred, true_labels)\n",
    "        pred_labels = compute_pred_mask(pred)\n",
    "        true_labels = true_labels.detach().cpu().numpy()\n",
    "        batch_confusion_matrix = metrics.multilabel_confusion_matrix(true_labels.reshape(-1), pred_labels.reshape(-1),labels=list(self.class_idx2name_dict.keys()))\n",
    "        #print(f'pred_labels type={pred_labels.dtype}')\n",
    "        #print(f'true_labels type={true_labels.dtype}')\n",
    "        #print(f'val_batch_conf_type={batch_confusion_matrix.dtype}')\n",
    "        #print(batch_confusion_matrix)\n",
    "        self.val_confusion_matrix += batch_confusion_matrix.astype(np.int64)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        for metric_info_dict in self.metrics_info_list:\n",
    "            metric_dict = compute_metric_from_confusion(self.train_confusion_matrix, metric_info_dict, self.class_idx2name_dict)\n",
    "            for name, value in metric_dict.items():\n",
    "                name = f'train_{name}'\n",
    "                self.log(name, value, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.train_confusion_matrix = np.zeros((len(self.name2class_idx_dict), 2, 2), dtype=np.int64)\n",
    "        \n",
    "    \n",
    "    def on_validation_epoch_end(self):\n",
    "        for metric_info_dict in self.metrics_info_list:\n",
    "            metric_dict = compute_metric_from_confusion(self.val_confusion_matrix, metric_info_dict, self.class_idx2name_dict)\n",
    "            for name, value in metric_dict.items():\n",
    "                name = f'val_{name}'\n",
    "                self.log(name, value, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.val_confusion_matrix = np.zeros((len(self.name2class_idx_dict), 2, 2), dtype=np.int64)\n",
    "\n",
    "class MultitaskSegmentationModule(L.LightningModule):\n",
    "    def __init__(self, model, criterion, metrics_info_list, name2class_idx_dict, applicable_surfaces_dict) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.name2class_idx_dict = name2class_idx_dict\n",
    "        self.class_idx2name_dict = {v:k for k, v in name2class_idx_dict.items()}\n",
    "        self.idx2appl = {int(v): 'appl' if v else 'non_appl' for v in applicable_surfaces_dict.values()}\n",
    "        self.applicable_indices = [name2class_idx_dict[cl] for cl, ap in applicable_surfaces_dict.items() if ap]\n",
    "        self.classes_train_confusion_matrix = np.zeros((len(self.name2class_idx_dict), 2, 2), dtype=np.int64)\n",
    "        self.classes_val_confusion_matrix = np.zeros((len(self.name2class_idx_dict), 2, 2), dtype=np.int64)\n",
    "        self.applicable_train_confusion_matrix = np.zeros((len(self.idx2appl), 2, 2), dtype=np.int64)\n",
    "        self.applicable_val_confusion_matrix = np.zeros((len(self.idx2appl), 2, 2), dtype=np.int64)\n",
    "        self.metrics_info_list = metrics_info_list\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "    def compute_pred_lbels(self, pred):\n",
    "        return pred.max(dim=1)\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, classes_true_labels = batch\n",
    "\n",
    "        applicable_filter = classes_true_labels == self.applicable_indices[0]\n",
    "        for appl_i in self.applicable_indices[1:]:\n",
    "            applicable_filter = applicable_filter | (classes_true_labels == appl_i)\n",
    "        applicable_true_labels = torch.where(applicable_filter==True, 1, 0)\n",
    "\n",
    "        applicable_pred, classes_pred = self.model(data)\n",
    "\n",
    "        loss = self.criterion(applicable_pred, classes_pred, applicable_true_labels, classes_true_labels)\n",
    "        \n",
    "        applicable_pred_labels = compute_pred_mask(applicable_pred)\n",
    "        classes_pred_labels = compute_pred_mask(classes_pred)\n",
    "        applicable_true_labels = applicable_true_labels.detach().cpu().numpy()\n",
    "        classes_true_labels = classes_true_labels.detach().cpu().numpy()\n",
    "        classes_batch_confusion_matrix = metrics.multilabel_confusion_matrix(classes_true_labels.reshape(-1), classes_pred_labels.reshape(-1),labels=list(self.class_idx2name_dict.keys()))\n",
    "        applicable_batch_confusion_matrix = metrics.multilabel_confusion_matrix(applicable_true_labels.reshape(-1), applicable_pred_labels.reshape(-1),labels=[0, 1])\n",
    "        #print(f'train_batch_conf_type={batch_confusion_matrix.dtype}')\n",
    "        #print(batch_confusion_matrix)\n",
    "        self.classes_train_confusion_matrix += classes_batch_confusion_matrix.astype(np.int64)\n",
    "        self.applicable_train_confusion_matrix += applicable_batch_confusion_matrix.astype(np.int64)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data, classes_true_labels = batch\n",
    "\n",
    "        applicable_filter = classes_true_labels == self.applicable_indices[0]\n",
    "        for appl_i in self.applicable_indices[1:]:\n",
    "            applicable_filter = applicable_filter | (classes_true_labels == appl_i)\n",
    "        applicable_true_labels = torch.where(applicable_filter==True, 1, 0)\n",
    "\n",
    "        applicable_pred, classes_pred = self.model(data)\n",
    "\n",
    "        loss = self.criterion(applicable_pred, classes_pred, applicable_true_labels, classes_true_labels)\n",
    "        \n",
    "        applicable_pred_labels = compute_pred_mask(applicable_pred)\n",
    "        classes_pred_labels = compute_pred_mask(classes_pred)\n",
    "        applicable_true_labels = applicable_true_labels.detach().cpu().numpy()\n",
    "        classes_true_labels = classes_true_labels.detach().cpu().numpy()\n",
    "        classes_batch_confusion_matrix = metrics.multilabel_confusion_matrix(classes_true_labels.reshape(-1), classes_pred_labels.reshape(-1),labels=list(self.class_idx2name_dict.keys()))\n",
    "        applicable_batch_confusion_matrix = metrics.multilabel_confusion_matrix(applicable_true_labels.reshape(-1), applicable_pred_labels.reshape(-1),labels=[0, 1])\n",
    "        #print(f'train_batch_conf_type={batch_confusion_matrix.dtype}')\n",
    "        #print(batch_confusion_matrix)\n",
    "        self.classes_val_confusion_matrix += classes_batch_confusion_matrix.astype(np.int64)\n",
    "        self.applicable_val_confusion_matrix += applicable_batch_confusion_matrix.astype(np.int64)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        for metric_info_dict in self.metrics_info_list:\n",
    "            metric_name = metric_info_dict['name']\n",
    "            class_metric_dict = compute_metric_from_confusion(self.classes_train_confusion_matrix, metric_info_dict, self.class_idx2name_dict)\n",
    "            mean = 0\n",
    "            for name, value in class_metric_dict.items():\n",
    "                if 'mean' in name:\n",
    "                    class_mean = value\n",
    "                name = f'tr_cl_{name}'\n",
    "                self.log(name, value, on_step=False, on_epoch=True, prog_bar=True)\n",
    "            appl_metric_dict = compute_metric_from_confusion(self.applicable_train_confusion_matrix, metric_info_dict, self.idx2appl)\n",
    "            for name, value in appl_metric_dict.items():\n",
    "                if 'mean' in name:\n",
    "                    appl_mean = value\n",
    "                name = f'tr_ap_{name}'\n",
    "                self.log(name, value, on_step=False, on_epoch=True, prog_bar=True)\n",
    "            mean = (class_mean+appl_mean)/2\n",
    "            self.log(f'tr_{metric_name}_mean', mean, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.classes_train_confusion_matrix = np.zeros((len(self.name2class_idx_dict), 2, 2), dtype=np.int64)\n",
    "        self.applicable_train_confusion_matrix = np.zeros((len(self.idx2appl), 2, 2), dtype=np.int64)\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        for metric_info_dict in self.metrics_info_list:\n",
    "            metric_name = metric_info_dict['name']\n",
    "            class_metric_dict = compute_metric_from_confusion(self.classes_val_confusion_matrix, metric_info_dict, self.class_idx2name_dict)\n",
    "            mean = 0\n",
    "            for name, value in class_metric_dict.items():\n",
    "                if 'mean' in name:\n",
    "                    class_mean = value\n",
    "                name = f'v_cl_{name}'\n",
    "                self.log(name, value, on_step=False, on_epoch=True, prog_bar=True)\n",
    "            appl_metric_dict = compute_metric_from_confusion(self.applicable_val_confusion_matrix, metric_info_dict, self.idx2appl)\n",
    "            for name, value in appl_metric_dict.items():\n",
    "                if 'mean' in name:\n",
    "                    appl_mean = value\n",
    "                name = f'v_ap_{name}'\n",
    "                self.log(name, value, on_step=False, on_epoch=True, prog_bar=True)\n",
    "            mean = (class_mean+appl_mean)/2\n",
    "            self.log(f'v_{metric_name}_mean', mean, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.classes_val_confusion_matrix = np.zeros((len(self.name2class_idx_dict), 2, 2), dtype=np.int64)\n",
    "        self.applicable_val_confusion_matrix = np.zeros((len(self.idx2appl), 2, 2), dtype=np.int64)\n",
    "\n",
    "def prepare_channel_indices_str(channel_indices_list):\n",
    "    last = None\n",
    "    first = None\n",
    "    intervals = []\n",
    "    for i, ch_idx in enumerate(sorted(channel_indices_list)):\n",
    "        #print(f'ch_idx={ch_idx}, last==ch_idx-1:{last==ch_idx-1}')\n",
    "        #print(f'first={first};last={last}')\n",
    "        if last is not None:\n",
    "            if last != ch_idx-1:\n",
    "                #last = prev_idx\n",
    "                if first == last:\n",
    "                    intervals.append([last])\n",
    "                else:\n",
    "                    intervals.append([first, last])\n",
    "\n",
    "                first = ch_idx\n",
    "                last = ch_idx\n",
    "            else:\n",
    "                last = ch_idx\n",
    "            if i == len(channel_indices_list)-1:\n",
    "                if first == last:\n",
    "                    intervals.append([last])\n",
    "                else:\n",
    "                    intervals.append([first, last])\n",
    "        else:\n",
    "            first = ch_idx\n",
    "            last = ch_idx\n",
    "\n",
    "        #channels_str += f'{ch},'\n",
    "    chanels_str = ''\n",
    "    for i, interval in enumerate(intervals):\n",
    "        if len(interval) == 1:\n",
    "            ptr = f'{interval[0]}'\n",
    "        else:\n",
    "            ptr = f'{interval[0]}-{interval[-1]}'\n",
    "        if i != len(intervals) - 1:\n",
    "            ptr = f'{ptr},'\n",
    "\n",
    "        chanels_str += ptr\n",
    "    return chanels_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_torch_model(\n",
    "        model_dict,\n",
    "        image_channels,\n",
    "        nn_in_cannels_num,\n",
    "        class_num,\n",
    "        channel_indices_list,\n",
    "        is_multitask,\n",
    "        preprocess_params,\n",
    "        fuze_params):\n",
    "    model_creation_unction = model_dict['creation_function']\n",
    "    weights = model_dict['weights']\n",
    "    model_name = model_dict['model_name']\n",
    "    model = model_creation_unction(weights=weights)\n",
    "    \n",
    "    # заменяем входной слой\n",
    "    conv1 = model.backbone.conv1\n",
    "\n",
    "    weights = conv1.weight\n",
    "    new_weight = torch.cat([weights.mean(dim=1).unsqueeze(1)]*nn_in_cannels_num, dim=1)\n",
    "    new_conv1 = nn.Conv2d(\n",
    "        in_channels=nn_in_cannels_num,\n",
    "        out_channels=conv1.out_channels,\n",
    "        kernel_size=conv1.kernel_size,\n",
    "        stride=conv1.stride,\n",
    "        padding=conv1.padding,\n",
    "        dilation=conv1.dilation,\n",
    "        groups=conv1.groups,\n",
    "        bias=conv1.bias is not None\n",
    "    )\n",
    "    new_conv1.weight = nn.Parameter(new_weight)\n",
    "    if conv1.bias is not None:\n",
    "        new_conv1.bias = model.backbone.conv1.bias\n",
    "    model.backbone.conv1 = new_conv1\n",
    "\n",
    "    # заменяем выходные слои\n",
    "    if is_multitask:\n",
    "        model.classifier = nn.Sequential(*list(model.classifier.children())[:-1])\n",
    "    else:\n",
    "        classifier_conv = model.classifier[-1]\n",
    "        new_classifier_conv = nn.Conv2d(\n",
    "            in_channels=classifier_conv.in_channels,\n",
    "            out_channels=class_num,\n",
    "            kernel_size=classifier_conv.kernel_size,\n",
    "            stride=classifier_conv.kernel_size,\n",
    "            padding=classifier_conv.padding,\n",
    "            dilation=classifier_conv.dilation,\n",
    "            groups=classifier_conv.groups,\n",
    "            bias=classifier_conv.bias is not None,\n",
    "            )\n",
    "        model.classifier[-1] = new_classifier_conv\n",
    "    #!!!!!\n",
    "    if 'fcn' in model_name.lower():\n",
    "        model.classifier = models.segmentation.fcn.FCNHead(in_channels=2048, channels=class_num)\n",
    "    elif 'dlv3' in model_name.lower():\n",
    "        model.classifier = models.segmentation.deeplabv3.DeepLabHead(in_channels=2048, num_classes=class_num)\n",
    "\n",
    "    model = FCNSegmentationWrapper(model)\n",
    "    if is_multitask:\n",
    "        model = MultitaskModel(model, nn_output_size=512, appl_class_num=2, surf_class_num=class_num)\n",
    "\n",
    "    if preprocess_params['type'] == 'no':\n",
    "        preprocess_layer = nn.Identity()\n",
    "    elif preprocess_params['type'] == '1L':\n",
    "        preprocess1_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=image_channels, out_channels=nn_in_cannels_num, kernel_size=1),\n",
    "            nn.BatchNorm2d(nn_in_cannels_num)\n",
    "        )\n",
    "    elif preprocess_params['type'] == '2L':\n",
    "        preprocess_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=image_channels, out_channels=32, kernel_size=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=nn_in_cannels_num, kernel_size=1),\n",
    "            nn.BatchNorm2d(nn_in_cannels_num),\n",
    "            nn.ReLU())\n",
    "    elif preprocess_params['type'] == 'SpInd':\n",
    "        preprocess_layer = SpectralDiffIndexModule(channel_indices_list=channel_indices_list, channels_in_index=2, out_channels=8)\n",
    "    \n",
    "    if fuze_params['type'] == 'no':\n",
    "        model = MultispectralNN(model, preprocess_layer)\n",
    "    elif fuze_params['type'] == 'no':\n",
    "        model = MultispectralFuseOut(model, preprocess_layer, preprocessing_out_dim=nn_in_cannels_num, fusion_type=fuze_params['type'], class_num=class_num)\n",
    "\n",
    "    train_transforms = v2.Compose([v2.ToDtype(torch.float32, scale=True)])\n",
    "    test_transforms = v2.Compose([v2.ToDtype(torch.float32, scale=True)])\n",
    "\n",
    "    multitask_str = '_MT' if is_multitask else ''\n",
    "    \n",
    "    model_name = f'{model_name}pr'\n",
    "    if preprocess_params['type'] != 'no':\n",
    "        model_name += f'-P{preprocess_params[\"type\"]}'\n",
    "    if fuze_params['type'] != 'no':\n",
    "        model_name += f'-Fuz{fuze_params[\"type\"].capitalize()}'\n",
    "    #model_name = f'{model_name}pr-P2L-FuzOutAdd({nn_in_cannels_num})' + multitask_str\n",
    "    \n",
    "    #model_name = f'{model_name}pr' + multitask_str\n",
    "\n",
    "    return {'name': model_name, 'model': model, 'train_transforms':train_transforms, 'test_transforms':test_transforms}\n",
    "\n",
    "def prepare_smp_model(\n",
    "        model_dict,\n",
    "        image_channels,\n",
    "        nn_in_cannels_num,\n",
    "        class_num,\n",
    "        channel_indices_list,\n",
    "        is_multitask,\n",
    "        preprocess_params,\n",
    "        fuze_params):\n",
    "    model_creation_unction = model_dict['creation_function']\n",
    "    encoder_name = model_dict['encoder_name']\n",
    "    model_name = model_dict['model_name']\n",
    "    model = model_creation_unction(encoder_name=encoder_name, in_channels=nn_in_cannels_num, classes=class_num)\n",
    "    \n",
    "    #model = smp.Unet(encoder_name='resnet50', classes=class_num)\n",
    "\n",
    "    conv1 = model.encoder.conv1\n",
    "\n",
    "    weights = conv1.weight\n",
    "    new_weight = torch.cat([weights.mean(dim=1).unsqueeze(1)]*nn_in_cannels_num, dim=1)\n",
    "    new_conv1 = nn.Conv2d(\n",
    "        in_channels=nn_in_cannels_num,\n",
    "        out_channels=conv1.out_channels,\n",
    "        kernel_size=conv1.kernel_size,\n",
    "        stride=conv1.stride,\n",
    "        padding=conv1.padding,\n",
    "        dilation=conv1.dilation,\n",
    "        groups=conv1.groups,\n",
    "        bias=conv1.bias is not None\n",
    "    )\n",
    "    new_conv1.weight = nn.Parameter(new_weight)\n",
    "    if conv1.bias is not None:\n",
    "        new_conv1.bias = model.encoder.conv1.bias\n",
    "    model.encoder.conv1 = new_conv1\n",
    "\n",
    "    if is_multitask:\n",
    "        model = MultitaskModel(model, nn_output_size=512, appl_class_num=2, surf_class_num=class_num)\n",
    "\n",
    "    if preprocess_params['type'] == 'no':\n",
    "        preprocess_layer = nn.Identity()\n",
    "    elif preprocess_params['type'] == '1L':\n",
    "        preprocess1_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=image_channels, out_channels=nn_in_cannels_num, kernel_size=1),\n",
    "            nn.BatchNorm2d(nn_in_cannels_num)\n",
    "        )\n",
    "    elif preprocess_params['type'] == '2L':\n",
    "        preprocess_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=image_channels, out_channels=32, kernel_size=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=nn_in_cannels_num, kernel_size=1),\n",
    "            nn.BatchNorm2d(nn_in_cannels_num),\n",
    "            nn.ReLU())\n",
    "    elif preprocess_params['type'] == 'SpInd':\n",
    "        preprocess_layer = SpectralDiffIndexModule(channel_indices_list=channel_indices_list, channels_in_index=2, out_channels=8)\n",
    "    \n",
    "    if fuze_params['type'] == 'no':\n",
    "        model = MultispectralNN(model, preprocess_layer)\n",
    "    elif fuze_params['type'] == 'no':\n",
    "        model = MultispectralFuseOut(model, preprocess_layer, preprocessing_out_dim=nn_in_cannels_num, fusion_type=fuze_params['type'], class_num=class_num)\n",
    "    \n",
    "    train_transforms = v2.Compose([v2.Resize((160,160), antialias=True),v2.ToDtype(torch.float32, scale=True)])\n",
    "    test_transforms = v2.Compose([v2.Resize((160,160), antialias=True),v2.ToDtype(torch.float32, scale=True)])\n",
    "    \n",
    "    multitask_str = '_MT' if is_multitask else ''\n",
    "    \n",
    "    model_name = f'{model_name}pr'\n",
    "    if preprocess_params['type'] != 'no':\n",
    "        model_name += f'-P{preprocess_params[\"type\"]}'\n",
    "    if fuze_params['type'] != 'no':\n",
    "        model_name += f'-Fuz{fuze_params[\"type\"].capitalize()}'\n",
    "    #model_name = f'{model_name}pr-P2L-FuzOutAdd({nn_in_cannels_num})' + multitask_str\n",
    "    return {'name': model_name, 'model': model, 'train_transforms':train_transforms, 'test_transforms':test_transforms}\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/resnet50-19c8e357.pth\" to C:\\Users\\mokhail/.cache\\torch\\hub\\checkpoints\\resnet50-19c8e357.pth\n",
      "100%|██████████| 97.8M/97.8M [00:08<00:00, 11.8MB/s]\n",
      "Using cache found in C:\\Users\\mokhail/.cache\\torch\\hub\\adeelh_pytorch-multi-class-focal-loss_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 9, 160, 160]) torch.Size([16, 3, 160, 160])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'unet_res50pr'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path_to_dataset_root = r'I:\\LANDCOVER_DATA\\MULTISPECTRAL_SATELLITE_DATA\\DATA_FOR_TRAINIG'\n",
    "path_to_dataset_root = r'I:\\LANDCOVER_DATA\\MULTISPECTRAL_SATELLITE_DATA\\DATA_FOR_TRAINIG'\n",
    "path_to_dataset_root = r'I:\\LANDCOVER_DATA\\MULTISPECTRAL_SATELLITE_DATA\\DATA_FOR_TRAINIG'\n",
    "path_to_dataset_info_csv = os.path.join(path_to_dataset_root, 'data_info_table.csv')\n",
    "path_to_surface_classes_json = os.path.join(path_to_dataset_root, 'surface_classes.json')\n",
    "with open(path_to_surface_classes_json) as fd:\n",
    "    surface_classes_list = json.load(fd)\n",
    "\n",
    "images_df = pd.read_csv(path_to_dataset_info_csv)\n",
    "\n",
    "path_to_partition_json = os.path.join(path_to_dataset_root, 'dataset_partition.json')\n",
    "#r'i:\\LANDCOVER_DATA\\MULTISPECTRAL_SATELLITE_DATA\\MULTISPECTRAL_DATA_FOR_TRAINIG\\dataset_partition.json'\n",
    "with open(path_to_partition_json) as fd:\n",
    "    partition_dict = json.load(fd)\n",
    "\n",
    "# заменить при перетасовке классов\n",
    "applicable_surfaces_dict = {\n",
    "    'UNLABELED': False,\n",
    "    'buildings_territory': False,\n",
    "    'natural_ground': True,\n",
    "    'natural_grow': True,\n",
    "    'natural_wetland': True,\n",
    "    'natural_wood': True,\n",
    "    'quasi_natural_grow': False,\n",
    "    'transport': False,\n",
    "    'water': False\n",
    "}\n",
    "\n",
    "train_images_df = []\n",
    "for train_square in partition_dict['train_squares']:\n",
    "    train_images_df.append(images_df[images_df['square_id']==train_square])\n",
    "train_images_df = pd.concat(train_images_df, ignore_index=True)\n",
    "\n",
    "test_images_df = []\n",
    "for test_square in partition_dict['test_squares']:\n",
    "    test_images_df.append(images_df[images_df['square_id']==test_square])\n",
    "test_images_df = pd.concat(test_images_df, ignore_index=True)\n",
    "\n",
    "#train_images_df, test_images_df = train_test_split(images_df, test_size=0.3, random_state=0)\n",
    "\n",
    "class_num = images_df['class_num'].iloc[0]\n",
    "\n",
    "class_name2idx_dict = {n:i for i, n in enumerate(surface_classes_list)}\n",
    "\n",
    "classes_pixels_distribution_df = images_df[surface_classes_list]\n",
    "classes_pixels_num = classes_pixels_distribution_df.sum()\n",
    "classes_weights = classes_pixels_num / classes_pixels_num.sum()\n",
    "classes_weights = classes_weights[surface_classes_list].to_numpy()\n",
    "\n",
    "transforms = v2.Compose([v2.ToDtype(torch.float32, scale=True)])\n",
    "#channel_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "#channel_indices = [1, 2, 3, 7]\n",
    "channel_indices = [1, 2, 3]\n",
    "in_channels_num = len(channel_indices)\n",
    "is_multitask = False\n",
    "'''\n",
    "deeplab_dict = {\n",
    "    'model_name': 'dlv3_res50',\n",
    "    'creation_function': models.segmentation.deeplabv3_resnet50,\n",
    "    'weights': models.segmentation.DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1\n",
    "}\n",
    "fcn_dict = {\n",
    "    'model_name': 'fcn_res50',\n",
    "    'creation_function': models.segmentation.fcn_resnet50,\n",
    "    'weights': models.segmentation.FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1\n",
    "}\n",
    "\n",
    "model_dict = prepare_torch_model(\n",
    "    model_dict=deeplab_dict,\n",
    "    image_channels=in_channels_num,\n",
    "    nn_in_cannels_num=in_channels_num,\n",
    "    class_num=class_num,\n",
    "    channel_indices_list=channel_indices,\n",
    "    is_multitask=is_multitask,\n",
    "    preprocess_params={'type':'no'},\n",
    "    fuze_params={'type':'no'}\n",
    "    )\n",
    "#model_dict  = prepare_torch_model(image_channels=in_channels_num, nn_in_cannels_num=in_channels_num, class_num=class_num, channel_indices_list=channel_indices, is_multitask=is_multitask)\n",
    "#model_dict  = prepare_torch_model(image_channels=in_channels_num, nn_in_cannels_num=in_channels_num, class_num=2, channel_indices_list=channel_indices, is_multitask=is_multitask)\n",
    "#model_dict  = prepare_torch_model(image_channels=in_channels_num, nn_in_cannels_num=in_channels_num, class_num=class_num, channel_indices_list=channel_indices, is_multitask=is_multitask)\n",
    "'''\n",
    "unet_dict = {\n",
    "    'model_name': 'unet_res50',\n",
    "    'creation_function': smp.Unet,\n",
    "    'encoder_name': 'resnet50'\n",
    "}\n",
    "model_dict  = prepare_smp_model(\n",
    "    model_dict=unet_dict,\n",
    "    image_channels=in_channels_num,\n",
    "    nn_in_cannels_num=in_channels_num,\n",
    "    class_num=class_num,\n",
    "    channel_indices_list=channel_indices,\n",
    "    is_multitask=False,\n",
    "    preprocess_params={'type':'no'},\n",
    "    fuze_params={'type':'no'}\n",
    "    )\n",
    "\n",
    "model_name = model_dict['name']\n",
    "model = model_dict['model']\n",
    "train_transforms = model_dict['train_transforms']\n",
    "test_transforms = model_dict['test_transforms']\n",
    "\n",
    "device = torch.device('cuda:0')\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "gamma_val = 2\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "multitask_criterion = MultitaskLoss(nn.CrossEntropyLoss(), nn.CrossEntropyLoss())\n",
    "classes_weights = torch.as_tensor(classes_weights, dtype=torch.float32, device=device)\n",
    "focal_criterion = torch.hub.load(\n",
    "        'adeelh/pytorch-multi-class-focal-loss',\n",
    "        model='FocalLoss',\n",
    "        alpha=classes_weights,\n",
    "        gamma=gamma_val,\n",
    "        reduction='mean',\n",
    "        force_reload=False\n",
    "    )\n",
    "\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "test_opt = torch.optim.Adam(model.parameters())\n",
    "\n",
    "train_dataset = SegmentationDataset(path_to_dataset_root=path_to_dataset_root, samples_df=train_images_df, channel_indices=channel_indices, transforms=train_transforms, device=device)\n",
    "test_dataset = SegmentationDataset(path_to_dataset_root=path_to_dataset_root, samples_df=test_images_df,channel_indices=channel_indices, transforms=test_transforms, device=device)\n",
    "#train_dataset = SegmentationDatasetAppl(path_to_dataset_root=path_to_dataset_root, samples_df=test_images_df, channel_indices=channel_indices, name2class_idx_dict=class_name2idx_dict, applicable_surfaces_dict=applicable_surfaces_dict, transforms=test_transforms, device=device)\n",
    "#test_dataset = SegmentationDatasetAppl(path_to_dataset_root=path_to_dataset_root, samples_df=test_images_df, channel_indices=channel_indices, name2class_idx_dict=class_name2idx_dict, applicable_surfaces_dict=applicable_surfaces_dict, transforms=test_transforms, device=device)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "for data, labels in train_loader:\n",
    "    break\n",
    "    pred = model(data)\n",
    "    loss = criterion(pred, labels)\n",
    "\n",
    "ret = model(data)\n",
    "if is_multitask:\n",
    "    img_size = list(ret[0].shape[2:])\n",
    "    print(ret[0].shape, ret[1].shape)\n",
    "else:\n",
    "    img_size = list(ret.shape[2:])\n",
    "    print(ret.shape, data.shape)\n",
    "#\n",
    "\n",
    "model_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Однозадачное обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:654: Checkpoint directory C:\\Users\\mokhail\\python_programming\\MultispectralSegmentation\\saving_dir\\unet_res50pr-120ep-9cl-160-ch_[1-3] exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | MultispectralNN  | 32.5 M | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "32.5 M    Trainable params\n",
      "0         Non-trainable params\n",
      "32.5 M    Total params\n",
      "130.089   Total estimated model params size (MB)\n",
      "226       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############################\n",
      "unet_res50pr-120ep-9cl-160-ch_[1-3]\n",
      "#############################\n",
      "\n",
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           \r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n",
      "c:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\loops\\fit_loop.py:298: The number of training batches (25) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119: 100%|██████████| 25/25 [00:06<00:00,  3.65it/s, v_num=0, val_loss=0.944, val_iou_UNLABELED=0.818, val_iou_buildings_territory=0.748, val_iou_natural_ground=0.0716, val_iou_natural_grow=0.277, val_iou_natural_wetland=0.458, val_iou_natural_wood=0.811, val_iou_quasi_natural_grow=0.598, val_iou_transport=0.177, val_iou_water=0.615, val_iou_mean=0.508, val_recall_UNLABELED=0.928, val_recall_buildings_territory=0.866, val_recall_natural_ground=0.0837, val_recall_natural_grow=0.391, val_recall_natural_wetland=0.645, val_recall_natural_wood=0.926, val_recall_quasi_natural_grow=0.743, val_recall_transport=0.238, val_recall_water=0.699, val_recall_mean=0.613, val_precision_UNLABELED=0.874, val_precision_buildings_territory=0.846, val_precision_natural_ground=0.332, val_precision_natural_grow=0.486, val_precision_natural_wetland=0.613, val_precision_natural_wood=0.867, val_precision_quasi_natural_grow=0.754, val_precision_transport=0.406, val_precision_water=0.836, val_precision_mean=0.668, train_loss=0.125, train_iou_UNLABELED=0.942, train_iou_buildings_territory=0.907, train_iou_natural_ground=0.788, train_iou_natural_grow=0.862, train_iou_natural_wetland=0.901, train_iou_natural_wood=0.941, train_iou_quasi_natural_grow=0.921, train_iou_transport=0.549, train_iou_water=0.767, train_iou_mean=0.842, train_recall_UNLABELED=0.968, train_recall_buildings_territory=0.960, train_recall_natural_ground=0.862, train_recall_natural_grow=0.921, train_recall_natural_wetland=0.946, train_recall_natural_wood=0.972, train_recall_quasi_natural_grow=0.960, train_recall_transport=0.678, train_recall_water=0.853, train_recall_mean=0.902, train_precision_UNLABELED=0.973, train_precision_buildings_territory=0.942, train_precision_natural_ground=0.902, train_precision_natural_grow=0.931, train_precision_natural_wetland=0.949, train_precision_natural_wood=0.967, train_precision_quasi_natural_grow=0.957, train_precision_transport=0.743, train_precision_water=0.884, train_precision_mean=0.916]                                       "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=120` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 119: 100%|██████████| 25/25 [00:06<00:00,  3.60it/s, v_num=0, val_loss=0.944, val_iou_UNLABELED=0.818, val_iou_buildings_territory=0.748, val_iou_natural_ground=0.0716, val_iou_natural_grow=0.277, val_iou_natural_wetland=0.458, val_iou_natural_wood=0.811, val_iou_quasi_natural_grow=0.598, val_iou_transport=0.177, val_iou_water=0.615, val_iou_mean=0.508, val_recall_UNLABELED=0.928, val_recall_buildings_territory=0.866, val_recall_natural_ground=0.0837, val_recall_natural_grow=0.391, val_recall_natural_wetland=0.645, val_recall_natural_wood=0.926, val_recall_quasi_natural_grow=0.743, val_recall_transport=0.238, val_recall_water=0.699, val_recall_mean=0.613, val_precision_UNLABELED=0.874, val_precision_buildings_territory=0.846, val_precision_natural_ground=0.332, val_precision_natural_grow=0.486, val_precision_natural_wetland=0.613, val_precision_natural_wood=0.867, val_precision_quasi_natural_grow=0.754, val_precision_transport=0.406, val_precision_water=0.836, val_precision_mean=0.668, train_loss=0.125, train_iou_UNLABELED=0.942, train_iou_buildings_territory=0.907, train_iou_natural_ground=0.788, train_iou_natural_grow=0.862, train_iou_natural_wetland=0.901, train_iou_natural_wood=0.941, train_iou_quasi_natural_grow=0.921, train_iou_transport=0.549, train_iou_water=0.767, train_iou_mean=0.842, train_recall_UNLABELED=0.968, train_recall_buildings_territory=0.960, train_recall_natural_ground=0.862, train_recall_natural_grow=0.921, train_recall_natural_wetland=0.946, train_recall_natural_wood=0.972, train_recall_quasi_natural_grow=0.960, train_recall_transport=0.678, train_recall_water=0.853, train_recall_mean=0.902, train_precision_UNLABELED=0.973, train_precision_buildings_territory=0.942, train_precision_natural_ground=0.902, train_precision_natural_grow=0.931, train_precision_natural_wetland=0.949, train_precision_natural_wood=0.967, train_precision_quasi_natural_grow=0.957, train_precision_transport=0.743, train_precision_water=0.884, train_precision_mean=0.916]\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 100\n",
    "#model_name = 'FCN-150ep-11cl-150-ch-res-10-20m'\n",
    "channels_str = prepare_channel_indices_str(channel_indices)\n",
    "\n",
    "model_name = f'{model_name}-{epoch_num}ep-{class_num}cl-{img_size[0]}-ch_[{channels_str}]'\n",
    "#model_name = f'{model_name}-{epoch_num}ep-2cl-{img_size[0]}-ch_[{channels_str}]'\n",
    "print('#############################')\n",
    "print(model_name)\n",
    "print('#############################')\n",
    "print()\n",
    "#model_name = 'TEST'\n",
    "\n",
    "metrics_info_list = [\n",
    "    {'name': 'iou', 'numerator': ['tp'], 'denominator': ['tp', 'fp', 'fn']},\n",
    "    {'name': 'recall', 'numerator': ['tp'], 'denominator': ['tp', 'fn']},\n",
    "    {'name': 'precision', 'numerator': ['tp'], 'denominator': ['tp', 'fp']}\n",
    "    ]\n",
    "\n",
    "#segmentation_module = SegmentationModule(model, criterion, metrics_info_list, {'non_appl':0, 'appl':1})\n",
    "segmentation_module = SegmentationModule(model, criterion, metrics_info_list, class_name2idx_dict)\n",
    "path_to_saving_dir = 'saving_dir'\n",
    "logger = CSVLogger(\n",
    "    save_dir = path_to_saving_dir,\n",
    "    name=model_name, \n",
    "    flush_logs_every_n_steps=1,\n",
    "    )\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    mode=\"max\",\n",
    "    filename=model_name+\"-{epoch:02d}-{val_iou_mean:.3}\",\n",
    "    dirpath=os.path.join(path_to_saving_dir, model_name), \n",
    "    save_top_k=1, monitor=\"val_iou_mean\"\n",
    "    )\n",
    "\n",
    "trainer = L.Trainer(logger=logger,\n",
    "        max_epochs=epoch_num, \n",
    "        callbacks=[checkpoint_callback],\n",
    "        accelerator = 'cuda'\n",
    "        )\n",
    "\n",
    "trainer.fit(segmentation_module , train_loader, test_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Многозадачное обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:654: Checkpoint directory C:\\Users\\mokhail\\python_programming\\MultispectralSegmentation\\saving_dir\\fcn_res50pr-P1L(13)_MT-100ep-9cl-150-ch_[0-12] exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type           | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | model     | MultitaskModel | 35.3 M | train\n",
      "1 | criterion | MultitaskLoss  | 0      | train\n",
      "-----------------------------------------------------\n",
      "35.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "35.3 M    Total params\n",
      "141.394   Total estimated model params size (MB)\n",
      "170       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############################\n",
      "fcn_res50pr-P1L(13)_MT-100ep-9cl-150-ch_[0-12]\n",
      "#############################\n",
      "\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 98/98 [00:24<00:00,  3.97it/s, v_num=1, val_loss=1.680, v_cl_iou_UNLABELED=0.772, v_cl_iou_buildings_territory=0.737, v_cl_iou_natural_ground=0.0843, v_cl_iou_natural_grow=0.281, v_cl_iou_natural_wetland=0.229, v_cl_iou_natural_wood=0.785, v_cl_iou_quasi_natural_grow=0.601, v_cl_iou_transport=0.0979, v_cl_iou_water=0.673, v_cl_iou_mean=0.473, v_ap_iou_non_appl=0.740, v_ap_iou_appl=0.842, v_ap_iou_mean=0.791, v_iou_mean=0.632, v_cl_recall_UNLABELED=0.887, v_cl_recall_buildings_territory=0.849, v_cl_recall_natural_ground=0.106, v_cl_recall_natural_grow=0.384, v_cl_recall_natural_wetland=0.267, v_cl_recall_natural_wood=0.947, v_cl_recall_quasi_natural_grow=0.784, v_cl_recall_transport=0.128, v_cl_recall_water=0.724, v_cl_recall_mean=0.564, v_ap_recall_non_appl=0.841, v_ap_recall_appl=0.920, v_ap_recall_mean=0.881, v_recall_mean=0.722, v_cl_precision_UNLABELED=0.856, v_cl_precision_buildings_territory=0.848, v_cl_precision_natural_ground=0.291, v_cl_precision_natural_grow=0.513, v_cl_precision_natural_wetland=0.612, v_cl_precision_natural_wood=0.821, v_cl_precision_quasi_natural_grow=0.721, v_cl_precision_transport=0.294, v_cl_precision_water=0.905, v_cl_precision_mean=0.651, v_ap_precision_non_appl=0.860, v_ap_precision_appl=0.909, v_ap_precision_mean=0.884, v_precision_mean=0.768, train_loss=0.172, tr_cl_iou_UNLABELED=0.891, tr_cl_iou_buildings_territory=0.920, tr_cl_iou_natural_ground=0.854, tr_cl_iou_natural_grow=0.870, tr_cl_iou_natural_wetland=0.910, tr_cl_iou_natural_wood=0.938, tr_cl_iou_quasi_natural_grow=0.925, tr_cl_iou_transport=0.460, tr_cl_iou_water=0.796, tr_cl_iou_mean=0.840, tr_ap_iou_non_appl=0.920, tr_ap_iou_appl=0.954, tr_ap_iou_mean=0.937, tr_iou_mean=0.889, tr_cl_recall_UNLABELED=0.907, tr_cl_recall_buildings_territory=0.966, tr_cl_recall_natural_ground=0.932, tr_cl_recall_natural_grow=0.929, tr_cl_recall_natural_wetland=0.951, tr_cl_recall_natural_wood=0.975, tr_cl_recall_quasi_natural_grow=0.969, tr_cl_recall_transport=0.581, tr_cl_recall_water=0.873, tr_cl_recall_mean=0.898, tr_ap_recall_non_appl=0.950, tr_ap_recall_appl=0.982, tr_ap_recall_mean=0.966, tr_recall_mean=0.932, tr_cl_precision_UNLABELED=0.980, tr_cl_precision_buildings_territory=0.951, tr_cl_precision_natural_ground=0.911, tr_cl_precision_natural_grow=0.931, tr_cl_precision_natural_wetland=0.954, tr_cl_precision_natural_wood=0.961, tr_cl_precision_quasi_natural_grow=0.953, tr_cl_precision_transport=0.687, tr_cl_precision_water=0.900, tr_cl_precision_mean=0.914, tr_ap_precision_non_appl=0.968, tr_ap_precision_appl=0.971, tr_ap_precision_mean=0.970, tr_precision_mean=0.942]                   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 98/98 [00:24<00:00,  3.94it/s, v_num=1, val_loss=1.680, v_cl_iou_UNLABELED=0.772, v_cl_iou_buildings_territory=0.737, v_cl_iou_natural_ground=0.0843, v_cl_iou_natural_grow=0.281, v_cl_iou_natural_wetland=0.229, v_cl_iou_natural_wood=0.785, v_cl_iou_quasi_natural_grow=0.601, v_cl_iou_transport=0.0979, v_cl_iou_water=0.673, v_cl_iou_mean=0.473, v_ap_iou_non_appl=0.740, v_ap_iou_appl=0.842, v_ap_iou_mean=0.791, v_iou_mean=0.632, v_cl_recall_UNLABELED=0.887, v_cl_recall_buildings_territory=0.849, v_cl_recall_natural_ground=0.106, v_cl_recall_natural_grow=0.384, v_cl_recall_natural_wetland=0.267, v_cl_recall_natural_wood=0.947, v_cl_recall_quasi_natural_grow=0.784, v_cl_recall_transport=0.128, v_cl_recall_water=0.724, v_cl_recall_mean=0.564, v_ap_recall_non_appl=0.841, v_ap_recall_appl=0.920, v_ap_recall_mean=0.881, v_recall_mean=0.722, v_cl_precision_UNLABELED=0.856, v_cl_precision_buildings_territory=0.848, v_cl_precision_natural_ground=0.291, v_cl_precision_natural_grow=0.513, v_cl_precision_natural_wetland=0.612, v_cl_precision_natural_wood=0.821, v_cl_precision_quasi_natural_grow=0.721, v_cl_precision_transport=0.294, v_cl_precision_water=0.905, v_cl_precision_mean=0.651, v_ap_precision_non_appl=0.860, v_ap_precision_appl=0.909, v_ap_precision_mean=0.884, v_precision_mean=0.768, train_loss=0.172, tr_cl_iou_UNLABELED=0.891, tr_cl_iou_buildings_territory=0.920, tr_cl_iou_natural_ground=0.854, tr_cl_iou_natural_grow=0.870, tr_cl_iou_natural_wetland=0.910, tr_cl_iou_natural_wood=0.938, tr_cl_iou_quasi_natural_grow=0.925, tr_cl_iou_transport=0.460, tr_cl_iou_water=0.796, tr_cl_iou_mean=0.840, tr_ap_iou_non_appl=0.920, tr_ap_iou_appl=0.954, tr_ap_iou_mean=0.937, tr_iou_mean=0.889, tr_cl_recall_UNLABELED=0.907, tr_cl_recall_buildings_territory=0.966, tr_cl_recall_natural_ground=0.932, tr_cl_recall_natural_grow=0.929, tr_cl_recall_natural_wetland=0.951, tr_cl_recall_natural_wood=0.975, tr_cl_recall_quasi_natural_grow=0.969, tr_cl_recall_transport=0.581, tr_cl_recall_water=0.873, tr_cl_recall_mean=0.898, tr_ap_recall_non_appl=0.950, tr_ap_recall_appl=0.982, tr_ap_recall_mean=0.966, tr_recall_mean=0.932, tr_cl_precision_UNLABELED=0.980, tr_cl_precision_buildings_territory=0.951, tr_cl_precision_natural_ground=0.911, tr_cl_precision_natural_grow=0.931, tr_cl_precision_natural_wetland=0.954, tr_cl_precision_natural_wood=0.961, tr_cl_precision_quasi_natural_grow=0.953, tr_cl_precision_transport=0.687, tr_cl_precision_water=0.900, tr_cl_precision_mean=0.914, tr_ap_precision_non_appl=0.968, tr_ap_precision_appl=0.971, tr_ap_precision_mean=0.970, tr_precision_mean=0.942]\n"
     ]
    }
   ],
   "source": [
    "# multitask\n",
    "epoch_num = 100\n",
    "#model_name = 'FCN-150ep-11cl-150-ch-res-10-20m'\n",
    "channels_str = prepare_channel_indices_str(channel_indices)\n",
    "\n",
    "model_name = f'{model_name}-{epoch_num}ep-{class_num}cl-{img_size[0]}-ch_[{channels_str}]'\n",
    "print('#############################')\n",
    "print(model_name)\n",
    "print('#############################')\n",
    "print()\n",
    "#model_name = 'TEST'\n",
    "\n",
    "metrics_info_list = [\n",
    "    {'name': 'iou', 'numerator': ['tp'], 'denominator': ['tp', 'fp', 'fn']},\n",
    "    {'name': 'recall', 'numerator': ['tp'], 'denominator': ['tp', 'fn']},\n",
    "    {'name': 'precision', 'numerator': ['tp'], 'denominator': ['tp', 'fp']}\n",
    "    ]\n",
    "metrics_info_list = [\n",
    "    {'name': 'iou', 'numerator': ['tp'], 'denominator': ['tp', 'fp', 'fn']}\n",
    "    ]\n",
    "\n",
    "segmentation_module = MultitaskSegmentationModule(model, multitask_criterion, metrics_info_list, class_name2idx_dict, applicable_surfaces_dict)\n",
    "path_to_saving_dir = 'saving_dir'\n",
    "logger = CSVLogger(\n",
    "    save_dir = path_to_saving_dir,\n",
    "    name=model_name, \n",
    "    flush_logs_every_n_steps=1, \n",
    "    )\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    mode=\"max\",\n",
    "    filename=model_name+\"-{epoch:02d}-{v_ap_iou_mean:.3}-{v_cl_iou_mean:.3}-{v_iou_mean:.3}\",\n",
    "    dirpath=os.path.join(path_to_saving_dir, model_name), \n",
    "    save_top_k=1, monitor=\"v_iou_mean\"\n",
    "    )\n",
    "\n",
    "trainer = L.Trainer(logger=logger,\n",
    "        max_epochs=epoch_num, \n",
    "        callbacks=[checkpoint_callback],\n",
    "        accelerator = 'cuda'\n",
    "        )\n",
    "\n",
    "trainer.fit(segmentation_module , train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048, 19, 19])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.backbone(torch.randn(1, 3, 150, 150))['out'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Черновики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "buildings_territory      0.137217\n",
       "natural_ground           0.000395\n",
       "natural_grow             0.114785\n",
       "natural_wetland          0.066782\n",
       "natural_wood             0.465326\n",
       "quasi_natural_ground     0.003570\n",
       "quasi_natural_grow       0.112242\n",
       "quasi_natural_wetland    0.005356\n",
       "quasi_natural_wood       0.000000\n",
       "transport                0.029412\n",
       "water                    0.027175\n",
       "UNLABELED                0.037740\n",
       "dtype: float64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "classes_weights[surface_classes_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor(1.6373, grad_fn=<NllLossBackward0>)\n",
      "None\n",
      "tensor([[-0.6013, -1.8251, -0.3468,  1.6361,  0.4586]], requires_grad=True)\n",
      "tensor([4])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 5, requires_grad=True)\n",
    "target = torch.empty(1, dtype=torch.long).random_(5)\n",
    "print(target.grad)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "print(output)\n",
    "print(target.grad)\n",
    "print(input)\n",
    "print(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5006]])\n",
      "tensor([1])\n",
      "tensor([[0.5006, 0.5006, 0.5006]], grad_fn=<MmBackward0>)\n",
      "tensor([[ 0.1669],\n",
      "        [-0.3337],\n",
      "        [ 0.1669]])\n"
     ]
    }
   ],
   "source": [
    "class_num = 3\n",
    "in_features = 1\n",
    "fc = nn.Linear(in_features, class_num, bias=False)\n",
    "fc.weight = nn.Parameter(torch.ones(class_num, in_features))\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "x = torch.randn(1, in_features)\n",
    "t = torch.empty(1, dtype=torch.long).random_(class_num)\n",
    "out = fc(x)\n",
    "print(x)\n",
    "print(t)\n",
    "print(out)\n",
    "loss = criterion(out, t)\n",
    "loss.backward()\n",
    "print(fc.weight.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "paths_to_delete = glob.glob(r'i:\\AVABOS\\new_projects2\\*\\cut\\*')\n",
    "paths_to_delete = [p for p in paths_to_delete if os.path.isdir(p)]\n",
    "for p in tqdm(paths_to_delete):\n",
    "    shutil.rmtree(p, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, paths_to_images_list, transforms, device):\n",
    "        '''\n",
    "        path_to_dataset - путь до корневой папки с датасетом\n",
    "        instance_names_list - список имен экземпляров БЕЗ РАСШИРЕНИЯ!\n",
    "        transforms - аугментация изображений\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.paths_to_images_list = paths_to_images_list\n",
    "        self.transforms = transforms\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths_to_images_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path_to_image = self.paths_to_images_list[idx]\n",
    "\n",
    "        \n",
    "\n",
    "        #image = torch.as_tensor(np.load(path_to_image))\n",
    "        #image = np.load(path_to_image)\n",
    "        image = torchvision.io.decode_image(path_to_image)\n",
    "        # метки читаем как одноканальное изображение\n",
    "        \n",
    "        \n",
    "        image = tv_tensors.Image(image, device=self.device)\n",
    "        \n",
    "        transforms_dict = {'image':image}\n",
    "        transformed = self.transforms(transforms_dict)\n",
    "        return transformed['image']\n",
    "paths_to_images = glob.glob(r'i:\\embedding_logo_datasets\\icon645\\colored_icons_final\\*\\*.png')\n",
    "transforms = v2.Compose(\n",
    "    [\n",
    "        v2.Resize((256, 256), antialias=True),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "     \n",
    "     ]\n",
    "    )\n",
    "ds = TestDataset(paths_to_images, transforms, torch.device('cuda'))\n",
    "loader = torch.utils.data.DataLoader(ds, batch_size=64, shuffle=True)\n",
    "for d in tqdm(loader):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>train_iou_UNLABELED</th>\n",
       "      <th>train_iou_buildings_territory</th>\n",
       "      <th>train_iou_mean</th>\n",
       "      <th>train_iou_natural_ground</th>\n",
       "      <th>train_iou_natural_grow</th>\n",
       "      <th>train_iou_natural_wetland</th>\n",
       "      <th>train_iou_natural_wood</th>\n",
       "      <th>train_iou_quasi_natural_ground</th>\n",
       "      <th>...</th>\n",
       "      <th>val_iou_natural_grow</th>\n",
       "      <th>val_iou_natural_wetland</th>\n",
       "      <th>val_iou_natural_wood</th>\n",
       "      <th>val_iou_quasi_natural_ground</th>\n",
       "      <th>val_iou_quasi_natural_grow</th>\n",
       "      <th>val_iou_quasi_natural_wetland</th>\n",
       "      <th>val_iou_quasi_natural_wood</th>\n",
       "      <th>val_iou_transport</th>\n",
       "      <th>val_iou_water</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161972</td>\n",
       "      <td>0.357346</td>\n",
       "      <td>0.734698</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.115869</td>\n",
       "      <td>0.056907</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056050</td>\n",
       "      <td>0.227701</td>\n",
       "      <td>1.393110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>314</td>\n",
       "      <td>0.840341</td>\n",
       "      <td>0.625207</td>\n",
       "      <td>0.391350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.283973</td>\n",
       "      <td>0.437569</td>\n",
       "      <td>0.799967</td>\n",
       "      <td>0.006817</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.483770</td>\n",
       "      <td>0.640979</td>\n",
       "      <td>0.825922</td>\n",
       "      <td>0.082659</td>\n",
       "      <td>0.653898</td>\n",
       "      <td>0.566895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024938</td>\n",
       "      <td>0.277133</td>\n",
       "      <td>0.568032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>629</td>\n",
       "      <td>0.852058</td>\n",
       "      <td>0.649716</td>\n",
       "      <td>0.442709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.360270</td>\n",
       "      <td>0.495965</td>\n",
       "      <td>0.833841</td>\n",
       "      <td>0.049213</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>944</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.273776</td>\n",
       "      <td>0.656812</td>\n",
       "      <td>0.766886</td>\n",
       "      <td>0.101420</td>\n",
       "      <td>0.460483</td>\n",
       "      <td>0.745244</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.072769</td>\n",
       "      <td>0.265772</td>\n",
       "      <td>1.006425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>944</td>\n",
       "      <td>0.861290</td>\n",
       "      <td>0.682973</td>\n",
       "      <td>0.515124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454220</td>\n",
       "      <td>0.613231</td>\n",
       "      <td>0.861346</td>\n",
       "      <td>0.125224</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  step  train_iou_UNLABELED  train_iou_buildings_territory  \\\n",
       "0      0   314                  NaN                            NaN   \n",
       "1      0   314             0.840341                       0.625207   \n",
       "2      1   629                  NaN                            NaN   \n",
       "3      1   629             0.852058                       0.649716   \n",
       "4      2   944                  NaN                            NaN   \n",
       "5      2   944             0.861290                       0.682973   \n",
       "\n",
       "   train_iou_mean  train_iou_natural_ground  train_iou_natural_grow  \\\n",
       "0             NaN                       NaN                     NaN   \n",
       "1        0.391350                       0.0                0.283973   \n",
       "2             NaN                       NaN                     NaN   \n",
       "3        0.442709                       0.0                0.360270   \n",
       "4             NaN                       NaN                     NaN   \n",
       "5        0.515124                       0.0                0.454220   \n",
       "\n",
       "   train_iou_natural_wetland  train_iou_natural_wood  \\\n",
       "0                        NaN                     NaN   \n",
       "1                   0.437569                0.799967   \n",
       "2                        NaN                     NaN   \n",
       "3                   0.495965                0.833841   \n",
       "4                        NaN                     NaN   \n",
       "5                   0.613231                0.861346   \n",
       "\n",
       "   train_iou_quasi_natural_ground  ...  val_iou_natural_grow  \\\n",
       "0                             NaN  ...              0.161972   \n",
       "1                        0.006817  ...                   NaN   \n",
       "2                             NaN  ...              0.483770   \n",
       "3                        0.049213  ...                   NaN   \n",
       "4                             NaN  ...              0.273776   \n",
       "5                        0.125224  ...                   NaN   \n",
       "\n",
       "   val_iou_natural_wetland  val_iou_natural_wood  \\\n",
       "0                 0.357346              0.734698   \n",
       "1                      NaN                   NaN   \n",
       "2                 0.640979              0.825922   \n",
       "3                      NaN                   NaN   \n",
       "4                 0.656812              0.766886   \n",
       "5                      NaN                   NaN   \n",
       "\n",
       "   val_iou_quasi_natural_ground  val_iou_quasi_natural_grow  \\\n",
       "0                      0.000085                    0.115869   \n",
       "1                           NaN                         NaN   \n",
       "2                      0.082659                    0.653898   \n",
       "3                           NaN                         NaN   \n",
       "4                      0.101420                    0.460483   \n",
       "5                           NaN                         NaN   \n",
       "\n",
       "   val_iou_quasi_natural_wetland  val_iou_quasi_natural_wood  \\\n",
       "0                       0.056907                         0.0   \n",
       "1                            NaN                         NaN   \n",
       "2                       0.566895                         0.0   \n",
       "3                            NaN                         NaN   \n",
       "4                       0.745244                         0.0   \n",
       "5                            NaN                         NaN   \n",
       "\n",
       "   val_iou_transport  val_iou_water  val_loss  \n",
       "0           0.056050       0.227701  1.393110  \n",
       "1                NaN            NaN       NaN  \n",
       "2           0.024938       0.277133  0.568032  \n",
       "3                NaN            NaN       NaN  \n",
       "4           0.072769       0.265772  1.006425  \n",
       "5                NaN            NaN       NaN  \n",
       "\n",
       "[6 rows x 30 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r'saving_dir\\my_exp_name\\version_1\\metrics.csv'\n",
    "pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "deep_learning"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
