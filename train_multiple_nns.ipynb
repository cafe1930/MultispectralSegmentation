{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "import glob\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "import json\n",
    "\n",
    "from itertools import combinations\n",
    "\n",
    "import torchvision\n",
    "from torchvision.transforms import v2\n",
    "from torchvision import tv_tensors\n",
    "from torchvision import models\n",
    "\n",
    "import segmentation_models_pytorch as smp\n",
    "\n",
    "import lightning as L\n",
    "from lightning.pytorch.callbacks import ModelCheckpoint\n",
    "from lightning.pytorch.loggers import CSVLogger\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Объявление классов и функций"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# datasets\n",
    "class SegmentationDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path_to_dataset_root:str, samples_df:pd.DataFrame, channel_indices:list, transforms:v2._transform.Transform, device:torch.device):\n",
    "        '''\n",
    "        In:\n",
    "            path_to_dataset_root - путь до корневой папки с датасетом\n",
    "            samples_df - pandas.DataFrame с информацией о файлах\n",
    "            channel_indices - список с номерами каналов мультиспектрального изображения\n",
    "            transforms - аугментация изображений\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.path_to_dataset_root = path_to_dataset_root\n",
    "        self.samples_df = samples_df\n",
    "        self.channel_indices = channel_indices\n",
    "        self.transforms = transforms\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples_df.iloc[idx]\n",
    "\n",
    "        file_name = sample['file_name']\n",
    "\n",
    "        path_to_image = os.path.join(self.path_to_dataset_root, 'images', f'{file_name}.npy')\n",
    "        path_to_labels = os.path.join(self.path_to_dataset_root, 'labels', f'{file_name}.npy')\n",
    "\n",
    "        image = torch.as_tensor(np.load(path_to_image), dtype=torch.int16)[self.channel_indices]\n",
    "        #image = np.load(path_to_image)\n",
    "        # метки читаем как одноканальное изображение\n",
    "        label = np.load(path_to_labels)\n",
    "        label = np.where(label >= 0, label, 0)\n",
    "        #label = torch.as_tensor(np.load(path_to_labels), dtype=torch.uint8).long()\n",
    "        label = torch.as_tensor(label, dtype=torch.uint8).long()\n",
    "        \n",
    "        \n",
    "        image = tv_tensors.Image(image, device=self.device)\n",
    "        label = tv_tensors.Mask(label, device=self.device)\n",
    "\n",
    "        transforms_dict = {'image':image, 'mask':label}\n",
    "        transformed = self.transforms(transforms_dict)\n",
    "        return transformed['image'], transformed['mask']#, image\n",
    "    \n",
    "class SegmentationDatasetApplSurf(torch.utils.data.Dataset):\n",
    "    def __init__(self, path_to_dataset_root, samples_df, channel_indices, name2class_idx_dict, applicable_surfaces_dict, transforms, device):\n",
    "        super().__init__()\n",
    "        self.path_to_dataset_root = path_to_dataset_root\n",
    "        self.samples_df = samples_df\n",
    "        self.channel_indices = channel_indices\n",
    "        self.transforms = transforms\n",
    "        self.device = device\n",
    "        self.applicable_surfaces_dict = applicable_surfaces_dict\n",
    "        self.idx2appl = {int(v): 'appl' if v else 'non_appl' for v in applicable_surfaces_dict.values()}\n",
    "        self.applicable_indices = [name2class_idx_dict[cl] for cl, ap in applicable_surfaces_dict.items() if ap]\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.samples_df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples_df.iloc[idx]\n",
    "\n",
    "        file_name = sample['file_name']\n",
    "\n",
    "        path_to_image = os.path.join(self.path_to_dataset_root, 'images', f'{file_name}.npy')\n",
    "        path_to_labels = os.path.join(self.path_to_dataset_root, 'labels', f'{file_name}.npy')\n",
    "\n",
    "        image = torch.as_tensor(np.load(path_to_image), dtype=torch.int16)[self.channel_indices]\n",
    "        #image = np.load(path_to_image)\n",
    "        # метки читаем как одноканальное изображение\n",
    "        label = torch.as_tensor(np.load(path_to_labels), dtype=torch.uint8).long()\n",
    "\n",
    "        applicable_filter = label == self.applicable_indices[0]\n",
    "        for appl_i in self.applicable_indices[1:]:\n",
    "            applicable_filter = applicable_filter | (label == appl_i)\n",
    "        applicable_label = torch.where(applicable_filter==True, 1, 0)\n",
    "        \n",
    "        \n",
    "        image = tv_tensors.Image(image, device=self.device)\n",
    "        applicable_label = tv_tensors.Mask(applicable_label, device=self.device)\n",
    "\n",
    "        transforms_dict = {'image':image, 'mask':applicable_label}\n",
    "        transformed = self.transforms(transforms_dict)\n",
    "        return transformed['image'], transformed['mask']#, image\n",
    "    \n",
    "class FCNSegmentationWrapper(nn.Module):\n",
    "    def __init__(self, model:nn.Module):\n",
    "        '''\n",
    "        Обертка для модели FCN из библиотеки pytorch\n",
    "        In:\n",
    "            model - нейронная сеть\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "    def forward(self, x):\n",
    "        return self.model(x)['out']\n",
    "    \n",
    "class MultispectralNN(nn.Module):\n",
    "    def __init__(self, main_model:nn.Module, preprocessing_block:nn.Module):\n",
    "        '''\n",
    "        Нейронная сеть с блоком предварительной обработки каналов мультиспектра\n",
    "        In:\n",
    "            main_model - нейронная сеть\n",
    "            preprocessing_block - блок предобработки каналов мультиспектра\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.preprocessing_block = preprocessing_block\n",
    "        self.main_model = main_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.preprocessing_block(x)\n",
    "        return self.main_model(x)\n",
    "    \n",
    "class MultispectralNNOutProcess(nn.Module):\n",
    "    def __init__(self, main_model:nn.Module, preprocessing_block:nn.Module, out_block:nn.Module):\n",
    "        '''\n",
    "        Нейронная сеть с блоками пред- и постобработки\n",
    "        In:\n",
    "            main_model - нейронная сеть\n",
    "            preprocessing_block - блок предобработки каналов мультиспектра\n",
    "            out_block - блок постобработки\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.preprocessing_block = preprocessing_block\n",
    "        self.main_model = main_model\n",
    "        self.out_block = out_block\n",
    "\n",
    "    def forward(self, x):\n",
    "        preprocessed = self.preprocessing_block(x)\n",
    "        result = self.main_model(x)\n",
    "        return self.out_block(result, preprocessed)\n",
    "    \n",
    "class MultispectralDataOutput(nn.Module):\n",
    "    def __init__(self, in_channels, class_num):\n",
    "        super().__init__()\n",
    "        self.low_level_conv = nn.Conv1d(in_channels, class_num, kernel_size=1)\n",
    "        self.out_conv = nn.Conv1d(class_num*2, class_num, kernel_size=1)\n",
    "    def forward(self, cnn_out, low_level_out):\n",
    "        low_level_results = self.low_level_conv(low_level_out)\n",
    "        nn.ChannelShuffle(groups=2)\n",
    "\n",
    "class MultispectralFuseOut(nn.Module):\n",
    "    def __init__(self, main_model:nn.Module, multispectral_preprocessing_block:nn.Module, fusion_type:str, preprocessing_out_dim:int, class_num:int):\n",
    "        '''\n",
    "        In:\n",
    "            main_model - нейронная сеть\n",
    "            multispectral_preprocessing_block - блок предобработки каналов мультиспектра\n",
    "            fusion_type - строка с типом слияния  ('shuffle', 'concat', 'add')\n",
    "            preprocessing_out_dim - целое число, выражающее разерность блока предобрботки каналов мультиспектра\n",
    "            class_num: кол-во классов\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.multispectral_preprocessing_block = multispectral_preprocessing_block\n",
    "        self.main_model = main_model\n",
    "\n",
    "        self.multispectral_preout_block = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=preprocessing_out_dim,out_channels=class_num, kernel_size=1),\n",
    "            nn.BatchNorm2d(class_num),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fusion_type = fusion_type\n",
    "        if fusion_type == 'shuffle':\n",
    "            self.fusion_block = nn.Sequential(\n",
    "                #nn.Dropout2d(0.3),\n",
    "                nn.ChannelShuffle(groups=2),\n",
    "                nn.Conv2d(in_channels=class_num*2, out_channels=class_num, kernel_size=1, groups=class_num)\n",
    "            )\n",
    "        elif fusion_type == 'concat':\n",
    "            self.fusion_block = nn.Conv2d(in_channels=class_num*2, out_channels=class_num, kernel_size=1)\n",
    "        elif fusion_type == 'add':\n",
    "            self.fusion_block = nn.Conv2d(in_channels=class_num, out_channels=class_num, kernel_size=1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        multispectral_preprocessed_out = self.multispectral_preprocessing_block(x)\n",
    "        multispectral_out = self.multispectral_preout_block(multispectral_preprocessed_out)\n",
    "        #print(multispectral_preprocessed_out.shape)\n",
    "        #print(multispectral_out.shape)\n",
    "        main_out = self.main_model(multispectral_preprocessed_out)\n",
    "        if self.fusion_type == 'add':\n",
    "            concat_out = multispectral_out + main_out\n",
    "        else:\n",
    "            concat_out = torch.cat([multispectral_out, main_out], dim=1)\n",
    "        \n",
    "\n",
    "        return self.fusion_block(concat_out)\n",
    "\n",
    "class MultitaskLoss(nn.Module):\n",
    "    def __init__(self, loss1, loss2):\n",
    "        super().__init__()\n",
    "        self.loss1 = loss1\n",
    "        self.loss2 = loss2\n",
    "        \n",
    "    def forward(self, pred1, pred2, target1, target2):\n",
    "        loss_val1 = self.loss1(pred1, target1)\n",
    "        loss_val2 = self.loss1(pred2, target2)\n",
    "        return loss_val1 + loss_val2\n",
    "        #applicable_target = \n",
    "        #coarse_target = \n",
    "\n",
    "class MultitaskModel(nn.Module):\n",
    "    def __init__(self, model, nn_output_size, appl_class_num, surf_class_num):\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.applicable_head = nn.Sequential(\n",
    "            #nn.Conv2d(nn_output_size, out_channels=nn_output_size//4, kernel_size=3, padding=1),\n",
    "            #nn.BatchNorm2d(nn_output_size//4),\n",
    "            #nn.Dropout2d(p=0.3),\n",
    "            #nn.Conv2d(nn_output_size//4, appl_class_num, kernel_size=1)\n",
    "            nn.Conv2d(nn_output_size, appl_class_num, kernel_size=1)\n",
    "        )\n",
    "        self.surf_head = nn.Sequential(\n",
    "            #nn.Conv2d(nn_output_size, out_channels=nn_output_size//4, kernel_size=3, padding=1),\n",
    "            #nn.BatchNorm2d(nn_output_size//4),\n",
    "            #nn.Dropout2d(p=0.3),\n",
    "            #nn.Conv2d(nn_output_size//4, surf_class_num, kernel_size=1)\n",
    "            nn.Conv2d(nn_output_size, surf_class_num, kernel_size=1)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        h = self.model(x)\n",
    "        appl_out = self.applicable_head(h)\n",
    "        surf_out = self.surf_head(h)\n",
    "        return appl_out, surf_out\n",
    "\n",
    "class SpectralDiffIndexModule(nn.Module):\n",
    "    def __init__(self, channel_indices_list, channels_in_index, out_channels):\n",
    "        super().__init__()\n",
    "        self.channel_indices_list = channel_indices_list\n",
    "        combinations_list = list(combinations(channel_indices_list, channels_in_index))\n",
    "        \n",
    "        self.combinations_list = np.array(combinations_list).reshape(-1).tolist()\n",
    "        in_channels = len(self.combinations_list)\n",
    "        #self.make_channels_combinations = MakeChannelsCombinations(self.combinations_list)\n",
    "        self.numerator = nn.Conv2d(in_channels=in_channels, out_channels=in_channels//channels_in_index, kernel_size=1, groups=in_channels//channels_in_index, bias=False)\n",
    "        self.denominator = nn.Conv2d(in_channels=in_channels, out_channels=in_channels//channels_in_index, kernel_size=1, groups=in_channels//channels_in_index, bias=False)\n",
    "        self.out_block = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=in_channels//channels_in_index, out_channels=out_channels, kernel_size=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            #nn.ReLU()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        channels_combinations = x[:,self.combinations_list]\n",
    "        numerator_results = self.numerator(channels_combinations)\n",
    "        denominator_results = self.denominator(channels_combinations)\n",
    "        indices = numerator_results / (denominator_results+1e-7)\n",
    "        output = self.out_block(indices)\n",
    "        return output\n",
    "\n",
    "def decode_confusion_matrix_2x2(confusion_matrix):\n",
    "    '''\n",
    "    In: Матрица ошибок со следующей структурой:\n",
    "    [[tn, fp],\n",
    "     [fn, tp]]\n",
    "    Out: словарь {'tp':tp_val, 'tn': tn_val, 'fp': fp_val, 'fn': fn_val}\n",
    "    '''\n",
    "    tp = confusion_matrix[1, 1]\n",
    "    tn = confusion_matrix[0, 0]\n",
    "    fp = confusion_matrix[0, 1]\n",
    "    fn = confusion_matrix[1, 0]\n",
    "    return {'tp':tp, 'tn': tn, 'fp': fp, 'fn': fn}\n",
    "\n",
    "def compute_accuracy_from_confusion(multiclass_confusion_matrix):\n",
    "    '''\n",
    "    In:\n",
    "        multiclass_confusion_matrix - np.ndarray размера (class_num, 2, 2)\n",
    "    Out:\n",
    "        accuracy - float результат вычисления точности\n",
    "    '''\n",
    "\n",
    "    confusion_sum = multiclass_confusion_matrix.sum(axis=0)\n",
    "    tp, tn, fp, fn = decode_confusion_matrix_2x2(confusion_sum)\n",
    "    accuracy = 0\n",
    "    if tp+tn+fp+fn != 0:\n",
    "        accuracy = (tp+tn)/(tp+tn+fp+fn)\n",
    "    return accuracy\n",
    "\n",
    "def compute_metric_from_confusion(multiclass_confusion_matrix:np.ndarray, metric_params_dict:dict, idx2class_name_dict=None):\n",
    "    '''\n",
    "    Вычисление значений метрик, вычисляющитхся для одного класса (precsion, recall, f1-score, IoU, Dice)\n",
    "    Вычисляет как метрики для всех классов, так и невзвешенное среднее значение метрики для всех классов\n",
    "    In:\n",
    "        multiclass_confusion_matrix - многоклассовая матрица ошибок\n",
    "        metric_params_dict - словарь c информацией о том, как вычислять метрики. Cловарь метрик имеет след. структуру:\n",
    "            {'name': 'имя_метрики', 'numerator': [список со строковыми обозначениями 'tp', 'fp', 'tn', 'fn'], 'denominator': [список со строковыми обозначениями 'tp', 'fp', 'tn', 'fn']}\n",
    "            Списки 'numerator' и 'denominator' зависят от формулы вычисления метрики. Например, precision=tp/(tp+fp)\n",
    "            Тогда numerator: ['tp'], denominator: ['tp', 'fp']\n",
    "            Метрики вычисляются по общей формуле metric_val = sum(numerator_vals_list)/sum(denominators_vals_list)\n",
    "    Out:\n",
    "        metric_dict - словарь со структурой {metric_name: metric_val}\n",
    "    '''\n",
    "    #print(f'metric_params_dict={metric_params_dict}')\n",
    "    mean_metric = 0\n",
    "    # {class_name: iou_val}\n",
    "    metric_dict = {}\n",
    "    actual_classes_num = 0\n",
    "    metric_name = metric_params_dict['name']\n",
    "    # отдельная обработка метрики accuracy, которая считается для всех классов сразу\n",
    "    if metric_name == 'accuracy':\n",
    "        # получаем матрицу ошибок для всех классов сразу\n",
    "        confusion_sum = multiclass_confusion_matrix.sum(axis=0)\n",
    "        # выделяем словарь, содержащий общее количество распознаваний {'tp': tp_num, 'fp': fp_num, 'tn': tn_num, 'fn': fn_num}\n",
    "        confusion_vals_dict = decode_confusion_matrix_2x2(confusion_sum)\n",
    "        # выделение списка значений 'tp', 'fp', 'tn', 'fn', входящих в числитель формулы метрики\n",
    "        numerator = [confusion_vals_dict[v] for v in metric_params_dict['numerator']]\n",
    "        # выделение списка значений ('tp', 'fp', 'tn', 'fn'), входящих в знаменатель формулы метрики\n",
    "        denominator = [confusion_vals_dict[v] for v in metric_params_dict['denominator']]\n",
    "        accuracy = 0\n",
    "        if np.sum(denominator) != 0:\n",
    "            class_metric = np.sum(numerator)/np.sum(denominator)\n",
    "        metric_dict[metric_name] = class_metric\n",
    "        return metric_dict\n",
    "\n",
    "    for idx, class_confusion in enumerate(multiclass_confusion_matrix):\n",
    "        # выделяем словарь, содержащий количество распознаваний {'tp': tp_num, 'fp': fp_num, 'tn': tn_num, 'fn': fn_num} для определенного класса\n",
    "        confusion_vals_dict = decode_confusion_matrix_2x2(class_confusion)\n",
    "        \n",
    "        if class_confusion.sum() != confusion_vals_dict['tn']:\n",
    "            actual_classes_num += 1\n",
    "\n",
    "        #print(f'actual_classes_num={actual_classes_num}')\n",
    "        class_metric = 0\n",
    "        # выделение списка значений 'tp', 'fp', 'tn', 'fn', входящих в числитель формулы метрики\n",
    "        numerator = [confusion_vals_dict[v] for v in metric_params_dict['numerator']]\n",
    "        # выделение списка значений 'tp', 'fp', 'tn', 'fn', входящих в знаменатель формулы метрики\n",
    "        denominator = [confusion_vals_dict[v] for v in metric_params_dict['denominator']]\n",
    "        \n",
    "        if np.sum(denominator) != 0:\n",
    "            class_metric = np.sum(numerator)/np.sum(denominator)\n",
    "        # вычисление суммарного значения метрики для дальнейшего вычисления среднего згначения по всем классам\n",
    "        mean_metric += class_metric\n",
    "        class_name = f'{metric_name}_{idx}'\n",
    "        if idx2class_name_dict is not None:\n",
    "            class_name = f'{metric_name}_{idx2class_name_dict[idx]}'\n",
    "        metric_dict[class_name] = class_metric\n",
    "    if actual_classes_num == 0:\n",
    "        metric_dict[f'{metric_name}_mean'] = 0\n",
    "    else:\n",
    "        # вычисление среднего значения метрики\n",
    "        metric_dict[f'{metric_name}_mean'] = mean_metric/actual_classes_num\n",
    "    return metric_dict\n",
    "\n",
    "def compute_pred_mask(pred):\n",
    "    '''\n",
    "    Определение маски классов на основе сгенерированной softmax маски\n",
    "    '''\n",
    "    pred = pred.detach()\n",
    "    _, pred_mask = pred.max(dim=1)\n",
    "    return pred_mask.cpu().numpy()\n",
    "\n",
    "class SegmentationModule(L.LightningModule):\n",
    "    def __init__(self, model:nn.Module, criterion:nn.Module, metrics_info_list:list, name2class_idx_dict:dict) -> None:\n",
    "        '''\n",
    "        Модуль Lightning для обучения сегментационной сети\n",
    "        In:\n",
    "            model - нейронная сеть\n",
    "            criterion - функция потерь\n",
    "            metrics_info_list - список со словарями метрик. Каждый словарь метрик имеет след. структуру:\n",
    "                {'name': 'имя_метрики', 'numerator': [список со строковыми обозначениями 'tp', 'fp', 'tn', 'fn'], 'denominator': [список со строковыми обозначениями 'tp', 'fp', 'tn', 'fn']}\n",
    "                Списки 'numerator' и 'denominator' зависят от формулы вычисления метрики. Например, precision=tp/(tp+fp)\n",
    "                Тогда numerator: ['tp'], denominator: ['tp', 'fp']\n",
    "            name2class_idx_dict - словарь с отображением {class_name(str): class_idx(int)}\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.name2class_idx_dict = name2class_idx_dict\n",
    "        # словарь, выполняющий обратное отображение class_idx в class_name\n",
    "        self.class_idx2name_dict = {v:k for k, v in name2class_idx_dict.items()}\n",
    "        # train_confusion_matrix - матрица ошибок для вычисления метрик на этапе обучения. Размер (class_num, 2, 2) должен соответствовать размеру, генериуемому функцией sklearn.metrics.multilabel_confusion_matrix\n",
    "        self.train_confusion_matrix = np.zeros((len(self.name2class_idx_dict), 2, 2), dtype=np.int64)\n",
    "        # val_confusion_matrix - матрица ошибок для вычисления метрик на этапе тестирования. Размер (class_num, 2, 2) должен соответствовать размеру, генериуемому функцией sklearn.metrics.multilabel_confusion_matrix\n",
    "        self.val_confusion_matrix = np.zeros((len(self.name2class_idx_dict), 2, 2), dtype=np.int64)\n",
    "        self.metrics_info_list = metrics_info_list\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, true_labels = batch\n",
    "        pred = self.model(data)\n",
    "        loss = self.criterion(pred, true_labels)\n",
    "        # вычисление сгенерированной маски\n",
    "        pred_labels = compute_pred_mask(pred)\n",
    "        true_labels = true_labels.detach().cpu().numpy()\n",
    "        # вычисление многоклассовой матрицы ошибок для одного пакета (batch) обработанных данных\n",
    "        batch_confusion_matrix = metrics.multilabel_confusion_matrix(true_labels.reshape(-1), pred_labels.reshape(-1),labels=list(self.class_idx2name_dict.keys()))\n",
    "        #print(f'train_batch_conf_type={batch_confusion_matrix.dtype}')\n",
    "        #print(batch_confusion_matrix)\n",
    "        # приплюсовываем результаты на батче к результатам на эпохе\n",
    "        self.train_confusion_matrix += batch_confusion_matrix.astype(np.int64)\n",
    "        # т.к. мы вычисляем общую ошибку на всей эпохе, то записываем в лог только значение функции потерь\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data, true_labels = batch\n",
    "        pred = self.model(data)\n",
    "        loss = self.criterion(pred, true_labels)\n",
    "        pred_labels = compute_pred_mask(pred)\n",
    "        true_labels = true_labels.detach().cpu().numpy()\n",
    "        batch_confusion_matrix = metrics.multilabel_confusion_matrix(true_labels.reshape(-1), pred_labels.reshape(-1),labels=list(self.class_idx2name_dict.keys()))\n",
    "        #print(f'pred_labels type={pred_labels.dtype}')\n",
    "        #print(f'true_labels type={true_labels.dtype}')\n",
    "        #print(f'val_batch_conf_type={batch_confusion_matrix.dtype}')\n",
    "        #print(batch_confusion_matrix)\n",
    "        self.val_confusion_matrix += batch_confusion_matrix.astype(np.int64)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        '''\n",
    "        Декодирование результатов тренировочной эпохи и запись их в лог\n",
    "        '''\n",
    "        # итерируем по рассматриваемым метрикам (структуру self.metrics_info_list см. в конструкторе)\n",
    "        for metric_info_dict in self.metrics_info_list:\n",
    "            metric_dict = compute_metric_from_confusion(self.train_confusion_matrix, metric_info_dict, self.class_idx2name_dict)\n",
    "            for name, value in metric_dict.items():\n",
    "                name = f'train_{name}'\n",
    "                self.log(name, value, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.train_confusion_matrix = np.zeros((len(self.name2class_idx_dict), 2, 2), dtype=np.int64)\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        '''\n",
    "        Декодирование результатов тестовой эпохи и запись их в лог\n",
    "        (работает точно также, как и )\n",
    "        '''\n",
    "        for metric_info_dict in self.metrics_info_list:\n",
    "            metric_dict = compute_metric_from_confusion(self.val_confusion_matrix, metric_info_dict, self.class_idx2name_dict)\n",
    "            for name, value in metric_dict.items():\n",
    "                name = f'val_{name}'\n",
    "                self.log(name, value, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.val_confusion_matrix = np.zeros((len(self.name2class_idx_dict), 2, 2), dtype=np.int64)\n",
    "\n",
    "class MultitaskSegmentationModule(L.LightningModule):\n",
    "    def __init__(self, model, criterion, metrics_info_list, name2class_idx_dict, applicable_surfaces_dict) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.criterion = criterion\n",
    "        self.name2class_idx_dict = name2class_idx_dict\n",
    "        self.class_idx2name_dict = {v:k for k, v in name2class_idx_dict.items()}\n",
    "        self.idx2appl = {int(v): 'appl' if v else 'non_appl' for v in applicable_surfaces_dict.values()}\n",
    "        self.applicable_indices = [name2class_idx_dict[cl] for cl, ap in applicable_surfaces_dict.items() if ap]\n",
    "        self.classes_train_confusion_matrix = np.zeros((len(self.name2class_idx_dict), 2, 2), dtype=np.int64)\n",
    "        self.classes_val_confusion_matrix = np.zeros((len(self.name2class_idx_dict), 2, 2), dtype=np.int64)\n",
    "        self.applicable_train_confusion_matrix = np.zeros((len(self.idx2appl), 2, 2), dtype=np.int64)\n",
    "        self.applicable_val_confusion_matrix = np.zeros((len(self.idx2appl), 2, 2), dtype=np.int64)\n",
    "        self.metrics_info_list = metrics_info_list\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        return torch.optim.Adam(self.parameters())\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        data, classes_true_labels = batch\n",
    "\n",
    "        applicable_filter = classes_true_labels == self.applicable_indices[0]\n",
    "        for appl_i in self.applicable_indices[1:]:\n",
    "            applicable_filter = applicable_filter | (classes_true_labels == appl_i)\n",
    "        applicable_true_labels = torch.where(applicable_filter==True, 1, 0)\n",
    "\n",
    "        applicable_pred, classes_pred = self.model(data)\n",
    "\n",
    "        loss = self.criterion(applicable_pred, classes_pred, applicable_true_labels, classes_true_labels)\n",
    "        \n",
    "        applicable_pred_labels = compute_pred_mask(applicable_pred)\n",
    "        classes_pred_labels = compute_pred_mask(classes_pred)\n",
    "        applicable_true_labels = applicable_true_labels.detach().cpu().numpy()\n",
    "        classes_true_labels = classes_true_labels.detach().cpu().numpy()\n",
    "        classes_batch_confusion_matrix = metrics.multilabel_confusion_matrix(classes_true_labels.reshape(-1), classes_pred_labels.reshape(-1),labels=list(self.class_idx2name_dict.keys()))\n",
    "        applicable_batch_confusion_matrix = metrics.multilabel_confusion_matrix(applicable_true_labels.reshape(-1), applicable_pred_labels.reshape(-1),labels=[0, 1])\n",
    "        #print(f'train_batch_conf_type={batch_confusion_matrix.dtype}')\n",
    "        #print(batch_confusion_matrix)\n",
    "        self.classes_train_confusion_matrix += classes_batch_confusion_matrix.astype(np.int64)\n",
    "        self.applicable_train_confusion_matrix += applicable_batch_confusion_matrix.astype(np.int64)\n",
    "        self.log('train_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        data, classes_true_labels = batch\n",
    "\n",
    "        applicable_filter = classes_true_labels == self.applicable_indices[0]\n",
    "        for appl_i in self.applicable_indices[1:]:\n",
    "            applicable_filter = applicable_filter | (classes_true_labels == appl_i)\n",
    "        applicable_true_labels = torch.where(applicable_filter==True, 1, 0)\n",
    "\n",
    "        applicable_pred, classes_pred = self.model(data)\n",
    "\n",
    "        loss = self.criterion(applicable_pred, classes_pred, applicable_true_labels, classes_true_labels)\n",
    "        \n",
    "        applicable_pred_labels = compute_pred_mask(applicable_pred)\n",
    "        classes_pred_labels = compute_pred_mask(classes_pred)\n",
    "        applicable_true_labels = applicable_true_labels.detach().cpu().numpy()\n",
    "        classes_true_labels = classes_true_labels.detach().cpu().numpy()\n",
    "        classes_batch_confusion_matrix = metrics.multilabel_confusion_matrix(classes_true_labels.reshape(-1), classes_pred_labels.reshape(-1),labels=list(self.class_idx2name_dict.keys()))\n",
    "        applicable_batch_confusion_matrix = metrics.multilabel_confusion_matrix(applicable_true_labels.reshape(-1), applicable_pred_labels.reshape(-1),labels=[0, 1])\n",
    "        #print(f'train_batch_conf_type={batch_confusion_matrix.dtype}')\n",
    "        #print(batch_confusion_matrix)\n",
    "        self.classes_val_confusion_matrix += classes_batch_confusion_matrix.astype(np.int64)\n",
    "        self.applicable_val_confusion_matrix += applicable_batch_confusion_matrix.astype(np.int64)\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        return loss\n",
    "    \n",
    "    def on_train_epoch_end(self):\n",
    "        for metric_info_dict in self.metrics_info_list:\n",
    "            metric_name = metric_info_dict['name']\n",
    "            class_metric_dict = compute_metric_from_confusion(self.classes_train_confusion_matrix, metric_info_dict, self.class_idx2name_dict)\n",
    "            mean = 0\n",
    "            for name, value in class_metric_dict.items():\n",
    "                if 'mean' in name:\n",
    "                    class_mean = value\n",
    "                name = f'tr_cl_{name}'\n",
    "                self.log(name, value, on_step=False, on_epoch=True, prog_bar=True)\n",
    "            appl_metric_dict = compute_metric_from_confusion(self.applicable_train_confusion_matrix, metric_info_dict, self.idx2appl)\n",
    "            for name, value in appl_metric_dict.items():\n",
    "                if 'mean' in name:\n",
    "                    appl_mean = value\n",
    "                name = f'tr_ap_{name}'\n",
    "                self.log(name, value, on_step=False, on_epoch=True, prog_bar=True)\n",
    "            mean = (class_mean+appl_mean)/2\n",
    "            self.log(f'tr_{metric_name}_mean', mean, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.classes_train_confusion_matrix = np.zeros((len(self.name2class_idx_dict), 2, 2), dtype=np.int64)\n",
    "        self.applicable_train_confusion_matrix = np.zeros((len(self.idx2appl), 2, 2), dtype=np.int64)\n",
    "        \n",
    "    def on_validation_epoch_end(self):\n",
    "        for metric_info_dict in self.metrics_info_list:\n",
    "            metric_name = metric_info_dict['name']\n",
    "            class_metric_dict = compute_metric_from_confusion(self.classes_val_confusion_matrix, metric_info_dict, self.class_idx2name_dict)\n",
    "            mean = 0\n",
    "            for name, value in class_metric_dict.items():\n",
    "                if 'mean' in name:\n",
    "                    class_mean = value\n",
    "                name = f'v_cl_{name}'\n",
    "                self.log(name, value, on_step=False, on_epoch=True, prog_bar=True)\n",
    "            appl_metric_dict = compute_metric_from_confusion(self.applicable_val_confusion_matrix, metric_info_dict, self.idx2appl)\n",
    "            for name, value in appl_metric_dict.items():\n",
    "                if 'mean' in name:\n",
    "                    appl_mean = value\n",
    "                name = f'v_ap_{name}'\n",
    "                self.log(name, value, on_step=False, on_epoch=True, prog_bar=True)\n",
    "            mean = (class_mean+appl_mean)/2\n",
    "            self.log(f'v_{metric_name}_mean', mean, on_step=False, on_epoch=True, prog_bar=True)\n",
    "        self.classes_val_confusion_matrix = np.zeros((len(self.name2class_idx_dict), 2, 2), dtype=np.int64)\n",
    "        self.applicable_val_confusion_matrix = np.zeros((len(self.idx2appl), 2, 2), dtype=np.int64)\n",
    "\n",
    "def prepare_channel_indices_str(channel_indices_list):\n",
    "    '''\n",
    "    Функция формирует строку, содержащую обрабатываемые индексы мультиспектра. Нужна для формирования красивого имени сохраняемого файла\n",
    "    '''\n",
    "    last = None\n",
    "    first = None\n",
    "    intervals = []\n",
    "    for i, ch_idx in enumerate(sorted(channel_indices_list)):\n",
    "        #print(f'ch_idx={ch_idx}, last==ch_idx-1:{last==ch_idx-1}')\n",
    "        #print(f'first={first};last={last}')\n",
    "        if last is not None:\n",
    "            if last != ch_idx-1:\n",
    "                #last = prev_idx\n",
    "                if first == last:\n",
    "                    intervals.append([last])\n",
    "                else:\n",
    "                    intervals.append([first, last])\n",
    "\n",
    "                first = ch_idx\n",
    "                last = ch_idx\n",
    "            else:\n",
    "                last = ch_idx\n",
    "            if i == len(channel_indices_list)-1:\n",
    "                if first == last:\n",
    "                    intervals.append([last])\n",
    "                else:\n",
    "                    intervals.append([first, last])\n",
    "        else:\n",
    "            first = ch_idx\n",
    "            last = ch_idx\n",
    "\n",
    "        #channels_str += f'{ch},'\n",
    "    chanels_str = ''\n",
    "    for i, interval in enumerate(intervals):\n",
    "        if len(interval) == 1:\n",
    "            ptr = f'{interval[0]}'\n",
    "        else:\n",
    "            ptr = f'{interval[0]}-{interval[-1]}'\n",
    "        if i != len(intervals) - 1:\n",
    "            ptr = f'{ptr},'\n",
    "\n",
    "        chanels_str += ptr\n",
    "    return chanels_str"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Функции-фабрики для генерирования различных конфигураций нейронных сетей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_torch_model(\n",
    "        model_dict:dict,\n",
    "        image_channels:int,\n",
    "        nn_in_cannels_num:int,\n",
    "        class_num:int,\n",
    "        channel_indices_list:list,\n",
    "        is_multitask:bool,\n",
    "        preprocess_params:dict,\n",
    "        fuze_params:dict):\n",
    "    '''\n",
    "    Функция генерирует конфигурацию сегментационной нейронной сети из библиотеки torchvision.models.segmentation\n",
    "    In:\n",
    "        model_dict - словарь с параметрами модели. Структура словаря {'model_name': строковое_имя_модели, 'creation_function': функция_создания_модели (класс модели), 'weights': веса модели}\n",
    "        image_channels:int - количество каналов изображения\n",
    "        nn_in_cannels_num:int - количество входных каналов нейронной сети\n",
    "        class_num:int - количество классов\n",
    "        channel_indices_list:list - список индексов каналов обрабатываемого мультиспектрального изображения\n",
    "        is_multitask:bool - флаг, обозначающий мнгогозадачное обучение\n",
    "        preprocess_params:dict - параметры предварительной обработки каналов мультиспектра. Словарь структуры {'type':'1L'}, на месте '1L' мб\n",
    "            'no' - отсутствие доп. блока предобработки,\n",
    "            1L - однослойная модель предобработки\n",
    "            2L - двухслойная модель предобработки\n",
    "            SpInd - слой, вычисляющий множество спектральных яркостных разностных индексов\n",
    "        fuze_params:dict - параметры связывания выхода нейронной сети и блока преобработки. Словарь структуры {'type':'no'}, на месте 'no' мб\n",
    "            shuffle - модуль channel_shuffle (см. https://arxiv.org/abs/1707.01083)\n",
    "            concat - конкатенация\n",
    "            add - сложение\n",
    "    Out:\n",
    "        dict с моделью следующей структуры {'name': model_name, 'model': model, 'train_transforms':train_transforms, 'test_transforms':test_transforms}\n",
    "    '''\n",
    "    model_creation_unction = model_dict['creation_function']\n",
    "    weights = model_dict['weights']\n",
    "    model_name = model_dict['model_name']\n",
    "    model = model_creation_unction(weights=weights)\n",
    "    \n",
    "    # заменяем входной слой, если количество входных каналов мультиспектра не равно трем\n",
    "    conv1 = model.backbone.conv1\n",
    "\n",
    "    weights = conv1.weight\n",
    "    # если количество входных каналов не равно трем, то мы \n",
    "    # 1. Усредняем величины весов трех входных каналов предобученной нейронной сети\n",
    "    # 2. Копируем полученные усредненные веса для всех входных каналов нового слоя \n",
    "    new_weight = torch.cat([weights.mean(dim=1).unsqueeze(1)]*nn_in_cannels_num, dim=1)\n",
    "    # создаем новый слой\n",
    "    new_conv1 = nn.Conv2d(\n",
    "        in_channels=nn_in_cannels_num,\n",
    "        out_channels=conv1.out_channels,\n",
    "        kernel_size=conv1.kernel_size,\n",
    "        stride=conv1.stride,\n",
    "        padding=conv1.padding,\n",
    "        dilation=conv1.dilation,\n",
    "        groups=conv1.groups,\n",
    "        bias=conv1.bias is not None\n",
    "    )\n",
    "\n",
    "    new_conv1.weight = nn.Parameter(new_weight)\n",
    "    if conv1.bias is not None:\n",
    "        # заменяем пороги, если они были в изначальном входном слое\n",
    "        new_conv1.bias = model.backbone.conv1.bias\n",
    "    # заменяем веса, если количнество входных каналов не равно трем \n",
    "    if nn_in_cannels_num != 3:\n",
    "        model.backbone.conv1 = new_conv1\n",
    "\n",
    "    # заменяем выходные слои, если количество каналов изображения не равно трем\n",
    "    # \n",
    "    if is_multitask:\n",
    "        model.classifier = nn.Sequential(*list(model.classifier.children())[:-1])\n",
    "    else:\n",
    "        classifier_conv = model.classifier[-1]\n",
    "        new_classifier_conv = nn.Conv2d(\n",
    "            in_channels=classifier_conv.in_channels,\n",
    "            out_channels=class_num,\n",
    "            kernel_size=classifier_conv.kernel_size,\n",
    "            stride=classifier_conv.kernel_size,\n",
    "            padding=classifier_conv.padding,\n",
    "            dilation=classifier_conv.dilation,\n",
    "            groups=classifier_conv.groups,\n",
    "            bias=classifier_conv.bias is not None,\n",
    "            )\n",
    "        model.classifier[-1] = new_classifier_conv\n",
    "    \n",
    "    # выбор модели доп классификатора для модели из torchvision\n",
    "    if 'fcn' in model_name.lower():\n",
    "        model.classifier = models.segmentation.fcn.FCNHead(in_channels=2048, channels=class_num)\n",
    "    elif 'dlv3' in model_name.lower():\n",
    "        model.classifier = models.segmentation.deeplabv3.DeepLabHead(in_channels=2048, num_classes=class_num)\n",
    "\n",
    "    # нам нужна обертка модели, чтобы для того, чтобы у нее был один выход (в моделях torchvision.models.segmentation по умолчанию два выхода - сегментационный и доп. классификатор)\n",
    "    model = FCNSegmentationWrapper(model)\n",
    "    if is_multitask:\n",
    "        model = MultitaskModel(model, nn_output_size=512, appl_class_num=2, surf_class_num=class_num)\n",
    "\n",
    "    # добавление дополнительных блоков предобработки каналов мультиспектра, если они есть \n",
    "    if preprocess_params['type'] == 'no':\n",
    "        preprocess_layer = nn.Identity()\n",
    "    elif preprocess_params['type'] == '1L':\n",
    "        # создаем один сверточный слой с ядром размером 1x1\n",
    "        # активация отсутствует, т.к. наличие активации показало худшие результаты\n",
    "        preprocess_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=image_channels, out_channels=nn_in_cannels_num, kernel_size=1),\n",
    "            nn.BatchNorm2d(nn_in_cannels_num)\n",
    "        )\n",
    "    elif preprocess_params['type'] == '2L':\n",
    "        # создаем двухслойную сверточную нейронную сеть с ядрами размером 1x1\n",
    "        preprocess_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=image_channels, out_channels=32, kernel_size=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=nn_in_cannels_num, kernel_size=1),\n",
    "            nn.BatchNorm2d(nn_in_cannels_num),\n",
    "            nn.ReLU())\n",
    "    elif preprocess_params['type'] == 'SpInd':\n",
    "        # слой, вычисляющий множество спектральных яркостных разностных индексов\n",
    "        preprocess_layer = SpectralDiffIndexModule(channel_indices_list=channel_indices_list, channels_in_index=2, out_channels=8)\n",
    "    \n",
    "    if fuze_params['type'] == 'no':\n",
    "        model = MultispectralNN(model, preprocess_layer)\n",
    "    else:\n",
    "        # добавляем слой, выполняющий слияние выхода нейронной сети и результатов блока предобработки мультиспектральных изображений\n",
    "        model = MultispectralFuseOut(model, preprocess_layer, preprocessing_out_dim=nn_in_cannels_num, fusion_type=fuze_params['type'], class_num=class_num)\n",
    "\n",
    "    # добавление аугментаций\n",
    "    train_transforms = v2.Compose([v2.ToDtype(torch.float32, scale=True)])\n",
    "    test_transforms = v2.Compose([v2.ToDtype(torch.float32, scale=True)])\n",
    "\n",
    "    multitask_str = '_MT' if is_multitask else ''\n",
    "    # формирование имени модели для сохранения в лог \n",
    "    model_name = f'{model_name}pr'\n",
    "    if preprocess_params['type'] != 'no':\n",
    "        model_name += f'-P{preprocess_params[\"type\"]}'\n",
    "    if fuze_params['type'] != 'no':\n",
    "        model_name += f'-Fuz{fuze_params[\"type\"].capitalize()}'\n",
    "    \n",
    "    #model_name = f'{model_name}pr-P2L-FuzOutAdd({nn_in_cannels_num})' + multitask_str\n",
    "    \n",
    "    #model_name = f'{model_name}pr' + multitask_str\n",
    "\n",
    "    return {'name': model_name, 'model': model, 'train_transforms':train_transforms, 'test_transforms':test_transforms}\n",
    "\n",
    "def prepare_smp_model(\n",
    "        model_dict,\n",
    "        image_channels,\n",
    "        nn_in_cannels_num,\n",
    "        class_num,\n",
    "        channel_indices_list,\n",
    "        is_multitask,\n",
    "        preprocess_params,\n",
    "        fuze_params):\n",
    "    '''\n",
    "    Функция генерирует конфигурацию сегментационной нейронной сети из библиотеки segmentation_models_pytorch\n",
    "    In:\n",
    "        model_dict - словарь с параметрами модели. Структура словаря {'model_name': строковое_имя_модели, 'creation_function': функция_создания_модели (класс модели), 'weights': веса модели}\n",
    "        image_channels:int - количество каналов изображения\n",
    "        nn_in_cannels_num:int - количество входных каналов нейронной сети\n",
    "        class_num:int - количество классов\n",
    "        channel_indices_list:list - список индексов каналов обрабатываемого мультиспектрального изображения\n",
    "        is_multitask:bool - флаг, обозначающий мнгогозадачное обучение\n",
    "        preprocess_params:dict - параметры предварительной обработки каналов мультиспектра. Словарь структуры {'type':'1L'}, на месте '1L' мб\n",
    "            'no' - отсутствие доп. блока предобработки,\n",
    "            1L - однослойная модель предобработки\n",
    "            2L - двухслойная модель предобработки\n",
    "            SpInd - SpatialIndex??? (надо подробнее рассмотреть это потом)\n",
    "        fuze_params:dict - параметры связывания выхода нейронной сети и блока преобработки. Словарь структуры {'type':'no'}, на месте 'no' мб\n",
    "            shuffle - модуль channel_shuffle (см. https://arxiv.org/abs/1707.01083)\n",
    "            concat - конкатенация\n",
    "            add - сложение\n",
    "    Out:\n",
    "        dict с моделью следующей структуры {'name': model_name, 'model': model, 'train_transforms':train_transforms, 'test_transforms':test_transforms}\n",
    "    '''\n",
    "    model_creation_unction = model_dict['creation_function']\n",
    "    encoder_name = model_dict['encoder_name']\n",
    "    model_name = model_dict['model_name']\n",
    "    #model = model_creation_unction(encoder_name=encoder_name, encoder_weights='imagenet', encoder_depth=5, decoder_channels=(256, 128, 64, 64, 64), in_channels=nn_in_cannels_num, classes=class_num)\n",
    "    model = model_creation_unction(encoder_name=encoder_name, encoder_weights=None, in_channels=nn_in_cannels_num, classes=class_num)\n",
    "    #model = model_creation_unction(encoder_name=encoder_name, in_channels=nn_in_cannels_num, classes=class_num, img_size=112)\n",
    "\n",
    "    #print(model)\n",
    "    \n",
    "    #model = smp.Unet(encoder_name='resnet50', classes=class_num)\n",
    "    #print(model.encoder)\n",
    "    #conv1 = model.encoder.model.conv1\n",
    "    #conv1 = model.encoder.model.conv_stem\n",
    "    conv1 = model.encoder._conv_stem\n",
    "    #conv1 = model.encoder.model.patch_embed.proj\n",
    "    \n",
    "    '''\n",
    "    weights = conv1.weight\n",
    "    new_weight = torch.cat([weights.mean(dim=1).unsqueeze(1)]*nn_in_cannels_num, dim=1)\n",
    "    new_conv1 = nn.Conv2d(\n",
    "        in_channels=nn_in_cannels_num,\n",
    "        out_channels=conv1.out_channels,\n",
    "        kernel_size=conv1.kernel_size,\n",
    "        #stride=conv1.stride,\n",
    "        stride=(1,1),\n",
    "        padding=conv1.padding,\n",
    "        dilation=conv1.dilation,\n",
    "        groups=conv1.groups,\n",
    "        bias=conv1.bias is not None\n",
    "    )\n",
    "    new_conv1.weight = nn.Parameter(new_weight)\n",
    "    if conv1.bias is not None:\n",
    "        #new_conv1.bias = model.encoder.conv1.bias\n",
    "        #new_conv1.bias = model.encoder.model.patch_embed.proj.bias\n",
    "        new_conv1.bias = model.encoder.model.conv_stem.bias\n",
    "    # заменяем веса, если количнество входных каналов не равно трем \n",
    "    if nn_in_cannels_num != 3:\n",
    "        #model.encoder.conv1 = new_conv1\n",
    "        #model.encoder.model.conv_stem = new_conv1\n",
    "        model.encoder._conv_stem = new_conv1\n",
    "        #model.encoder.model.patch_embed.proj = new_conv1\n",
    "    '''\n",
    "    # делаем два входных слоя для более глубокой обработки низкоуровневых призников\n",
    "    conv1 = model.encoder._conv_stem\n",
    "    weights = conv1.weight\n",
    "    new_weight1 = torch.cat([weights.mean(dim=1).unsqueeze(1)]*nn_in_cannels_num, dim=1)\n",
    "    new_weight2 = torch.cat([weights.mean(dim=1).unsqueeze(1)]*conv1.out_channels, dim=1)\n",
    "    new_conv1 = nn.Conv2d(\n",
    "        in_channels=nn_in_cannels_num,\n",
    "        out_channels=conv1.out_channels,\n",
    "        kernel_size=conv1.kernel_size,\n",
    "        #stride=conv1.stride,\n",
    "        stride=(1,1),\n",
    "        #padding=conv1.padding,\n",
    "        padding=(1, 1),\n",
    "        dilation=conv1.dilation,\n",
    "        groups=conv1.groups,\n",
    "        bias=conv1.bias is not None\n",
    "    )\n",
    "\n",
    "    new_conv2 = nn.Conv2d(\n",
    "        in_channels=conv1.out_channels,\n",
    "        out_channels=conv1.out_channels,\n",
    "        kernel_size=conv1.kernel_size,\n",
    "        #stride=conv1.stride,\n",
    "        stride=(1,1),\n",
    "        padding=(1,1),\n",
    "        dilation=conv1.dilation,\n",
    "        groups=conv1.groups,\n",
    "        bias=conv1.bias is not None\n",
    "    )\n",
    "        \n",
    "    new_conv1.weight = nn.Parameter(new_weight1)\n",
    "    new_conv2.weight = nn.Parameter(new_weight2)\n",
    "    if conv1.bias is not None:\n",
    "        #new_conv1.bias = model.encoder.conv1.bias\n",
    "        #new_conv1.bias = model.encoder.model.patch_embed.proj.bias\n",
    "        new_conv1.bias = model.encoder.model.conv_stem.bias\n",
    "        new_conv2.bias = model.encoder.model.conv_stem.bias\n",
    "\n",
    "    new_conv = nn.Sequential(\n",
    "        new_conv1,\n",
    "        nn.BatchNorm2d(conv1.out_channels),\n",
    "        nn.SiLU(),\n",
    "        new_conv2,\n",
    "    )\n",
    "    # заменяем веса, если количнество входных каналов не равно трем \n",
    "    if nn_in_cannels_num != 3:\n",
    "        #model.encoder.conv1 = new_conv1\n",
    "        #model.encoder.model.conv_stem = new_conv1\n",
    "        model.encoder._conv_stem = new_conv\n",
    "        #model.encoder.model.patch_embed.proj = new_conv1\n",
    "\n",
    "    if is_multitask:\n",
    "        model = MultitaskModel(model, nn_output_size=512, appl_class_num=2, surf_class_num=class_num)\n",
    "\n",
    "    if preprocess_params['type'] == 'no':\n",
    "        preprocess_layer = nn.Identity()\n",
    "    elif preprocess_params['type'] == '1L':\n",
    "        preprocess_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=image_channels, out_channels=nn_in_cannels_num, kernel_size=1),\n",
    "            nn.BatchNorm2d(nn_in_cannels_num)\n",
    "        )\n",
    "    elif preprocess_params['type'] == '2L':\n",
    "        preprocess_layer = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=image_channels, out_channels=32, kernel_size=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=32, out_channels=nn_in_cannels_num, kernel_size=1),\n",
    "            nn.BatchNorm2d(nn_in_cannels_num),\n",
    "            nn.ReLU())\n",
    "    elif preprocess_params['type'] == 'SpInd':\n",
    "        preprocess_layer = SpectralDiffIndexModule(channel_indices_list=channel_indices_list, channels_in_index=2, out_channels=8)\n",
    "    \n",
    "    if fuze_params['type'] == 'no':\n",
    "        model = MultispectralNN(model, preprocess_layer)\n",
    "    else:\n",
    "        model = MultispectralFuseOut(model, preprocess_layer, preprocessing_out_dim=nn_in_cannels_num, fusion_type=fuze_params['type'], class_num=class_num)\n",
    "    \n",
    "    #train_transforms = v2.Compose([v2.Resize((160,160), antialias=True),v2.ToDtype(torch.float32, scale=True)])\n",
    "    #test_transforms = v2.Compose([v2.Resize((160,160), antialias=True),v2.ToDtype(torch.float32, scale=True)])\n",
    "    train_transforms = v2.Compose([v2.Resize((112,112), antialias=True),v2.ToDtype(torch.float32, scale=True)])\n",
    "    test_transforms = v2.Compose([v2.Resize((112,112), antialias=True),v2.ToDtype(torch.float32, scale=True)])\n",
    "    \n",
    "    multitask_str = '_MT' if is_multitask else ''\n",
    "    \n",
    "    model_name = f'{model_name}'\n",
    "    #model_name = f'{model_name}'\n",
    "    if preprocess_params['type'] != 'no':\n",
    "        model_name += f'-P{preprocess_params[\"type\"]}'\n",
    "    if fuze_params['type'] != 'no':\n",
    "        model_name += f'-Fuz{fuze_params[\"type\"].capitalize()}'\n",
    "    #model_name = f'{model_name}pr-P2L-FuzOutAdd({nn_in_cannels_num})' + multitask_str\n",
    "    return {'name': model_name, 'model': model, 'train_transforms':train_transforms, 'test_transforms':test_transforms}\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Создание объектов нейронной сети, датасетов, даталоадеров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\admin/.cache\\torch\\hub\\adeelh_pytorch-multi-class-focal-loss_master\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 13, 112, 112]) torch.Size([16, 11, 112, 112])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'unet_effnet_d-5_strd1_1inConv'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Путь до папки с данными (надо поменять на актуальный путь до папки с данными)\n",
    "#path_to_dataset_root = r'I:\\LANDCOVER_DATA\\MULTISPECTRAL_SATELLITE_DATA\\DATA_FOR_TRAINIG'\n",
    "#path_to_dataset_root = '/home/aggr/mikhail_u/DATA/DATA_FOR_TRAINIG'\n",
    "path_to_dataset_root = r'C:\\Users\\admin\\python_programming\\DATA\\MULTISPECTRAL_DATA_FOR_TRAINIG_NEW'\n",
    "path_to_dataset_info_csv = os.path.join(path_to_dataset_root, 'data_info_table.csv')\n",
    "path_to_surface_classes_json = os.path.join(path_to_dataset_root, 'surface_classes.json')\n",
    "\n",
    "# чтение списка имен классов поверхностей\n",
    "with open(path_to_surface_classes_json) as fd:\n",
    "    surface_classes_list = json.load(fd)\n",
    "# чтение таблицы с информацией о каждом изображении в выборке\n",
    "images_df = pd.read_csv(path_to_dataset_info_csv)\n",
    "\n",
    "path_to_partition_json = os.path.join(path_to_dataset_root, 'dataset_partition.json')\n",
    "# чтение словаря со списками квадратов, находящихся в обучающей и тестовой выборке\n",
    "with open(path_to_partition_json) as fd:\n",
    "    partition_dict = json.load(fd)\n",
    "\n",
    "# заменить при перетасовке классов\n",
    "applicable_surfaces_dict = {\n",
    "    'UNLABELED': False,\n",
    "    'buildings_territory': False,\n",
    "    'natural_ground': True,\n",
    "    'natural_grow': True,\n",
    "    'natural_wetland': True,\n",
    "    'natural_wood': True,\n",
    "    'quasi_natural_grow': False,\n",
    "    'transport': False,\n",
    "    'water': False\n",
    "}\n",
    "\n",
    "# формирование pandas DataFrame-ов с информацией об изображениях обучающей и тестовой выборках\n",
    "train_images_df = []\n",
    "for train_square in partition_dict['train_squares']:\n",
    "    train_images_df.append(images_df[images_df['square_id']==train_square])\n",
    "train_images_df = pd.concat(train_images_df, ignore_index=True)\n",
    "\n",
    "test_images_df = []\n",
    "for test_square in partition_dict['test_squares']:\n",
    "    test_images_df.append(images_df[images_df['square_id']==test_square])\n",
    "test_images_df = pd.concat(test_images_df, ignore_index=True)\n",
    "\n",
    "#train_images_df, test_images_df = train_test_split(images_df, test_size=0.3, random_state=0)\n",
    "\n",
    "class_num = images_df['class_num'].iloc[0]\n",
    "\n",
    "# формирование словаря, отображающейго имя класса поверхности в индекс класса\n",
    "class_name2idx_dict = {n:i for i, n in enumerate(surface_classes_list)}\n",
    "\n",
    "# вычисление распределений пикселей в классах поверхностей \n",
    "classes_pixels_distribution_df = images_df[surface_classes_list]\n",
    "classes_pixels_num = classes_pixels_distribution_df.sum()\n",
    "classes_weights = classes_pixels_num / classes_pixels_num.sum()\n",
    "classes_weights = classes_weights[surface_classes_list].to_numpy()\n",
    "\n",
    "# определение индексов обрабатываемых каналов мультиспектра\n",
    "channel_indices = [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12]\n",
    "#channel_indices = [1, 2, 3, 7]\n",
    "#channel_indices = [1, 2, 3]\n",
    "in_channels_num = len(channel_indices)\n",
    "is_multitask = False\n",
    "'''\n",
    "# словари с настройками\n",
    "deeplab_dict = {\n",
    "    'model_name': 'dlv3_res50',\n",
    "    'creation_function': models.segmentation.deeplabv3_resnet50,\n",
    "    'weights': models.segmentation.DeepLabV3_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1\n",
    "}\n",
    "fcn_dict = {\n",
    "    'model_name': 'fcn_res50',\n",
    "    'creation_function': models.segmentation.fcn_resnet50,\n",
    "    'weights': models.segmentation.FCN_ResNet50_Weights.COCO_WITH_VOC_LABELS_V1\n",
    "}\n",
    "\n",
    "# создаем модель по введенным настройкам из библиотеки torchvision\n",
    "model_dict = prepare_torch_model(\n",
    "    model_dict=fcn_dict,\n",
    "    image_channels=in_channels_num,\n",
    "    nn_in_cannels_num=in_channels_num,\n",
    "    class_num=class_num,\n",
    "    channel_indices_list=channel_indices,\n",
    "    is_multitask=is_multitask,\n",
    "    preprocess_params={'type':'no'}, #'no', 1L, 2L, SpInd\n",
    "    fuze_params={'type':'no'} #'no', shuffle, concat, add\n",
    "    )\n",
    "#model_dict  = prepare_torch_model(image_channels=in_channels_num, nn_in_cannels_num=in_channels_num, class_num=class_num, channel_indices_list=channel_indices, is_multitask=is_multitask)\n",
    "#model_dict  = prepare_torch_model(image_channels=in_channels_num, nn_in_cannels_num=in_channels_num, class_num=2, channel_indices_list=channel_indices, is_multitask=is_multitask)\n",
    "#model_dict  = prepare_torch_model(image_channels=in_channels_num, nn_in_cannels_num=in_channels_num, class_num=class_num, channel_indices_list=channel_indices, is_multitask=is_multitask)\n",
    "\n",
    "# раскомментировать, если надо обучить модель из библиотеки segmentation_models_pytorch\n",
    "'''\n",
    "unet_dict = {\n",
    "    'model_name': 'unet_effnet_d-5_strd1_1inConv',\n",
    "    'creation_function': smp.Unet,\n",
    "    'encoder_name': 'efficientnet-b0'\n",
    "}\n",
    "\n",
    "model_dict  = prepare_smp_model(\n",
    "    model_dict=unet_dict,\n",
    "    image_channels=in_channels_num,\n",
    "    nn_in_cannels_num=in_channels_num,\n",
    "    class_num=class_num,\n",
    "    channel_indices_list=channel_indices,\n",
    "    is_multitask=False,\n",
    "    preprocess_params={'type':'no'}, # 'no', 1L, 2L, SpInd\n",
    "    fuze_params={'type':'no'} # shuffle, concat, add\n",
    "    )\n",
    "\n",
    "# получаем основные параметры нейронной сети\n",
    "model_name = model_dict['name']\n",
    "model = model_dict['model']\n",
    "train_transforms = model_dict['train_transforms']\n",
    "test_transforms = model_dict['test_transforms']\n",
    "\n",
    "# по умолчанию, обучаем на GPU\n",
    "device = torch.device('cuda:0')\n",
    "#device = torch.device('cpu')\n",
    "\n",
    "# определение функций потерь (несколько ф-ций было взято для исследования, какая себя лучше проявит)\n",
    "gamma_val = 2\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "multitask_criterion = MultitaskLoss(nn.CrossEntropyLoss(), nn.CrossEntropyLoss())\n",
    "classes_weights = torch.as_tensor(classes_weights, dtype=torch.float32, device=device)\n",
    "focal_criterion = torch.hub.load(\n",
    "        'adeelh/pytorch-multi-class-focal-loss',\n",
    "        model='FocalLoss',\n",
    "        alpha=classes_weights,\n",
    "        gamma=gamma_val,\n",
    "        reduction='mean',\n",
    "        force_reload=False\n",
    "    )\n",
    "\n",
    "model = model.to(device)\n",
    "\n",
    "# создаем датасеты и даталоадеры\n",
    "train_dataset = SegmentationDataset(path_to_dataset_root=path_to_dataset_root, samples_df=train_images_df, channel_indices=channel_indices, transforms=train_transforms, device=device)\n",
    "test_dataset = SegmentationDataset(path_to_dataset_root=path_to_dataset_root, samples_df=test_images_df,channel_indices=channel_indices, transforms=test_transforms, device=device)\n",
    "#train_dataset = SegmentationDatasetApplSurf(path_to_dataset_root=path_to_dataset_root, samples_df=test_images_df, channel_indices=channel_indices, name2class_idx_dict=class_name2idx_dict, applicable_surfaces_dict=applicable_surfaces_dict, transforms=test_transforms, device=device)\n",
    "#test_dataset = SegmentationDatasetApplSurf(path_to_dataset_root=path_to_dataset_root, samples_df=test_images_df, channel_indices=channel_indices, name2class_idx_dict=class_name2idx_dict, applicable_surfaces_dict=applicable_surfaces_dict, transforms=test_transforms, device=device)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "# тестовое чтение данных\n",
    "for data, labels in train_loader:\n",
    "    break\n",
    "    pred = model(data)\n",
    "    loss = criterion(pred, labels)\n",
    "\n",
    "# тестовая обработка данных нейронной сетью\n",
    "ret = model(data)\n",
    "if is_multitask:\n",
    "    img_size = list(ret[0].shape[2:])\n",
    "    # печатаем выход нейронной сети\n",
    "    print(ret[0].shape, ret[1].shape)\n",
    "else:\n",
    "    img_size = list(ret.shape[2:])\n",
    "    # печатаем выход нейронной сети\n",
    "    print(data.shape, ret.shape)\n",
    "#\n",
    "\n",
    "model_name"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Однозадачное обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############################\n",
      "unet_effnet_d-5_strd1_1inConv-100ep-11cl-112-ch_[0-12]\n",
      "#############################\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\admin\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:654: Checkpoint directory C:\\Users\\admin\\python_programming\\MultispectralSegmentation\\saving_dir\\unet_effnet_d-5_strd1_1inConv-100ep-11cl-112-ch_[0-12] exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type             | Params | Mode \n",
      "-------------------------------------------------------\n",
      "0 | model     | MultispectralNN  | 6.3 M  | train\n",
      "1 | criterion | CrossEntropyLoss | 0      | train\n",
      "-------------------------------------------------------\n",
      "6.3 M     Trainable params\n",
      "0         Non-trainable params\n",
      "6.3 M     Total params\n",
      "25.060    Total estimated model params size (MB)\n",
      "330       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\admin\\miniconda3\\envs\\deep_learning\\Lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 88/88 [00:23<00:00,  3.81it/s, v_num=0, val_loss=1.070, val_iou_UNLABELED=0.823, val_iou_buildings_territory=0.705, val_iou_natural_ground=0.120, val_iou_natural_grow=0.116, val_iou_natural_wetland=0.353, val_iou_natural_wood=0.761, val_iou_quasi_natural_ground=0.00959, val_iou_quasi_natural_grow=0.467, val_iou_quasi_natural_wetland=0.229, val_iou_transport=0.303, val_iou_water=0.852, val_iou_mean=0.431, val_recall_UNLABELED=0.885, val_recall_buildings_territory=0.831, val_recall_natural_ground=0.213, val_recall_natural_grow=0.147, val_recall_natural_wetland=0.493, val_recall_natural_wood=0.932, val_recall_quasi_natural_ground=0.0182, val_recall_quasi_natural_grow=0.645, val_recall_quasi_natural_wetland=0.240, val_recall_transport=0.411, val_recall_water=0.898, val_recall_mean=0.519, val_precision_UNLABELED=0.922, val_precision_buildings_territory=0.823, val_precision_natural_ground=0.216, val_precision_natural_grow=0.355, val_precision_natural_wetland=0.554, val_precision_natural_wood=0.806, val_precision_quasi_natural_ground=0.0199, val_precision_quasi_natural_grow=0.628, val_precision_quasi_natural_wetland=0.835, val_precision_transport=0.536, val_precision_water=0.943, val_precision_mean=0.603, train_loss=0.257, train_iou_UNLABELED=0.871, train_iou_buildings_territory=0.847, train_iou_natural_ground=0.251, train_iou_natural_grow=0.711, train_iou_natural_wetland=0.800, train_iou_natural_wood=0.896, train_iou_quasi_natural_ground=0.657, train_iou_quasi_natural_grow=0.892, train_iou_quasi_natural_wetland=0.626, train_iou_transport=0.426, train_iou_water=0.809, train_iou_mean=0.708, train_recall_UNLABELED=0.924, train_recall_buildings_territory=0.937, train_recall_natural_ground=0.307, train_recall_natural_grow=0.814, train_recall_natural_wetland=0.889, train_recall_natural_wood=0.956, train_recall_quasi_natural_ground=0.807, train_recall_quasi_natural_grow=0.949, train_recall_quasi_natural_wetland=0.710, train_recall_transport=0.519, train_recall_water=0.857, train_recall_mean=0.788, train_precision_UNLABELED=0.938, train_precision_buildings_territory=0.899, train_precision_natural_ground=0.579, train_precision_natural_grow=0.850, train_precision_natural_wetland=0.889, train_precision_natural_wood=0.934, train_precision_quasi_natural_ground=0.780, train_precision_quasi_natural_grow=0.937, train_precision_quasi_natural_wetland=0.841, train_precision_transport=0.703, train_precision_water=0.935, train_precision_mean=0.844]                             "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 88/88 [00:23<00:00,  3.79it/s, v_num=0, val_loss=1.070, val_iou_UNLABELED=0.823, val_iou_buildings_territory=0.705, val_iou_natural_ground=0.120, val_iou_natural_grow=0.116, val_iou_natural_wetland=0.353, val_iou_natural_wood=0.761, val_iou_quasi_natural_ground=0.00959, val_iou_quasi_natural_grow=0.467, val_iou_quasi_natural_wetland=0.229, val_iou_transport=0.303, val_iou_water=0.852, val_iou_mean=0.431, val_recall_UNLABELED=0.885, val_recall_buildings_territory=0.831, val_recall_natural_ground=0.213, val_recall_natural_grow=0.147, val_recall_natural_wetland=0.493, val_recall_natural_wood=0.932, val_recall_quasi_natural_ground=0.0182, val_recall_quasi_natural_grow=0.645, val_recall_quasi_natural_wetland=0.240, val_recall_transport=0.411, val_recall_water=0.898, val_recall_mean=0.519, val_precision_UNLABELED=0.922, val_precision_buildings_territory=0.823, val_precision_natural_ground=0.216, val_precision_natural_grow=0.355, val_precision_natural_wetland=0.554, val_precision_natural_wood=0.806, val_precision_quasi_natural_ground=0.0199, val_precision_quasi_natural_grow=0.628, val_precision_quasi_natural_wetland=0.835, val_precision_transport=0.536, val_precision_water=0.943, val_precision_mean=0.603, train_loss=0.257, train_iou_UNLABELED=0.871, train_iou_buildings_territory=0.847, train_iou_natural_ground=0.251, train_iou_natural_grow=0.711, train_iou_natural_wetland=0.800, train_iou_natural_wood=0.896, train_iou_quasi_natural_ground=0.657, train_iou_quasi_natural_grow=0.892, train_iou_quasi_natural_wetland=0.626, train_iou_transport=0.426, train_iou_water=0.809, train_iou_mean=0.708, train_recall_UNLABELED=0.924, train_recall_buildings_territory=0.937, train_recall_natural_ground=0.307, train_recall_natural_grow=0.814, train_recall_natural_wetland=0.889, train_recall_natural_wood=0.956, train_recall_quasi_natural_ground=0.807, train_recall_quasi_natural_grow=0.949, train_recall_quasi_natural_wetland=0.710, train_recall_transport=0.519, train_recall_water=0.857, train_recall_mean=0.788, train_precision_UNLABELED=0.938, train_precision_buildings_territory=0.899, train_precision_natural_ground=0.579, train_precision_natural_grow=0.850, train_precision_natural_wetland=0.889, train_precision_natural_wood=0.934, train_precision_quasi_natural_ground=0.780, train_precision_quasi_natural_grow=0.937, train_precision_quasi_natural_wetland=0.841, train_precision_transport=0.703, train_precision_water=0.935, train_precision_mean=0.844]\n"
     ]
    }
   ],
   "source": [
    "epoch_num = 100\n",
    "#model_name = 'FCN-150ep-11cl-150-ch-res-10-20m'\n",
    "channels_str = prepare_channel_indices_str(channel_indices)\n",
    "\n",
    "# формируем имя модели для лога\n",
    "model_name = f'{model_name}-{epoch_num}ep-{class_num}cl-{img_size[0]}-ch_[{channels_str}]'\n",
    "#model_name = f'{model_name}-{epoch_num}ep-2cl-{img_size[0]}-ch_[{channels_str}]'\n",
    "print('#############################')\n",
    "print(model_name)\n",
    "print('#############################')\n",
    "print()\n",
    "#model_name = 'TEST'\n",
    "\n",
    "# создаем список словарей с информацией о вычисляемых метриках с помощью multiclass confusion matrix\n",
    "# см. подробнее ддокументацию к функции compute_metric_from_confusion\n",
    "metrics_info_list = [\n",
    "    {'name': 'iou', 'numerator': ['tp'], 'denominator': ['tp', 'fp', 'fn']},\n",
    "    {'name': 'recall', 'numerator': ['tp'], 'denominator': ['tp', 'fn']},\n",
    "    {'name': 'precision', 'numerator': ['tp'], 'denominator': ['tp', 'fp']}\n",
    "    ]\n",
    "\n",
    "# Создаем модуль Lightning\n",
    "segmentation_module = SegmentationModule(model, criterion, metrics_info_list, class_name2idx_dict)\n",
    "\n",
    "# задаем путь до папки с логгерами и создаем логгер, записывающий результаты в csv\n",
    "path_to_saving_dir = 'saving_dir'\n",
    "logger = CSVLogger(\n",
    "    save_dir = path_to_saving_dir,\n",
    "    name=model_name, \n",
    "    flush_logs_every_n_steps=1,\n",
    "    )\n",
    "\n",
    "# создаем объект, записывающий в чекпоинт лучшую модель\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    mode=\"max\",\n",
    "    filename=model_name+\"-{epoch:02d}-{val_iou_mean:.3}\",\n",
    "    dirpath=os.path.join(path_to_saving_dir, model_name), \n",
    "    save_top_k=1, monitor=\"val_iou_mean\"\n",
    "    )\n",
    "\n",
    "trainer = L.Trainer(logger=logger,\n",
    "        max_epochs=epoch_num, \n",
    "        callbacks=[checkpoint_callback],\n",
    "        accelerator = 'gpu'\n",
    "        )\n",
    "\n",
    "trainer.fit(segmentation_module , train_loader, test_loader)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Многозадачное обучение"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (cuda), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "c:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\callbacks\\model_checkpoint.py:654: Checkpoint directory C:\\Users\\mokhail\\python_programming\\MultispectralSegmentation\\saving_dir\\fcn_res50pr-P1L(13)_MT-100ep-9cl-150-ch_[0-12] exists and is not empty.\n",
      "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
      "\n",
      "  | Name      | Type           | Params | Mode \n",
      "-----------------------------------------------------\n",
      "0 | model     | MultitaskModel | 35.3 M | train\n",
      "1 | criterion | MultitaskLoss  | 0      | train\n",
      "-----------------------------------------------------\n",
      "35.3 M    Trainable params\n",
      "0         Non-trainable params\n",
      "35.3 M    Total params\n",
      "141.394   Total estimated model params size (MB)\n",
      "170       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#############################\n",
      "fcn_res50pr-P1L(13)_MT-100ep-9cl-150-ch_[0-12]\n",
      "#############################\n",
      "\n",
      "Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mokhail\\miniconda3\\envs\\deep_learning\\lib\\site-packages\\lightning\\pytorch\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=7` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 98/98 [00:24<00:00,  3.97it/s, v_num=1, val_loss=1.680, v_cl_iou_UNLABELED=0.772, v_cl_iou_buildings_territory=0.737, v_cl_iou_natural_ground=0.0843, v_cl_iou_natural_grow=0.281, v_cl_iou_natural_wetland=0.229, v_cl_iou_natural_wood=0.785, v_cl_iou_quasi_natural_grow=0.601, v_cl_iou_transport=0.0979, v_cl_iou_water=0.673, v_cl_iou_mean=0.473, v_ap_iou_non_appl=0.740, v_ap_iou_appl=0.842, v_ap_iou_mean=0.791, v_iou_mean=0.632, v_cl_recall_UNLABELED=0.887, v_cl_recall_buildings_territory=0.849, v_cl_recall_natural_ground=0.106, v_cl_recall_natural_grow=0.384, v_cl_recall_natural_wetland=0.267, v_cl_recall_natural_wood=0.947, v_cl_recall_quasi_natural_grow=0.784, v_cl_recall_transport=0.128, v_cl_recall_water=0.724, v_cl_recall_mean=0.564, v_ap_recall_non_appl=0.841, v_ap_recall_appl=0.920, v_ap_recall_mean=0.881, v_recall_mean=0.722, v_cl_precision_UNLABELED=0.856, v_cl_precision_buildings_territory=0.848, v_cl_precision_natural_ground=0.291, v_cl_precision_natural_grow=0.513, v_cl_precision_natural_wetland=0.612, v_cl_precision_natural_wood=0.821, v_cl_precision_quasi_natural_grow=0.721, v_cl_precision_transport=0.294, v_cl_precision_water=0.905, v_cl_precision_mean=0.651, v_ap_precision_non_appl=0.860, v_ap_precision_appl=0.909, v_ap_precision_mean=0.884, v_precision_mean=0.768, train_loss=0.172, tr_cl_iou_UNLABELED=0.891, tr_cl_iou_buildings_territory=0.920, tr_cl_iou_natural_ground=0.854, tr_cl_iou_natural_grow=0.870, tr_cl_iou_natural_wetland=0.910, tr_cl_iou_natural_wood=0.938, tr_cl_iou_quasi_natural_grow=0.925, tr_cl_iou_transport=0.460, tr_cl_iou_water=0.796, tr_cl_iou_mean=0.840, tr_ap_iou_non_appl=0.920, tr_ap_iou_appl=0.954, tr_ap_iou_mean=0.937, tr_iou_mean=0.889, tr_cl_recall_UNLABELED=0.907, tr_cl_recall_buildings_territory=0.966, tr_cl_recall_natural_ground=0.932, tr_cl_recall_natural_grow=0.929, tr_cl_recall_natural_wetland=0.951, tr_cl_recall_natural_wood=0.975, tr_cl_recall_quasi_natural_grow=0.969, tr_cl_recall_transport=0.581, tr_cl_recall_water=0.873, tr_cl_recall_mean=0.898, tr_ap_recall_non_appl=0.950, tr_ap_recall_appl=0.982, tr_ap_recall_mean=0.966, tr_recall_mean=0.932, tr_cl_precision_UNLABELED=0.980, tr_cl_precision_buildings_territory=0.951, tr_cl_precision_natural_ground=0.911, tr_cl_precision_natural_grow=0.931, tr_cl_precision_natural_wetland=0.954, tr_cl_precision_natural_wood=0.961, tr_cl_precision_quasi_natural_grow=0.953, tr_cl_precision_transport=0.687, tr_cl_precision_water=0.900, tr_cl_precision_mean=0.914, tr_ap_precision_non_appl=0.968, tr_ap_precision_appl=0.971, tr_ap_precision_mean=0.970, tr_precision_mean=0.942]                   "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=100` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 99: 100%|██████████| 98/98 [00:24<00:00,  3.94it/s, v_num=1, val_loss=1.680, v_cl_iou_UNLABELED=0.772, v_cl_iou_buildings_territory=0.737, v_cl_iou_natural_ground=0.0843, v_cl_iou_natural_grow=0.281, v_cl_iou_natural_wetland=0.229, v_cl_iou_natural_wood=0.785, v_cl_iou_quasi_natural_grow=0.601, v_cl_iou_transport=0.0979, v_cl_iou_water=0.673, v_cl_iou_mean=0.473, v_ap_iou_non_appl=0.740, v_ap_iou_appl=0.842, v_ap_iou_mean=0.791, v_iou_mean=0.632, v_cl_recall_UNLABELED=0.887, v_cl_recall_buildings_territory=0.849, v_cl_recall_natural_ground=0.106, v_cl_recall_natural_grow=0.384, v_cl_recall_natural_wetland=0.267, v_cl_recall_natural_wood=0.947, v_cl_recall_quasi_natural_grow=0.784, v_cl_recall_transport=0.128, v_cl_recall_water=0.724, v_cl_recall_mean=0.564, v_ap_recall_non_appl=0.841, v_ap_recall_appl=0.920, v_ap_recall_mean=0.881, v_recall_mean=0.722, v_cl_precision_UNLABELED=0.856, v_cl_precision_buildings_territory=0.848, v_cl_precision_natural_ground=0.291, v_cl_precision_natural_grow=0.513, v_cl_precision_natural_wetland=0.612, v_cl_precision_natural_wood=0.821, v_cl_precision_quasi_natural_grow=0.721, v_cl_precision_transport=0.294, v_cl_precision_water=0.905, v_cl_precision_mean=0.651, v_ap_precision_non_appl=0.860, v_ap_precision_appl=0.909, v_ap_precision_mean=0.884, v_precision_mean=0.768, train_loss=0.172, tr_cl_iou_UNLABELED=0.891, tr_cl_iou_buildings_territory=0.920, tr_cl_iou_natural_ground=0.854, tr_cl_iou_natural_grow=0.870, tr_cl_iou_natural_wetland=0.910, tr_cl_iou_natural_wood=0.938, tr_cl_iou_quasi_natural_grow=0.925, tr_cl_iou_transport=0.460, tr_cl_iou_water=0.796, tr_cl_iou_mean=0.840, tr_ap_iou_non_appl=0.920, tr_ap_iou_appl=0.954, tr_ap_iou_mean=0.937, tr_iou_mean=0.889, tr_cl_recall_UNLABELED=0.907, tr_cl_recall_buildings_territory=0.966, tr_cl_recall_natural_ground=0.932, tr_cl_recall_natural_grow=0.929, tr_cl_recall_natural_wetland=0.951, tr_cl_recall_natural_wood=0.975, tr_cl_recall_quasi_natural_grow=0.969, tr_cl_recall_transport=0.581, tr_cl_recall_water=0.873, tr_cl_recall_mean=0.898, tr_ap_recall_non_appl=0.950, tr_ap_recall_appl=0.982, tr_ap_recall_mean=0.966, tr_recall_mean=0.932, tr_cl_precision_UNLABELED=0.980, tr_cl_precision_buildings_territory=0.951, tr_cl_precision_natural_ground=0.911, tr_cl_precision_natural_grow=0.931, tr_cl_precision_natural_wetland=0.954, tr_cl_precision_natural_wood=0.961, tr_cl_precision_quasi_natural_grow=0.953, tr_cl_precision_transport=0.687, tr_cl_precision_water=0.900, tr_cl_precision_mean=0.914, tr_ap_precision_non_appl=0.968, tr_ap_precision_appl=0.971, tr_ap_precision_mean=0.970, tr_precision_mean=0.942]\n"
     ]
    }
   ],
   "source": [
    "# multitask\n",
    "epoch_num = 100\n",
    "#model_name = 'FCN-150ep-11cl-150-ch-res-10-20m'\n",
    "channels_str = prepare_channel_indices_str(channel_indices)\n",
    "\n",
    "model_name = f'{model_name}-{epoch_num}ep-{class_num}cl-{img_size[0]}-ch_[{channels_str}]'\n",
    "print('#############################')\n",
    "print(model_name)\n",
    "print('#############################')\n",
    "print()\n",
    "#model_name = 'TEST'\n",
    "\n",
    "metrics_info_list = [\n",
    "    {'name': 'iou', 'numerator': ['tp'], 'denominator': ['tp', 'fp', 'fn']},\n",
    "    {'name': 'recall', 'numerator': ['tp'], 'denominator': ['tp', 'fn']},\n",
    "    {'name': 'precision', 'numerator': ['tp'], 'denominator': ['tp', 'fp']}\n",
    "    ]\n",
    "metrics_info_list = [\n",
    "    {'name': 'iou', 'numerator': ['tp'], 'denominator': ['tp', 'fp', 'fn']}\n",
    "    ]\n",
    "\n",
    "segmentation_module = MultitaskSegmentationModule(model, multitask_criterion, metrics_info_list, class_name2idx_dict, applicable_surfaces_dict)\n",
    "path_to_saving_dir = 'saving_dir'\n",
    "logger = CSVLogger(\n",
    "    save_dir = path_to_saving_dir,\n",
    "    name=model_name, \n",
    "    flush_logs_every_n_steps=1, \n",
    "    )\n",
    "\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    mode=\"max\",\n",
    "    filename=model_name+\"-{epoch:02d}-{v_ap_iou_mean:.3}-{v_cl_iou_mean:.3}-{v_iou_mean:.3}\",\n",
    "    dirpath=os.path.join(path_to_saving_dir, model_name), \n",
    "    save_top_k=1, monitor=\"v_iou_mean\"\n",
    "    )\n",
    "\n",
    "trainer = L.Trainer(logger=logger,\n",
    "        max_epochs=epoch_num, \n",
    "        callbacks=[checkpoint_callback],\n",
    "        accelerator = 'cuda'\n",
    "        )\n",
    "\n",
    "trainer.fit(segmentation_module , train_loader, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 2048, 19, 19])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.model.backbone(torch.randn(1, 3, 150, 150))['out'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Черновики"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "buildings_territory      0.137217\n",
       "natural_ground           0.000395\n",
       "natural_grow             0.114785\n",
       "natural_wetland          0.066782\n",
       "natural_wood             0.465326\n",
       "quasi_natural_ground     0.003570\n",
       "quasi_natural_grow       0.112242\n",
       "quasi_natural_wetland    0.005356\n",
       "quasi_natural_wood       0.000000\n",
       "transport                0.029412\n",
       "water                    0.027175\n",
       "UNLABELED                0.037740\n",
       "dtype: float64"
      ]
     },
     "execution_count": 116,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "classes_weights[surface_classes_list]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "None\n",
      "tensor(1.6373, grad_fn=<NllLossBackward0>)\n",
      "None\n",
      "tensor([[-0.6013, -1.8251, -0.3468,  1.6361,  0.4586]], requires_grad=True)\n",
      "tensor([4])\n"
     ]
    }
   ],
   "source": [
    "input = torch.randn(1, 5, requires_grad=True)\n",
    "target = torch.empty(1, dtype=torch.long).random_(5)\n",
    "print(target.grad)\n",
    "loss = nn.CrossEntropyLoss()\n",
    "output = loss(input, target)\n",
    "output.backward()\n",
    "print(output)\n",
    "print(target.grad)\n",
    "print(input)\n",
    "print(target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 2])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.5006]])\n",
      "tensor([1])\n",
      "tensor([[0.5006, 0.5006, 0.5006]], grad_fn=<MmBackward0>)\n",
      "tensor([[ 0.1669],\n",
      "        [-0.3337],\n",
      "        [ 0.1669]])\n"
     ]
    }
   ],
   "source": [
    "class_num = 3\n",
    "in_features = 1\n",
    "fc = nn.Linear(in_features, class_num, bias=False)\n",
    "fc.weight = nn.Parameter(torch.ones(class_num, in_features))\n",
    "criterion = nn.CrossEntropyLoss(reduction='sum')\n",
    "x = torch.randn(1, in_features)\n",
    "t = torch.empty(1, dtype=torch.long).random_(class_num)\n",
    "out = fc(x)\n",
    "print(x)\n",
    "print(t)\n",
    "print(out)\n",
    "loss = criterion(out, t)\n",
    "loss.backward()\n",
    "print(fc.weight.grad)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "import shutil\n",
    "from tqdm import tqdm\n",
    "\n",
    "paths_to_delete = glob.glob(r'i:\\AVABOS\\new_projects2\\*\\cut\\*')\n",
    "paths_to_delete = [p for p in paths_to_delete if os.path.isdir(p)]\n",
    "for p in tqdm(paths_to_delete):\n",
    "    shutil.rmtree(p, ignore_errors=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, paths_to_images_list, transforms, device):\n",
    "        '''\n",
    "        path_to_dataset - путь до корневой папки с датасетом\n",
    "        instance_names_list - список имен экземпляров БЕЗ РАСШИРЕНИЯ!\n",
    "        transforms - аугментация изображений\n",
    "        '''\n",
    "        super().__init__()\n",
    "        self.paths_to_images_list = paths_to_images_list\n",
    "        self.transforms = transforms\n",
    "        self.device = device\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths_to_images_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        path_to_image = self.paths_to_images_list[idx]\n",
    "\n",
    "        #image = torch.as_tensor(np.load(path_to_image))\n",
    "        #image = np.load(path_to_image)\n",
    "        image = torchvision.io.decode_image(path_to_image)\n",
    "        # метки читаем как одноканальное изображение\n",
    "                \n",
    "        image = tv_tensors.Image(image, device=self.device)\n",
    "        \n",
    "        transforms_dict = {'image':image}\n",
    "        transformed = self.transforms(transforms_dict)\n",
    "        return transformed['image']\n",
    "    \n",
    "paths_to_images = glob.glob(r'i:\\embedding_logo_datasets\\icon645\\colored_icons_final\\*\\*.png')\n",
    "transforms = v2.Compose(\n",
    "    [\n",
    "        v2.Resize((256, 256), antialias=True),\n",
    "        v2.ToDtype(torch.float32, scale=True),\n",
    "     \n",
    "     ]\n",
    "    )\n",
    "ds = TestDataset(paths_to_images, transforms, torch.device('cuda'))\n",
    "loader = torch.utils.data.DataLoader(ds, batch_size=64, shuffle=True)\n",
    "for d in tqdm(loader):\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>epoch</th>\n",
       "      <th>step</th>\n",
       "      <th>train_iou_UNLABELED</th>\n",
       "      <th>train_iou_buildings_territory</th>\n",
       "      <th>train_iou_mean</th>\n",
       "      <th>train_iou_natural_ground</th>\n",
       "      <th>train_iou_natural_grow</th>\n",
       "      <th>train_iou_natural_wetland</th>\n",
       "      <th>train_iou_natural_wood</th>\n",
       "      <th>train_iou_quasi_natural_ground</th>\n",
       "      <th>...</th>\n",
       "      <th>val_iou_natural_grow</th>\n",
       "      <th>val_iou_natural_wetland</th>\n",
       "      <th>val_iou_natural_wood</th>\n",
       "      <th>val_iou_quasi_natural_ground</th>\n",
       "      <th>val_iou_quasi_natural_grow</th>\n",
       "      <th>val_iou_quasi_natural_wetland</th>\n",
       "      <th>val_iou_quasi_natural_wood</th>\n",
       "      <th>val_iou_transport</th>\n",
       "      <th>val_iou_water</th>\n",
       "      <th>val_loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>314</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.161972</td>\n",
       "      <td>0.357346</td>\n",
       "      <td>0.734698</td>\n",
       "      <td>0.000085</td>\n",
       "      <td>0.115869</td>\n",
       "      <td>0.056907</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056050</td>\n",
       "      <td>0.227701</td>\n",
       "      <td>1.393110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>314</td>\n",
       "      <td>0.840341</td>\n",
       "      <td>0.625207</td>\n",
       "      <td>0.391350</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.283973</td>\n",
       "      <td>0.437569</td>\n",
       "      <td>0.799967</td>\n",
       "      <td>0.006817</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>629</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.483770</td>\n",
       "      <td>0.640979</td>\n",
       "      <td>0.825922</td>\n",
       "      <td>0.082659</td>\n",
       "      <td>0.653898</td>\n",
       "      <td>0.566895</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.024938</td>\n",
       "      <td>0.277133</td>\n",
       "      <td>0.568032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>629</td>\n",
       "      <td>0.852058</td>\n",
       "      <td>0.649716</td>\n",
       "      <td>0.442709</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.360270</td>\n",
       "      <td>0.495965</td>\n",
       "      <td>0.833841</td>\n",
       "      <td>0.049213</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "      <td>944</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>0.273776</td>\n",
       "      <td>0.656812</td>\n",
       "      <td>0.766886</td>\n",
       "      <td>0.101420</td>\n",
       "      <td>0.460483</td>\n",
       "      <td>0.745244</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.072769</td>\n",
       "      <td>0.265772</td>\n",
       "      <td>1.006425</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2</td>\n",
       "      <td>944</td>\n",
       "      <td>0.861290</td>\n",
       "      <td>0.682973</td>\n",
       "      <td>0.515124</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.454220</td>\n",
       "      <td>0.613231</td>\n",
       "      <td>0.861346</td>\n",
       "      <td>0.125224</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   epoch  step  train_iou_UNLABELED  train_iou_buildings_territory  \\\n",
       "0      0   314                  NaN                            NaN   \n",
       "1      0   314             0.840341                       0.625207   \n",
       "2      1   629                  NaN                            NaN   \n",
       "3      1   629             0.852058                       0.649716   \n",
       "4      2   944                  NaN                            NaN   \n",
       "5      2   944             0.861290                       0.682973   \n",
       "\n",
       "   train_iou_mean  train_iou_natural_ground  train_iou_natural_grow  \\\n",
       "0             NaN                       NaN                     NaN   \n",
       "1        0.391350                       0.0                0.283973   \n",
       "2             NaN                       NaN                     NaN   \n",
       "3        0.442709                       0.0                0.360270   \n",
       "4             NaN                       NaN                     NaN   \n",
       "5        0.515124                       0.0                0.454220   \n",
       "\n",
       "   train_iou_natural_wetland  train_iou_natural_wood  \\\n",
       "0                        NaN                     NaN   \n",
       "1                   0.437569                0.799967   \n",
       "2                        NaN                     NaN   \n",
       "3                   0.495965                0.833841   \n",
       "4                        NaN                     NaN   \n",
       "5                   0.613231                0.861346   \n",
       "\n",
       "   train_iou_quasi_natural_ground  ...  val_iou_natural_grow  \\\n",
       "0                             NaN  ...              0.161972   \n",
       "1                        0.006817  ...                   NaN   \n",
       "2                             NaN  ...              0.483770   \n",
       "3                        0.049213  ...                   NaN   \n",
       "4                             NaN  ...              0.273776   \n",
       "5                        0.125224  ...                   NaN   \n",
       "\n",
       "   val_iou_natural_wetland  val_iou_natural_wood  \\\n",
       "0                 0.357346              0.734698   \n",
       "1                      NaN                   NaN   \n",
       "2                 0.640979              0.825922   \n",
       "3                      NaN                   NaN   \n",
       "4                 0.656812              0.766886   \n",
       "5                      NaN                   NaN   \n",
       "\n",
       "   val_iou_quasi_natural_ground  val_iou_quasi_natural_grow  \\\n",
       "0                      0.000085                    0.115869   \n",
       "1                           NaN                         NaN   \n",
       "2                      0.082659                    0.653898   \n",
       "3                           NaN                         NaN   \n",
       "4                      0.101420                    0.460483   \n",
       "5                           NaN                         NaN   \n",
       "\n",
       "   val_iou_quasi_natural_wetland  val_iou_quasi_natural_wood  \\\n",
       "0                       0.056907                         0.0   \n",
       "1                            NaN                         NaN   \n",
       "2                       0.566895                         0.0   \n",
       "3                            NaN                         NaN   \n",
       "4                       0.745244                         0.0   \n",
       "5                            NaN                         NaN   \n",
       "\n",
       "   val_iou_transport  val_iou_water  val_loss  \n",
       "0           0.056050       0.227701  1.393110  \n",
       "1                NaN            NaN       NaN  \n",
       "2           0.024938       0.277133  0.568032  \n",
       "3                NaN            NaN       NaN  \n",
       "4           0.072769       0.265772  1.006425  \n",
       "5                NaN            NaN       NaN  \n",
       "\n",
       "[6 rows x 30 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = r'saving_dir\\my_exp_name\\version_1\\metrics.csv'\n",
    "pd.read_csv(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
